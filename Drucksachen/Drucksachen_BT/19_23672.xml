<document>
    <id>246676</id>
    <drucksachetyp>Bericht</drucksachetyp>
    <dokumentart>Drucksache</dokumentart>
    <autoren_anzahl>0</autoren_anzahl>
    <typ>Dokument</typ>
    <vorgangsbezug_anzahl>2</vorgangsbezug_anzahl>
    <dokumentnummer>19/23672</dokumentnummer>
    <wahlperiode>19</wahlperiode>
    <herausgeber>BT</herausgeber>
    <pdf_hash>2d31d4ba0e80287a45c19175cb657d0e</pdf_hash>
    <aktualisiert>2022-07-26T19:57:15+02:00</aktualisiert>
    <vorgangsbezug>
      <id>269053</id>
      <titel>Bericht des Ausschusses f&#252;r Bildung, Forschung und Technikfolgenabsch&#228;tzung (18. Ausschuss) gem&#228;&#223; &#167; 56a der Gesch&#228;ftsordnung
Technikfolgenabsch&#228;tzung (TA)
Autonome Waffensysteme</titel>
      <vorgangstyp>Bericht, Gutachten, Programm</vorgangstyp>
    </vorgangsbezug>
    <vorgangsbezug>
      <id>285981</id>
      <titel>Bericht des Ausschusses f&#252;r Bildung, Forschung und Technikfolgenabsch&#228;tzung (18. Ausschuss) gem&#228;&#223; &#167; 56a der Gesch&#228;ftsordnung
Technikfolgenabsch&#228;tzung (TA)
Autonome Waffensysteme</titel>
      <vorgangstyp>Bericht, Gutachten, Programm</vorgangstyp>
    </vorgangsbezug>
    <urheber>
      <einbringer>false</einbringer>
      <bezeichnung>AfBFT</bezeichnung>
      <titel>Ausschuss f&#252;r Bildung, Forschung und Technikfolgenabsch&#228;tzung</titel>
    </urheber>
    <fundstelle>
      <pdf_url>https://dserver.bundestag.de/btd/19/236/1923672.pdf</pdf_url>
      <id>246676</id>
      <dokumentnummer>19/23672</dokumentnummer>
      <datum>2020-10-21</datum>
      <verteildatum>2020-10-29</verteildatum>
      <dokumentart>Drucksache</dokumentart>
      <drucksachetyp>Bericht</drucksachetyp>
      <herausgeber>BT</herausgeber>
      <urheber>Ausschuss f&#252;r Bildung, Forschung und Technikfolgenabsch&#228;tzung</urheber>
    </fundstelle>
    <text>[Deutscher Bundestag Drucksache 19/23672 
19. Wahlperiode 21.10.2020 
Bericht
des Ausschusses f&#252;r Bildung, Forschung und 
Technikfolgenabsch&#228;tzung (18. Ausschuss)
gem&#228;&#223; &#167; 56a der Gesch&#228;ftsordnung
Technikfolgenabsch&#228;tzung (TA)
Autonome Waffensysteme
Inhaltsverzeichnis
Seite
Vorwort des Ausschusses ............................................................................ 5
Zusammenfassung ........................................................................................ 9
1 Einleitung ...................................................................................... 21
2 Abgrenzung des Untersuchungsgegenstands ............................. 23
2.1 Definitorische Ans&#228;tze ................................................................... 23
2.2 Autonom, semiautonom oder automatisiert?.................................. 24
2.3 Die Definition des US-Verteidigungsministeriums........................ 27
2.4 Die Qualit&#228;t menschlicher Kontrolle &#252;ber AWS............................ 28
3 Technische Grundlagen von Autonomie .................................... 31
3.1 Autonomie aus informationstechnologischer Sicht........................ 31
3.2 Autonome Funktionen aktueller milit&#228;rischer Systeme ................. 31
3.2.1 Autonomie f&#252;r Mobilit&#228;t ................................................................ 32
3.2.2 Autonomie f&#252;r Zielerkennung/Zielbestimmung............................. 32
3.2.3 Autonomie f&#252;r Informationsgewinnung......................................... 32
3.2.4 Autonomie f&#252;r die F&#228;higkeit zur Zusammenarbeit ........................ 33
3.3 K&#252;nstliche Intelligenz als Schl&#252;sseltechnologie ............................ 34
3.3.1 Maschinelles Lernen ...................................................................... 35
3.3.2 Was k&#246;nnen KI-Systeme heute leisten? ......................................... 36
3.3.3 Begrenzungen, Schwierigkeiten und Risiken bei KI-Systemen..... 38
Seite
4 Verbreitung, Status und Trends unbemannter
Waffensysteme .............................................................................. 45
4.1 &#220;berblick zu einsatzreifen unbemannten (teil)autonomen
Waffensystemen............................................................................. 47
4.1.1 Fliegende Systeme.......................................................................... 47
4.1.2 Bodensysteme ................................................................................ 51
4.1.3 Systeme zu Wasser......................................................................... 55
4.2 Forschungs- und Entwicklungstrends ............................................ 56
4.2.1 USA................................................................................................ 57
4.2.2 Europa ............................................................................................ 61
4.2.3 Weitere Schl&#252;sselakteure ............................................................... 65
5 Einsatzszenarien ........................................................................... 67
5.1 Argumente f&#252;r AWS ...................................................................... 67
5.2 Erwartete milit&#228;rische F&#228;higkeiten................................................. 69
5.3 Missionen/Einsatzszenarien ........................................................... 73
5.3.1 Erwartete Missionen....................................................................... 74
5.3.2 Denkbare milit&#228;rische Einsatzszenarien f&#252;r AWS ......................... 75
5.3.3 Zwischenfazit ................................................................................. 79
6 Sicherheitspolitische Implikationen von AWS .......................... 83
6.1 Mehr oder weniger Kriege?............................................................ 84
6.2 Ver&#228;nderung der Kriegsf&#252;hrung..................................................... 84
6.3 Destabilisierende Wirkung in Krisen ............................................. 86
6.4 Auswirkungen auf regionale Stabilit&#228;t ........................................... 87
6.5 Auswirkungen auf das strategische Gleichgewicht ........................ 87
6.6 R&#252;stungsdynamiken ....................................................................... 87
6.7 Unkontrollierte Weiterverbreitung ................................................. 88
6.8 Technologische Risiken ................................................................. 88
7 Humanit&#228;res V&#246;lkerrecht und autonome Waffensysteme........ 91
7.1 Pr&#252;fungspflicht (Artikel 36 ZP I) ................................................... 91
7.2 AWS im Lichte der Prinzipien des HVR ....................................... 92
8 Ethische Fragestellungen im Kontext autonomer
Waffensysteme .............................................................................. 99
8.1 AWS und die Ethik des Krieges..................................................... 99
8.1.1 Die Lehre vom gerechten Krieg ..................................................... 100
8.1.2 Erm&#246;glichen AWS eine ethischere Kriegsf&#252;hrung?....................... 101
8.2 AWS und die W&#252;rde des Menschen............................................... 106
8.2.1 Der Begriff der Menschenw&#252;rde.................................................... 107
Seite
8.2.2 Verletzt der Einsatz autonomer Waffensysteme die 
Menschenw&#252;rde?............................................................................ 108
8.3 Die Frage der Verantwortung......................................................... 110
8.3.1 Rechtliche Sicht.............................................................................. 111
8.3.2 Moralische Sicht............................................................................. 113
8.4 Fazit................................................................................................ 114
9 M&#246;glichkeiten der R&#252;stungskontrolle ........................................ 117
9.1 R&#252;stungs- und Exportkontrollabkommen mit Relevanz f&#252;r AWS ... 117
9.1.1 R&#252;stungskontrollvertr&#228;ge ............................................................... 118
9.1.2 Transparenz und vertrauens- und sicherheitsbildende
Ma&#223;nahmen.................................................................................... 120
9.1.3 Nichtverbreitung und Exportkontrolle ........................................... 121
9.2 Die Konvention &#252;ber bestimmte konventionelle Waffen ............... 124
9.2.1 Menschliche Kontrolle &#252;ber AWS ................................................. 125
9.2.2 Positionen wichtiger Staaten bzw. Organisationen ........................ 127
9.2.3 Gemeinsamer Vorschlag von Deutschland und Frankreich
auf der CCW GGE ......................................................................... 135
9.2.4 Ausgangslage f&#252;r die weitere Diskussion im Rahmen der
CCW............................................................................................... 136
9.3 Regulierungsans&#228;tze der pr&#228;ventiven R&#252;stungs- und 
Exportkontrolle............................................................................... 139
9.3.1 Vertrauens- und sicherheitsbildende Ma&#223;nahmen ......................... 141
9.3.2 Exportkontrolle............................................................................... 142
9.3.3 Verbindliche Regulierung bzw. Verbot von AWS......................... 143
9.4 Handlungsm&#246;glichkeiten................................................................ 145
9.4.1 Die M&#246;glichkeiten innerhalb der CCW aussch&#246;pfen ..................... 146
9.4.2 Engagement &#252;ber die CCW hinaus verbreitern .............................. 146
9.5 Fazit................................................................................................ 149
10 Literatur........................................................................................ 151
10.1 In Auftrag gegebene Gutachten...................................................... 151
10.2 Weitere Literatur ............................................................................ 151
11 Anhang .......................................................................................... 179
11.1 Abbildungen................................................................................... 179
11.2 Tabellen.......................................................................................... 179
11.3 K&#228;sten............................................................................................. 179
11.4 Abk&#252;rzungen .................................................................................. 180
Vorwort des Ausschusses
Die Digitalisierung aller Lebens- und Wirtschaftsbereiche macht auch vor dem
milit&#228;rischen Sektor nicht halt. Bereits heute werden etwa Navigations- und
Aufkl&#228;rungsfunktionen von unbemannten fliegenden Waffensystemen weitgehend autonom
&#252;bernommen. Technologische Fortschritte der letzten Jahre in den Bereichen Robotik und
K&#252;nstliche Intelligenz r&#252;cken nun Waffensysteme in den Bereich des M&#246;glichen, bei denen
immer mehr Funktionen bis hin zum tats&#228;chlichen Waffeneinsatz ohne bzw. nur mit
minimaler menschlicher Mitwirkung ausgef&#252;hrt werden k&#246;nnen. Die Entwicklung und der
m&#246;gliche Einsatz autonomer Waffensysteme (AWS) werfen eine Reihe von ethischen
und (v&#246;lker-)rechtlichen Fragen auf. Die internationale Staatengemeinschaft besch&#228;ftigt
sich derzeit mit diesen Fragen im Rahmen der UN-Waffenkonvention.
Vor diesem Hintergrund hat der Deutsche Bundestag &#8211; auf Initiative des Ausschusses
f&#252;r Bildung, Forschung und Technikfolgenabsch&#228;tzung &#8211; das B&#252;ro f&#252;r Technikfolgen-
Absch&#228;tzung beim Deutschen Bundestag (TAB) beauftragt, ein TA-Projekt zu
autonomen Waffensystemen durchzuf&#252;hren. Ziel der Untersuchung war es, technologische,
milit&#228;risch-strategische, v&#246;lkerrechtliche, ethische und nicht zuletzt
r&#252;stungskontrollpolitische Aspekte von AWS aufzuarbeiten, um orientierungs- und entscheidungsrelevantes
Wissen in diesem hochaktuellen Themenfeld bereitzustellen. Gleichzeitig konnte so das
Monitoring des Themenfelds &#187;Neue Technologien und R&#252;stungskontrolle&#171; fortgesetzt
werden. Im Rahmen dieses Monitorings hat das TAB bereits 2011 eine
Bestandsaufnahme und Folgenabsch&#228;tzung zur milit&#228;rischen Nutzung unbemannter Systeme
vorgelegt (Bundestagsdrucksache 17/6904 &#187;Stand und Perspektiven der milit&#228;rischen
Nutzung unbemannter Systeme&#171;), jedoch nicht mit speziellem Fokus auf den autonomen
Kampfeinsatz.
Der vorliegende Bericht illustriert anhand von existierenden und in der Entwicklung
befindlichen Systemen, welche Funktionen moderne Waffensysteme bereits heute und in
absehbarer Zukunft autonom aus&#252;ben k&#246;nnen. Auf dieser Grundlage werden m&#246;gliche
Einsatzszenarien von AWS diskutiert und sich daraus ergebende sicherheitspolitische
Implikationen analysiert. Diskutiert wird des Weiteren die ethische Fragestellung, ob
und ggf. inwiefern die Anwendung t&#246;dlicher Gewalt durch autonom agierende
Maschinen moralisch zul&#228;ssig ist, sowie die Frage, ob bzw. unter welchen Umst&#228;nden AWS im
Einklang mit den Normen des humanit&#228;ren V&#246;lkerrechts eingesetzt werden k&#246;nnten.
Abgerundet wird der Bericht durch einen &#220;berblick zu R&#252;stungs- und
Exportkontrollabkommen mit Relevanz f&#252;r AWS und Handlungsm&#246;glichkeiten, die aufzeigen, wie sich
m&#246;gliche Risiken von AWS einhegen lassen.
Der Deutsche Bundestag erh&#228;lt mit diesem Bericht des TAB eine fundierte und wertvolle
Informationsbasis f&#252;r die anstehende parlamentarische Befassung mit diesem
hochaktuellen Thema.
Berlin, den 29. September 2020
Dr. Ernst Dieter Rossmann
Vorsitzender
Stephan Albani Ren&#233; R&#246;spel Dr. Michael Espendiller
Berichterstatter Berichterstatter Berichterstatter
Dr. Andrew Ullmann Ralph Lenkert Dr. Anna Christmann
Berichterstatter Berichterstatter Berichterstatterin
Reinhard Gr&#252;nwald
Christoph Kehl
Autonome
Waffensysteme
Endbericht zum TA-Projekt
TAB-Arbeitsbericht Nr. 187
Das B&#252;ro f&#252;r Technikfolgen-Absch&#228;tzung beim Deutschen Bundestag (TAB) ber&#228;t
den Deutschen Bundestag und seine Aussch&#252;sse in Fragen des
wissenschaftlichtechnischen Wandels. Das TAB wird seit 1990 vom Institut f&#252;r
Technikfolgenabsch&#228;tzung und Systemanalyse (ITAS) des Karlsruher Instituts f&#252;r Technologie
(KIT) betrieben. Hierbei kooperiert es seit September 2013 mit dem IZT &#8211; Institut 
f&#252;r Zukunftsstudien und Technologiebewertung gGmbH sowie der VDI/VDE
Innovation + Technik GmbH.
Zusammenfassung
Robotische Waffensysteme, die ohne menschliches Zutun Ziele ausw&#228;hlen und bek&#228;mpfen k&#246;nnen, waren vor
nicht allzu langer Zeit ausschlie&#223;lich in der Dom&#228;ne der Science-Fiction beheimatet. Die enormen
technologischen Fortschritte, die in den letzten Jahren in den Bereichen der Robotik und der k&#252;nstlichen Intelligenz (KI)
erzielt wurden, haben diese Vorstellung autonom agierender Waffen nun an die Schwelle zur konkreten
Umsetzung ger&#252;ckt.
Automatisierung und Autonomie werden bereits heute f&#252;r eine breite Palette an Funktionen bei
Waffensystemen genutzt. Dazu geh&#246;ren die Suche und Identifizierung potenzieller Ziele mithilfe von Sensordaten, die
Zielverfolgung, Priorisierung und Bestimmung des Zeitpunkts f&#252;r den Angriff auf diese Ziele sowie die
Steuerung f&#252;r den Zielanflug (z. B. einer Rakete oder eines Marschflugk&#246;rpers). Bislang erfolgen jedoch die
Zielauswahl, die Angriffsentscheidung und schlie&#223;lich die Freigabe des Waffeneinsatzes durch einen menschlichen
Kommandeur bzw. Operator.
Ein autonomes Waffensystem (AWS) w&#228;re in der Lage, alle diese Schritte selbstt&#228;tig und ohne
menschliche bzw. mit nur minimaler menschlicher Mitwirkung durchzuf&#252;hren. Dies h&#228;tte zwei entscheidende
milit&#228;rische Vorteile: Zum einen ben&#246;tigt ein autonomes System keine Kommunikationsverbindung mit einer
Basisstation, zum anderen erlaubt es schnellere Reaktionszeiten in Gefechtssituationen, da keine Verz&#246;gerungen
durch die Laufzeiten einer Daten&#252;bertragung und durch die Entscheidungsfindung bzw. die Reaktionszeiten
eines menschlichen Operators auftreten. Die Steigerung der Autonomie von Waffensystemen steht daher in
allen technologisch fortgeschrittenen L&#228;ndern auf der Agenda.
Sehr weit verbreitet ist die Auffassung, dass neue Anwendungen k&#252;nstlicher Intelligenz im Begriff seien, 
s&#228;mtliche Wirtschafts- und Lebensbereiche grundlegend zu transformieren. In aktuellen
verteidigungspolitischen und milit&#228;rischen Strategiepapieren und Verlautbarungen etlicher L&#228;nder wird die Erwartung formuliert,
dass diese Transformation auch vor dem Milit&#228;rsektor nicht Halt machen werde und dass die z&#252;gige Einf&#252;hrung 
von KI-gest&#252;tzten Waffensystemen einen entscheidenden milit&#228;rischen Vorsprung verspreche.
Bef&#252;rworter dieser Entwicklung argumentieren, dass mit AWS unter Umst&#228;nden auch humanit&#228;re Vorteile
verbunden seien, da milit&#228;rische Operationen pr&#228;ziser durchgef&#252;hrt und somit die Zivilbev&#246;lkerung und zivile
Infrastrukturen besser gesch&#252;tzt werden k&#246;nnten. Kritiker &#228;u&#223;ern hingegen Bedenken, ob es ethisch vertretbar,
politisch verantwortbar und (v&#246;lker)rechtlich erlaubt sein k&#246;nne, die Entscheidung &#252;ber Leben und Tod von
Menschen an Maschinen zu delegieren. Auch w&#228;ren mit der Entwicklung und dem m&#246;glichen Einsatz von AWS
sicherheitspolitische Risiken sowie die Gefahr von R&#252;stungsspiralen und unkontrollierter Verbreitung
potenziell riskanter Technologien verbunden.
Mit dem vorliegenden Bericht wird ein breiter Analyseansatz verfolgt und eine Vielzahl von Facetten des
Themas abgedeckt: die Darstellung des technologischen Reifegrads und der Entwicklungsperspektiven von
AWS, eine Bestandsaufnahme von existierenden und in der Entwicklung befindlichen Systemen sowie eine
Analyse m&#246;glicher Einsatzszenarien und sich daraus ergebender sicherheitspolitischer Implikationen. Hinzu
kommen ethische und v&#246;lkerrechtliche Fragestellungen, die eng miteinander zusammenh&#228;ngen. Schlie&#223;lich
werden die aktuell vor allem im Rahmen der &#187;Convention on Certain Conventional Weapons&#171;1 (CCW) der 
Vereinten Nationen (United Nations &#8211; UN) angesiedelten diplomatischen Bem&#252;hungen um eine Einhegung der
mit AWS m&#246;glicherweise verbundenen Risiken beleuchtet und daraus &#220;berlegungen zu M&#246;glichkeiten der
pr&#228;ventiven R&#252;stungskontrolle abgeleitet.
Definition bzw. Abgrenzung des Untersuchungsgegenstands
Die Frage der Definition von AWS birgt eine erhebliche Brisanz, da sie oftmals in einen direkten
Zusammenhang mit m&#246;glichen Vereinbarungen zur R&#252;stungskontrolle gestellt wird. Dabei wird implizit oder explizit
angenommen, dass die Definition von AWS gleichzeitig den Rahmen setzt, welche Systeme ggf. zu regulieren
oder gar zu verbieten sind. Daher spiegelt die Haltung der Staaten und anderer Akteure in definitorischen Fragen
regelm&#228;&#223;ig deren Eigeninteressen und Verhandlungspositionen wider.
Convention on Prohibitions or Restrictions on the Use of Certain Conventional Weapons Which May Be Deemed to Be Excessively 
Injurious or to Have Indiscriminate Effects (&#220;bereinkommen &#252;ber das Verbot oder die Beschr&#228;nkung des Einsatzes bestimmter
konventioneller Waffen, die &#252;berm&#228;&#223;ige Leiden verursachen oder unterschiedslos wirken k&#246;nnen)
1
International werden diverse Ans&#228;tze verfolgt, um AWS zu definieren und von anderen Waffensystemen
abzugrenzen. Eine pr&#228;zise, allgemein akzeptierte Definition existiert bis heute nicht. Ob ein Waffensystem
autonom, semiautonom oder (hoch)automatisiert agieren kann, h&#228;ngt nicht nur von seinen technologischen
Eigenschaften ab, sondern auch von der Komplexit&#228;t der Umgebung, in der es eingesetzt werden soll, sowie ganz
entscheidend von der Qualit&#228;t der Interaktion zwischen menschlichem Operator und technischem System. Die
Zuordnung von Waffensystemen in die eine oder andere Kategorie ist in vielen F&#228;llen strittig.
F&#252;r die hier im Zentrum stehende Analyse milit&#228;risch-strategischer, ethischer und v&#246;lkerrechtlicher sowie
sicherheits- und r&#252;stungskontrollpolitischer Fragestellungen ist eine strenge Definition allerdings nicht
erforderlich. Daher wurde mit einer deskriptiven Charakterisierung gearbeitet, nach der ein AWS Auftr&#228;ge ohne
menschliche bzw. mit lediglich schwacher externer menschlicher Kontrolle selbstst&#228;ndig ausf&#252;hrt und die
F&#228;higkeit besitzt, in einer komplexen, dynamischen Umgebung auf unvorhersehbare Ereignisse zielgerichtet
reagieren zu k&#246;nnen. Ob ein konkretes System unter eine bestimmte strenge Definition autonomer Waffensysteme 
fallen w&#252;rde oder nicht, kann dahingestellt bleiben.
Die CCW: internationale Bem&#252;hungen zur Regulierung von AWS
Aufgrund von Bedenken zur ethischen Vertretbarkeit und v&#246;lkerrechtlichen Zul&#228;ssigkeit von AWS bem&#252;ht sich
die internationale Staatengemeinschaft derzeit auf Expertenebene unter dem Dach der CCW, m&#246;gliche durch 
AWS entstehende Risiken zu identifizieren und ggf. einzud&#228;mmen.
Eine der Kernfragen ist, welches Mindestma&#223; an menschlicher Kontrolle &#252;ber ein Waffensystem gegeben
sein muss, damit die v&#246;lkerrechtlichen Anforderungen eingehalten werden k&#246;nnen und die ethische und
juristische Verantwortung jederzeit gekl&#228;rt ist. Oft wird postuliert, dass es daf&#252;r ausreicht, wenn ein Mensch den
Waffeneinsatz ausl&#246;st. Dies wird gerne mit der Formel &#187;Mensch in der Entscheidungsschleife&#171; (&#187;human in the
loop&#171;) ausgedr&#252;ckt. Bei n&#228;herem Hinsehen erweist sich dieser Ansatz jedoch als bei Weitem nicht differenziert
genug, um Fragen der Handlungsurheberschaft und Verantwortung zu kl&#228;ren, die sich bei der Interaktion von
Menschen mit immer intelligenter werdenden Maschinen stellen.
Viele Nichtregierungsorganisationen (NGOs) und eine Reihe von L&#228;ndern favorisieren das Konzept &#187;
Meaningful Human Control&#171; (MHC; bedeutsame menschliche Kontrolle bzw. Steuerung) in der Bedeutung, dass
ein Angriff nur dann statthaft ist, wenn erstens ein menschlicher Bediener bei der Planung und Bewertung eines
Angriffs &#252;ber ad&#228;quate Informationen zu dessen Zielsetzung, Auswirkungen und Kontext verf&#252;gt, zweitens der
Angriff durch eine aktive Handlung eines Menschen initiiert wird und drittens die Menschen, die f&#252;r die Planung
und Durchf&#252;hrung des Angriffs verantwortlich sind, f&#252;r die Folgen zur Rechenschaft gezogen werden k&#246;nnen.
Andere Staaten lehnen das Konzept &#187;Meaningful Human Control&#171; ab. Die USA verweisen darauf, dass in
bestimmten F&#228;llen weniger menschliche Beteiligung sogar w&#252;nschenswert sei, da durch die Nutzung autonomer
Funktionen eine h&#246;here Pr&#228;zision und Geschwindigkeit als bei menschlicher Kontrolle m&#246;glich sei. Daher
favorisieren sie &#187;appropriate levels of human judgement&#171; (angemessene Niveaus menschlicher Beurteilung) beim
Waffeneinsatz.
Beim zentralen Punkt der Debatten im Rahmen der CCW, ob bzw. auf welche Weise autonome
Waffensysteme international reguliert werden sollten, gehen die Positionen der einzelnen L&#228;nder weit auseinander.
Einige sind der Auffassung, dass eine Dehumanisierung der Kriegsf&#252;hrung durch AWS auf jeden Fall verhindert
werden m&#252;sse, und fordern daher eine &#196;chtung und ein wirksames Verbot der Entwicklung, des Erwerbs, der
Stationierung und/oder des Einsatzes solcher Waffen. Andere sind der Ansicht, dass die Anwendung des
bestehenden V&#246;lkerrechts ausreichende Handhaben biete, um Waffensysteme jeglicher Art &#8211; und somit auch AWS
&#8211; zu regulieren. Daneben gibt es diejenigen, die sich vorstellen k&#246;nnen, dass der Einsatz von AWS sogar
humanit&#228;re Vorteile bieten k&#246;nnte, da sie sich im Gegensatz zu Menschen niemals von Emotionen wie Wut oder
Rache leiten lie&#223;en und damit das Risiko von Kriegsverbrechen gesenkt w&#252;rde. Schlie&#223;lich vertreten einige die
Position, dass noch nicht genug Wissen bereitstehe, um Fragen der Regulierung von AWS &#252;berhaupt
anzugehen.
Entwicklungsstand und Trends
Autonome Waffensysteme im strengen Sinne des Wortes, also bewaffnete unbemannte Plattformen, die f&#228;hig
sind, im Kampfeinsatz in einer komplexen, dynamischen Umgebung ohne jegliche menschliche Kontrolle
zielgerichtet zu agieren, gibt es noch nicht. Allerdings sind in verschiedenen Waffengattungen bereits bewaffnete
unbemannte Systeme einsatzreif, die &#252;ber einen relativ weitreichenden Grad an Automatisierung bzw.
Autonomie verf&#252;gen und deshalb als Vorl&#228;ufer von AWS klassifiziert werden k&#246;nnen. Im vorliegenden Bericht wird
anhand bereits genutzter sowie in fortgeschrittenen Stadien der Entwicklung befindlicher unbemannter
Waffensysteme &#8211; aufgeschl&#252;sselt nach den Einsatzbereichen Luft, Land und Wasser &#8211; der aktuelle Stand erreichter
Autonomie grob eingeordnet.
Ein Blick auf stationierte und teilweise bereits in Kampfhandlungen eingesetzte unbemannte
Waffensysteme (UWS) zeigt, dass in den letzten 10 Jahren die Zahl der staatlichen und nichtstaatlichen Akteure, die damit
ausger&#252;stet sind, stark zugenommen hat. Diese Entwicklung geht fast ausschlie&#223;lich auf das Konto
ferngesteuerter Kampfdrohnen, die mit Abstand zu den am h&#228;ufigsten produzierten und am weitesten verbreiteten UWS
geh&#246;ren. Zwar w&#228;chst auch die Bedeutung von unbemannten Boden- und Wasserfahrzeugen, ihre
Einsatzf&#228;higkeiten sind derzeit jedoch noch weitgehend auf andere Zwecke als den Waffeneinsatz begrenzt (Aufkl&#228;rung,
&#220;berwachung, Logistik etc.).
Noch Anfang der 2000er Jahre waren die USA die einzige Nation, die &#252;ber fortschrittliche MALE-
Kampfdrohnen (&#187;medium altitude long endurance&#171;) verf&#252;gte. Diese wurden speziell f&#252;r eine mittlere Flugh&#246;he (ca. 10
bis 15 km) und lange Reichweiten entwickelt (24 bis 48 Stunden Flugdauer); ein erster Kampfeinsatz erfolgte
im Oktober 2001 in Afghanistan. In den folgenden Jahren bauten die USA ihr Drohnenprogramm massiv aus
und diverse andere L&#228;nder begannen mit der Entwicklung bzw. Beschaffung von Kampfdrohnen. Es ist davon
auszugehen, dass inzwischen mindestens 30 Staaten &#252;ber fortschrittliche Kampfdrohnen verf&#252;gen, davon haben
10 L&#228;nder (einschlie&#223;lich der USA) diese bereits unter Waffengebrauch in Kampfhandlungen eingesetzt.
Deutschland besitzt bis heute noch keine Kampfdrohnen. Als fortschrittlichstes Flugger&#228;t (&#187;unmanned aerial
vehicle&#171; &#8211; UAV) setzt die Bundeswehr seit 2010 die von Israel geleaste, unbewaffnete &#8211; allerdings in der
aktuellen Version bewaffnungsf&#228;hige &#8211; Drohne &#187;Heron&#171; der MALE-Kategorie zu Aufkl&#228;rungszwecken ein.
Entwicklungsstand unbemannter (teil)autonomer Waffensysteme
Dass Flugger&#228;te in der Entwicklung am weitesten fortgeschritten und bei der Verbreitung von UWS
vorherrschend sind, h&#228;ngt wesentlich damit zusammen, dass Navigation, Orientierung und Funkkommunikation in der
Luft relativ einfach zu bewerkstelligen sind &#8211; deutlich einfacher jedenfalls, als dies an Land bzw. auf oder unter
Wasser der Fall ist. Allerdings sind die autonomen F&#228;higkeiten fliegender unbemannter Systeme insgesamt
nach wie vor begrenzt:
&#8250; Selbst bei den fortschrittlichen Kampfdrohnen beschr&#228;nkt sich die Autonomie &#252;blicherweise auf die
Flugkontrolle, auf Navigations- und Aufkl&#228;rungsfunktionen sowie die eventuelle R&#252;ckkehr im Falle eines
Funkabbruchs; der Waffeneinsatz geschieht in der Regel per Fernsteuerung und unterliegt somit in letzter
Instanz immer noch menschlicher Kontrolle. Zu den wenigen Drohnensystemen, die mit relativ
weitreichenden autonomen Angriffsfunktionen ausgestattet sind, geh&#246;ren die israelische Drohne &#187;Harpy&#171;, die
bereits in den 1990er Jahren entwickelt wurde, sowie deren Nachfolger &#187;Harop&#171;. Diese dienen prim&#228;r dem
Zweck, feindliche Flugabwehrsysteme auszuschalten. Sie suchen selbstt&#228;tig ein definiertes Gebiet nach
Radarsignalen ab, lokalisieren deren Quelle und st&#252;rzen sich auf das Zielobjekt, um es durch eine
Explosion zu zerst&#246;ren.
&#8250; G&#228;ngige Lenkwaffen werden &#252;blicherweise durch aktive menschliche Steuerung (z. B. per Laserstrahl) ins
Ziel gelenkt, oder sie steuern selbstst&#228;ndig ein vorab definiertes und einprogrammiertes Ziel an.
Lenkflugk&#246;rper, die bereits &#252;ber weitreichende autonome Funktionen verf&#252;gen, sind beispielsweise die britische 
&#187;Brimstone&#171; sowie der Antischiffsflugk&#246;rper (&#187;long range anti-ship missile&#171; &#8211; LRASM), der derzeit von
den USA entwickelt wird. Einmal gestartet, k&#246;nnen sie im Prinzip v&#246;llig eigenst&#228;ndig auf Basis von
gespeicherten Signaturen nach Zielen suchen. LRASM soll dar&#252;ber hinaus die F&#228;higkeit besitzen, die
Angriffsstrategie selbstst&#228;ndig mit anderen Flugk&#246;rpern in einer Salve oder einem Schwarm zu koordinieren.
Bei milit&#228;rischen Robotern, die am Boden operieren, k&#246;nnen station&#228;re und mobile Systeme unterschieden
werden. Die verf&#252;gbaren station&#228;ren Bodensysteme dienen vor allem defensiven Zwecken, z. B. f&#252;r:
&#8250; Personenabwehr, wie die robotischen Wachpostenkanonen, die von Israel (entlang des Gazastreifens)
sowie m&#246;glicherweise von S&#252;dkorea (entlang der demilitarisierten Zone) stationiert worden sind und &#252;ber
alle technischen Voraussetzungen f&#252;r einen vollautomatischen Betrieb verf&#252;gen sollen;
&#8250; Nahbereichsflugabwehr wie das deutsche System &#187;MANTIS&#171;, das US-amerikanische &#187;Phalanx CIWS&#171;
oder das israelische &#187;Iron Dome&#171;. Diese Systeme sind mit weitreichenden automatisierten Funktionen
ausgestattet, die sie in die Lage versetzen, auch kleine, schnell bewegliche Ziele mithilfe von Radar zu
identifizieren, anhand des Radarquerschnitts zu klassifizieren und schlie&#223;lich per Maschinenkanone oder
Abfangrakete zu bek&#228;mpfen &#8211; alles nach ihrer Aktivierung prinzipiell ohne menschliches Zutun.
Unbemannte Bodenfahrzeuge (&#187;unmanned ground vehicles&#171; &#8211; UGVs) sind grunds&#228;tzlich mit deutlich
komplexeren Anforderungen konfrontiert als ihre fliegenden Pendants: Sie haben keinen freien Luftraum vor sich,
sondern verschiedene Arten von Hindernissen bzw. teils unwegsames, un&#252;bersichtliches Gel&#228;nde. Das
erschwert die Navigation, die Umgebungserfassung sowie die Verhaltensplanung und erh&#246;ht die mechanischen
Herausforderungen bei der Fortbewegung vor allem in unwegsamem Terrain. Aus diesen Gr&#252;nden sind
autonome Funktionen bei UGVs erheblich schwieriger zu realisieren als bei UAVs. Die milit&#228;rischen UGV-
Systeme, die sich bereits im Einsatz befinden, sind in aller Regel nicht bewaffnet, werden aus der N&#228;he
ferngesteuert und haben vor allem unterst&#252;tzende Funktionen: Sie dienen z. B. Transportzwecken (wie das &#187;Squad
Mission Support System&#171; &#8211; SMSS f&#252;r die U.S. Army), der Aufkl&#228;rung oder der Minenr&#228;umung (wie der in gro&#223;er
St&#252;ckzahl produzierte Kleinroboter &#187;PackBot&#171;).
Gleichwohl wird intensiv an bewaffneten UGVs gearbeitet, die mit komplexeren autonomen F&#228;higkeiten
ausgestattet sind. Was Navigation und Umgebungserfassung angeht, gibt es naturgem&#228;&#223; gro&#223;e Schnittmengen
mit der zivilen Forschung zum autonomen Fahren. Einer der Vorreiter in diesem Bereich ist Israel, das zur
Grenzkontrolle mit einer ganzen Reihe teils bewaffneter oder zumindest bewaffnungsf&#228;higer UGVs operiert.
Ein Waffeneinsatz kann, zumindest derzeit, nur manuell ausgel&#246;st werden.
Auf und unter Wasser, insbesondere auf hoher See, besteht im Allgemeinen mehr Raum zum Man&#246;vrieren
als an Land. Au&#223;erdem sind die Meere arm an Hindernissen und weitgehend menschenleer, sodass das Risiko
von Kollateralsch&#228;den gering ist. All dies w&#252;rde den Unter- sowie &#220;berwasserbereich eigentlich f&#252;r den Einsatz
autonomer Waffensysteme pr&#228;destinieren. Dass bislang kaum einsatzf&#228;hige seegest&#252;tzte UWS existieren, h&#228;ngt
u. a. mit den potenziell riesigen Einsatzr&#228;umen zusammen, die vor allem mit Blick auf Energieversorgung und
Kommunikation (insbesondere unter Wasser) eine gro&#223;e Herausforderung darstellen. Zu den wenigen
verf&#252;gbaren unbemannten &#220;berwasserfahrzeugen (&#187;unmanned surface vehicles&#171; &#8211; USVs) geh&#246;rt das israelische &#187;
Silver Marlin&#171;. Es dient vor allem der &#220;berwachung und Aufkl&#228;rung und soll autonom vorgegebene Koordinaten
ansteuern k&#246;nnen. Neben einem Radar und verschiedenen elektrooptischen Sensoren ist das Boot mit einem
Maschinengewehr ausgestattet, das jedoch rein ferngesteuert ist.
Im Vergleich zu den USVs verf&#252;gen Unterwasserfahrzeuge (&#187;unmanned underwater vehicles&#171; &#8211; UUVs) 
&#252;ber eine ungleich h&#246;here milit&#228;rstrategische Bedeutung. Ihre Verbreitung nimmt weltweit zu, wenngleich die
Entwicklung bewaffneter Systeme noch in den Anf&#228;ngen steckt und die milit&#228;rischen Aufgaben verf&#252;gbarer 
Systeme sich bisher fast ausschlie&#223;lich auf Aufkl&#228;rung, &#220;berwachung und Seeminenr&#228;umung konzentrieren.
Beispiele sind u. a. der deutsche &#187;Seeotter MKII&#171;, die russische Unterwasserdrohne &#187;Klavesin-2&#171; oder der US-
amerikanische &#187;Echo Ranger&#171;, der mittels eines hybriden, wiederaufladbaren Energiesystems mehrere Monate
autark operieren k&#246;nnen soll. Dennoch ist zu erwarten, dass weitr&#228;umig operierenden Unterwassersystemen
perspektivisch eine hohe Bedeutung zukommen k&#246;nnte, sofern sich die Probleme bei der langfristigen
Energieversorgung sowie der weitreichenden Kommunikationsverbindung l&#246;sen lassen. Gerade die physikalisch
bedingten Einschr&#228;nkungen bei der Kommunikation unter Wasser begr&#252;nden ein starkes Motiv, einen weitgehend
autonomen Betrieb von UUVs zu implementieren.
Forschungs- und Entwicklungstrends
Die USA sind als Vorreiter und Treiber der Entwicklung zunehmend automatisierter Waffensysteme und als
wesentlicher Wegbereiter f&#252;r zuk&#252;nftige autonome Waffensysteme anzusehen. Daher werden im Bericht
Forschungs- und Entwicklungstrends bei AWS besonders am Beispiel der USA aufgezeigt. Ein weiterer Grund
hierf&#252;r ist, dass Details &#252;ber Forschungs- und Entwicklungsaktivit&#228;ten in den USA dank der vergleichsweise
offenen Informationspolitik wesentlich einfacher und umfassender &#246;ffentlich zug&#228;nglich sind als in anderen
L&#228;ndern.
Dass der langj&#228;hrige Trend in den USA deutlich in Richtung AWS geht, spiegelt sich z. B. in den vom
U.S. Department of Defense (DOD) regelm&#228;&#223;ig publizierten milit&#228;rischen Entwicklungsstrategien zu
unbemannten Systemen wieder. In der aktuellen &#187;Unmanned Systems Integrated Roadmap&#171; von 2017 werden
Autonomie und Robotik als Schl&#252;sselfaktoren f&#252;r die weitere Entwicklung unbemannter Systeme genannt, die das
Potenzial h&#228;tten, die zuk&#252;nftige Kriegsf&#252;hrung zu revolutionieren. Bereits heute investieren die USA stattliche
Summen in Forschung und Entwicklung (FuE). Geforscht wird sowohl in &#252;bergeordneten Forschungsinitiativen
wie dem &#187;Science of Autonomy Program&#171; des Office of Naval Research (ONR) als auch in kleineren
spezifischen Programmen.
Es ist ein erkl&#228;rtes Ziel der US-Streitkr&#228;fte, unbemannte Systeme in Schw&#228;rmen kommunizieren und
kooperativ zusammenarbeiten zu lassen. Das Schwarmprinzip basiert auf der taktischen Idee, die gegnerische
Abwehr dadurch zu &#252;berfordern, dass man ein bestimmtes Ziel gleichzeitig von vielen Seiten und mehrfach
angreift. Davon verspricht man sich eine besonders wirkungsvolle und robuste Art der Kampff&#252;hrung. Daneben
ist die Entwicklung von Teams aus Mensch und Maschine (&#187;manned-unmanned teaming&#171; &#8211; MUM-T) ein
weiteres zentrales Ziel von U.S. Air Force, U.S. Navy und U.S. Army. So kann z. B. dem Piloten eines Kampfjets
ein einzelnes bewaffnetes UAV als loyaler Fl&#252;gelmann (&#187;loyal wingman&#171;) zur Seite gestellt werden &#8211; oder 
sogar ein kompletter Schwarm, der der Kontrolle des Piloten untersteht. F&#252;r die fernere Zukunft setzen die US-
Streitkr&#228;fte auf unbemannte Fahrzeuge und komplexe Schwarmsysteme, die autonom agieren k&#246;nnen, um
menschliche Streitkr&#228;fte &#8211; auch in Gebieten, zu denen der Zugang aufgrund starker Verteidigungskr&#228;fte
erschwert ist &#8211; zu unterst&#252;tzen. Geplant ist, unbemannte taktische Fahrzeuge zuk&#252;nftig als &#187;force multiplier&#171;
einzusetzen, d. h. in unterst&#252;tzender Form als Teil interaktiver Mensch-Maschine-Operationen.
Die USA engagieren sich in einer erheblichen Anzahl konkreter Entwicklungs- und Beschaffungsprojekte
im Bereich unbemannter (teil)autonomer Systeme. Der klare Schwerpunkt liegt dabei auf autonomen
Kampfflugzeugen. So verfolgen die US-Streitkr&#228;fte das Vorhaben, eine tr&#228;gergest&#252;tzte Kampfdrohne mit
Tarnkappeneigenschaften zu entwickeln. Die FuE-Trends im Bereich der US-R&#252;stungsindustrie und des US-Milit&#228;rs dienen
wiederum als Vorbild f&#252;r andere wichtige staatliche Akteure, die ebenfalls stark in die Drohnenentwicklung
investieren (darunter z. B. China, Israel und Russland, aber auch westeurop&#228;ische Staaten). Im vorliegenden
Bericht werden die FuE-Trends weiterer Schl&#252;sselakteure &#252;berblicksartig betrachtet, insbesondere die
Aktivit&#228;ten in Deutschland, Frankreich und Gro&#223;britannien.
Einsatzszenarien
Eine milit&#228;risch-strategisch bedeutsame Fragestellung lautet, f&#252;r welche Art von Einsatzszenarien sich
zuk&#252;nftige AWS aufgrund ihrer Eigenschaften besonders eignen w&#252;rden. Zwei Charakteristika kommen hierbei
besonders zum Tragen. Zum einen ist dies die physische Abwesenheit eines menschlichen Operators. Dies
erm&#246;glicht u. a. kleinere und agilere Plattformen, die in Gebieten operieren k&#246;nnen, die f&#252;r Menschen nicht oder nur
schwer zug&#228;nglich sind. Zum anderen soll die F&#228;higkeit, autonom zu operieren, schnellere und effektivere
Entscheidungen in zeitkritischen Situationen gestatten. Zudem er&#246;ffnet dies neue Optionen zur Koordination in
einer Gruppe (bzw. einem Schwarm) von AWS oder aber zur Kooperation zwischen AWS und Menschen
(MUM-T). Die sich aus dem F&#228;higkeitsspektrum zuk&#252;nftiger AWS perspektivisch ergebenden
Einsatzm&#246;glichkeiten werden absehbar die Aufgabenverteilung zwischen Mensch und Maschine neu definieren.
Aufgrund ihrer potenziellen F&#228;higkeiten und daraus resultierender milit&#228;rischer Vorteile werden sich AWS
insbesondere f&#252;r Einsatzszenarien eignen, die f&#252;r Menschen wegen der Umgebungsbedingungen problematisch 
oder zu gef&#228;hrlich sind (z. B. Operationen unter Wasser oder auf durch starke Kr&#228;fte verteidigtem Terrain), die
sehr schnelle Reaktionszeiten oder eine hohe Man&#246;vrierf&#228;higkeit erfordern (z. B. Luftkampf) oder aber in einem
dynamischen Umfeld ohne eine permanente Kommunikationsverbindung stattfinden (z. B. verdeckte
Operationen hinter den feindlichen Linien).
Zum gegenw&#228;rtigen Zeitpunkt sind jedoch sowohl die technischen Eigenschaften als auch die
Verf&#252;gbarkeit zuk&#252;nftiger AWS noch spekulativ, sodass lediglich die Umrisse eines m&#246;glichen Portfolios an
Einsatzszenarien in groben Z&#252;gen erkennbar sind. Dessen konkrete Ausgestaltung h&#228;ngt von einem komplexen
Zusammenspiel mehrerer Faktoren ab. Hierbei spielen nicht nur technologische Entwicklungen eine wesentliche Rolle,
sondern auch die Wandlung der Streitkr&#228;ftestrukturen, die sich im Zuge der sukzessiven Ber&#252;cksichtigung von
Robotik und KI in Doktrinen, Strategien sowie Taktiken, Techniken und Prozeduren abzeichnet, sowie der
Konflikttypus, f&#252;r den Einsatzszenarien entworfen werden.
Ein Bereich, der bisher noch wenig beleuchtet ist, aber absehbar mit einer zunehmenden Verbreitung von
AWS erheblich an Bedeutung gewinnen wird, beinhaltet m&#246;gliche technologische oder operative
Gegenma&#223;nahmen, die von Antagonisten gegen AWS getroffen werden k&#246;nnten. Hardware (z. B. Sensoren) und Software
von AWS werden ganz eigene Schwachstellen aufweisen, die von entsprechend innovativen Widersachern
ausgenutzt werden k&#246;nnten. Hierzu geh&#246;ren M&#246;glichkeiten der elektronischen Manipulation (&#187;jamming&#171;, &#187;
spoofing&#171;, &#187;hacking&#171;), aber auch eher im Lowtechbereich angesiedelte Ma&#223;nahmen k&#246;nnten ggf. sehr effektiv sein.
Beispiele sind der Einsatz von Fangnetzen gegen kleine fliegende AWS, aufblasbare Attrappen oder thermische
Quellen, um hitzesuchende Sensoren zu t&#228;uschen.
Sicherheitspolitische Implikationen
In fast allen technologisch fortgeschrittenen L&#228;ndern wird derzeit die milit&#228;rische Nutzung von zunehmend 
autonomen Systemen vorangetrieben, denen ein transformatives Potenzial f&#252;r die Streitkr&#228;fte zugeschrieben
wird. Beispielsweise wird Autonomie &#8211; gemeinsam mit einer Handvoll weiterer Technologien &#8211; in der k&#252;rzlich 
ver&#246;ffentlichten nationalen Verteidigungsstrategie der USA als Schl&#252;sself&#228;higkeit angesehen, um &#187;die Kriege 
der Zukunft zu f&#252;hren und zu gewinnen&#171;. Es ist daher essenziell, sich mit den sicherheitspolitischen
Implikationen zu befassen, welche die Verbreitung und der m&#246;gliche Einsatz von AWS mit sich bringen k&#246;nnen.
Mehr oder weniger kriegerische Gewalt?
Ob die Verf&#252;gbarkeit von AWS dazu f&#252;hrt, dass im Konfliktfall schneller zum Mittel der milit&#228;rischen Gewalt
gegriffen wird oder dass milit&#228;rische Auseinandersetzungen gewaltsamer gef&#252;hrt werden, sind derzeit
kontrovers diskutierte Fragen.
Demokratisch gew&#228;hlte Regierungen stehen regelm&#228;&#223;ig unter erheblichem Rechtfertigungsdruck, wenn
eigene Soldaten gef&#228;hrdet sind bzw. get&#246;tet werden k&#246;nnen. Wenn der Einsatz von AWS das Risiko f&#252;r die 
eigenen Soldaten erheblich verringert (u. a. da AWS Aufgaben &#252;bernehmen, die f&#252;r Menschen gef&#228;hrlich sind),
k&#246;nnte die Hemmschwelle sinken, Gewalt einzusetzen. Dies wiederum k&#246;nnte dazu f&#252;hren, dass die
&#214;ffentlichkeit einen Krieg nicht mehr als letzten Ausweg bzw. als einschneidendes Ereignis von nationaler Bedeutung
wahrnimmt, sondern diesen als einen unter dem Primat diplomatischer und &#246;konomischer Abw&#228;gungen
stehenden Normalfall akzeptiert. Auch unterhalb der Schwelle ausgewachsener Kriege k&#246;nnten Milit&#228;reins&#228;tze zur
Durchsetzung politischer Ziele attraktiver und immer mehr zur Regel werden.
Dagegen kann argumentiert werden, dass AWS nichts grunds&#228;tzlich Neues bringen, sondern eine ohnehin
vonstattengehende Entwicklung lediglich graduell verst&#228;rken w&#252;rden &#8211; mit den Drohnenschl&#228;gen in
Afghanistan, Somalia und anderswo als prominente Beispiele. Dar&#252;ber hinaus sind Situationen vorstellbar, in denen ein
schnellerer und entschlossenerer Einsatz von Gewaltmitteln viel menschliches Leid verhindern k&#246;nnte,
beispielsweise wenn Warlords die Bev&#246;lkerung terrorisieren bzw. ethnische S&#228;uberungen durchf&#252;hren.
Generell verliert das Argument, dass AWS zu einem h&#228;ufigeren Einsatz von Gewalt f&#252;hren, erheblich an
Kraft, wenn man kein asymmetrisches Szenario &#8211; wie etwa gegenw&#228;rtig bei den Drohnenschl&#228;gen &#8211;, sondern 
eines mit Kontrahenten auf Augenh&#246;he betrachtet. Hier w&#252;rde f&#252;r die Seite, die AWS einsetzt, immer die Gefahr
bestehen, dass schmerzhafte Vergeltungsma&#223;nahmen des Gegners folgen k&#246;nnten bzw. im ung&#252;nstigsten Fall
eine Eskalation zu einem ausgewachsenen Krieg mit ungewissem Ausgang eintreten k&#246;nnte. Insbesondere f&#252;r
die Nuklearwaffenstaaten untereinander w&#252;rde dieses Eskalationsrisiko stark gegen einen als lokal begrenzt
intendierten Einsatz von AWS sprechen.
Auswirkungen auf regionale Stabilit&#228;t und das strategische Gleichgewicht
Die Verf&#252;gbarkeit von AWS k&#246;nnte sich besonders in Zeiten erh&#246;hter Spannungen bzw. bei Krisen sowohl auf
die Stabilit&#228;t in regionalen Kontexten als auch auf das strategische Gleichgewicht der Nuklearm&#228;chte
auswirken.
Auf der einen Seite k&#246;nnten AWS die Stabilit&#228;t dadurch erh&#246;hen, indem mit ihrer Hilfe (z. B. im Rahmen
von Aufkl&#228;rungsmissionen) mehr Informationen in k&#252;rzerer Zeit beschafft und ausgewertet werden k&#246;nnten
und somit eine bessere Grundlage sowie mehr Zeit f&#252;r menschliche Entscheidungstr&#228;ger zur Verf&#252;gung
st&#252;nden, alle Konsequenzen einer Eskalation zu durchdenken und die richtige Entscheidung zu treffen. Des
Weiteren k&#246;nnten AWS auch als defensive Systeme ausgelegt sein und so einen eskalationswilligen Akteur von einer
Aggression abbringen. Eine weitere M&#246;glichkeit, die Abschreckung zu erh&#246;hen, best&#252;nde darin, dass
glaubw&#252;rdig im Falle einer m&#246;glichen Aggression mit einem autonomen Gegenschlag gedroht w&#252;rde.
Auf der anderen Seite sind aber auch destabilisierende Wirkungen vorstellbar. So k&#246;nnten das operative
Geschehen und die Entscheidungsprozesse durch AWS derart beschleunigt werden, dass Menschen kognitiv
und hinsichtlich ihres Reaktionsverm&#246;gens an ihre Grenzen k&#228;men. So k&#246;nnte in einer Krise eine
Eskalationsspirale automatisiert und m&#246;glicherweise ungewollt in Gang gesetzt werden. F&#252;r ein solches Szenario wurde
der Begriff &#187;flash war&#171; gepr&#228;gt.
Die potenzielle Beschleunigung des Geschehens k&#246;nnte auch dazu f&#252;hren, dass Akteure dazu veranlasst 
werden, pr&#228;emptiv zuzuschlagen. Au&#223;erdem k&#246;nnte der zuvor skizzierte Versuch, das Abschreckungspotenzial 
durch autonome Antworten zu erh&#246;hen, auch die Stabilit&#228;t unterminieren, wenn z. B. bei technischen
Fehlfunktionen keine M&#246;glichkeit der menschlichen Intervention mehr best&#252;nde und so ein potenziell katastrophaler
Schlag unbeabsichtigt gef&#252;hrt w&#252;rde.
AWS w&#228;ren auch dazu pr&#228;destiniert, proaktiveres und eventuell provokativeres milit&#228;risches Verhalten zu
unterst&#252;tzen, wie z. B. das Eindringen in gegnerisches Territorium, um dort Aufkl&#228;rung zu betreiben. Ein
gegnerischer Staat k&#246;nnte gleichzeitig eine geringere Hemmung haben, eine solche unbemannte Plattform
abzuschie&#223;en, woraus wiederum ein unmittelbares neues Eskalationspotenzial erw&#252;chse.
Auf der globalen Ebene spielt das strategische Gleichgewicht zwischen den Nuklearwaffenstaaten nach
wie vor eine herausragende Rolle. Es basiert wesentlich auf der gesicherten F&#228;higkeit eines Zweitschlags und
der daraus resultierenden Abschreckung eines m&#246;glichen Erstschlags. Es w&#228;re vorstellbar, dass sehr potente
AWS zuk&#252;nftig als konventionelle Erstschlagwaffen zur Zerst&#246;rung gegnerischer Nuklearwaffenarsenale
eingesetzt werden k&#246;nnten, die m&#246;gliche Ziele (Raketensilos oder mit Nuklearwaffen best&#252;ckte U-Boote)
selbstst&#228;ndig aufkl&#228;ren, in deren N&#228;he unentdeckt verweilen und auf Befehl koordiniert diese Ziele angreifen und
zerst&#246;ren. AWS k&#246;nnten auch als Tr&#228;gerplattformen f&#252;r Nuklearwaffen verwendet werden, beispielsweise in
Form von autonomen Unterwasserfahrzeugen. Diese k&#246;nnten schneller, &#252;berraschender und koordinierter als
bisherige Tr&#228;gersysteme zuschlagen und vorhandene Verteidigungsma&#223;nahmen aushebeln. Eine solche
Nutzung von AWS w&#252;rde die strategische Stabilit&#228;t massiv infrage stellen. Dies wiederum k&#246;nnte weitere nukleare 
Abr&#252;stung unm&#246;glich machen und eine &#196;ra nuklearer Modernisierung oder gar nuklearer Aufr&#252;stung einl&#228;uten.
R&#252;stungsdynamiken und unkontrollierte Weiterverbreitung
Wie zuvor ausgef&#252;hrt, schreiben die Gro&#223;m&#228;chte autonomen Technologien langfristig einen hohen
milit&#228;rischen Stellenwert zu. Technologische Durchbr&#252;che einer Seite k&#246;nnten das bestehende Kr&#228;ftegleichgewicht
fundamental ersch&#252;ttern. Ein solches Bedrohungsszenario dient oftmals als Legitimation f&#252;r eigene
Anstrengungen auf diesem Feld. Zwischen den USA und China ist bereits heute eine beginnende R&#252;stungsdynamik bei
zunehmend automatisierten UWS zu beobachten. Auch Russland konzentriert sich darauf, im Bereich
einsatzreifer UWS den bisherigen Vorsprung des Westens aufzuholen, und scheint auch zuk&#252;nftig AWS entwickeln
zu wollen. Regionale Spannungsherde mit ausgepr&#228;gten R&#252;stungsspiralen bestehen heute u. a. zwischen Indien
und Pakistan bzw. S&#252;d- und Nordkorea. Die Beschaffung von AWS k&#246;nnte hier zus&#228;tzliche
R&#252;stungsdynamiken in Gang setzen.
Zuk&#252;nftige AWS werden auf technologischen Entwicklungen basieren, die ihren Ursprung gr&#246;&#223;tenteils im
zivilen Sektor haben. Ein gro&#223;er Teil der Hardware, vor allem aber deren Software basiert &#252;berwiegend auf
Dual-Use-Technologien. Die Forschung zur k&#252;nstlichen Intelligenz &#8211; einschlie&#223;lich maschinellem Lernen, Big-
Data-Analysen etc. &#8211; wird ma&#223;geblich von kommerziellen Unternehmen mit Blick auf private Konsumenten
und den zivilen Wirtschaftssektor vorangetrieben.
Entwicklung und Produktion von leistungsstarken AWS (mit hoher Nutzlast, Reichweite, Zuverl&#228;ssigkeit
etc.) werden trotz verf&#252;gbarer Dual-Use-Technologien allerdings erhebliche finanzielle und zeitliche
Ressourcen erfordern. Daher k&#246;nnten Staaten oder andere Akteure versuchen, in den Besitz von kleineren, weniger
leistungsf&#228;higen AWS zu gelangen, indem sie fortschrittliche zivile Technologien in bewaffnete AWS
&#252;berf&#252;hren, wie dies bei ferngesteuerten Drohnen (ohne autonome F&#228;higkeiten) bereits heute zu beobachten ist. Ob
solche AWS auch f&#252;r Terroristen attraktiv werden, ist derzeit noch nicht abzusehen, zumal &#228;hnliche Wirkungen
auch mit sehr viel einfacheren Mitteln und geringerem Aufwand erreichbar sind, wie Anschl&#228;ge des IS
(Islamischer Staat) in der j&#252;ngsten Zeit gezeigt haben.
Bereits heute existiert eine scharfe internationale Konkurrenz um den Export von bewaffneten UAVs,
besonders China und Israel sind hier sehr aktiv. Um ihre Wettbewerbsposition zu st&#228;rken, haben j&#252;ngst auch die
USA ihre bisher strengen Exportvorschriften gelockert. Der Wettbewerb um R&#252;stungsexporte k&#246;nnte zuk&#252;nftig 
in analoger Weise auch die Weiterverbreitung von Systemen mit mehr und mehr autonomen Funktionen
beschleunigen.
V&#246;lkerrechtliche und ethische Aspekte
Die Entwicklung und der m&#246;gliche Einsatz von zunehmend autonom agierenden Waffensystemen schaffen
gro&#223;e normative Unsicherheiten, die sich in der Kernfrage zuspitzen, ob und inwiefern es erlaubt sein soll,
Maschinen &#252;ber Tod oder Leben von Menschen entscheiden zu lassen. Diese Fragestellung erscheint nur mit
Blick auf die besondere Situation, wie sie im Kriegsfall herrscht, &#252;berhaupt zul&#228;ssig. Doch selbst in Kriegen
gibt es Grenzen des moralisch wie auch v&#246;lkerrechtlich Erlaubten, die sicherstellen sollen, dass ein gewisses
Ma&#223; an Menschlichkeit gewahrt bleibt. Obwohl noch nicht abzusehen ist, ob und wann autonome
Waffensysteme einsatzf&#228;hig sein werden, wird bereits seit einigen Jahren intensiv &#252;ber ihre normativen Implikationen
diskutiert.
AWS im Lichte der Prinzipien des humanit&#228;ren V&#246;lkerrechts
Wie jedes Waffensystem sind auch AWS im Kontext der geltenden Normen des humanit&#228;ren V&#246;lkerrechts
(HVR) zu betrachten. Das HVR soll im Falle eines internationalen bewaffneten Konflikts den gr&#246;&#223;tm&#246;glichen
Schutz von Zivilisten, nichtmilit&#228;rischen Geb&#228;uden und Infrastrukturen sowie der nat&#252;rlichen Umwelt
gew&#228;hrleisten und intendiert somit, eine Balance zwischen humanit&#228;ren Erw&#228;gungen und milit&#228;rischen
Notwendigkeiten zu schaffen. Das HVR bezieht sich nicht prim&#228;r auf technologische Systeme als solche, sondern zielt darauf
ab, die Art und Umst&#228;nde ihres Einsatzes einzuschr&#228;nken.
Zuk&#252;nftige AWS werden sich gegen&#252;ber bisherigen Waffensystemen, inklusive der heutigen unbemannten
Waffensysteme, dadurch auszeichnen, dass sie in einem sich dynamisch ver&#228;ndernden, nicht vorhersehbaren
Umfeld autonom agieren k&#246;nnen, bis zu einem gewissen Grad also selber Handlungsentscheidungen treffen
m&#252;ssen, ohne dabei einer direkten menschlichen Steuerung bzw. Kontrolle zu unterliegen. Ob ihr Einsatz
v&#246;lkerrechtlich zul&#228;ssig sein k&#246;nnte, muss bereits im Vorfeld, und zwar &#187;bei Pr&#252;fung, Entwicklung, Beschaffung
oder Einf&#252;hrung&#171; gepr&#252;ft und festgestellt werden. Hierzu haben sich die Vertragsparteien gem&#228;&#223; Artikel 36 des
Zusatzprotokolls I (ZP I) der Genfer Konventionen, die eine wichtige Komponente des HVR sind, verpflichtet.
Der Kernbereich des HVR fu&#223;t auf drei Prinzipien, die dabei in Betracht zu ziehen sind:
&#8250; Unterscheidungsgebot: Das HVR gebietet, dass Kampfhandlungen sich nur gegen milit&#228;rische Ziele &#8211;
Kombattanten und milit&#228;rische Objekte &#8211; richten d&#252;rfen; die Zivilbev&#246;lkerung und zivile Objekte sind zu
sch&#252;tzen und zu schonen. Dar&#252;ber hinaus d&#252;rfen gegnerische Kr&#228;fte nicht angegriffen werden, die au&#223;er
Gefecht (&#187;hors de combat&#171;) sind. Darunter fallen u. a. Soldaten, die ihre Intention, sich zu ergeben
angezeigt haben, und solche, die aufgrund einer Verletzung, Bewusstlosigkeit oder &#196;hnlichem nicht in der Lage
sind, sich zu verteidigen. Ein AWS, das v&#246;lkerrechtskonform eingesetzt werden soll, m&#252;sste also legitime
milit&#228;rische Ziele zuverl&#228;ssig und mit hoher Trefferquote unter realen Gefechtsbedingungen identifizieren
k&#246;nnen, auch bei ggf. ung&#252;nstigen Lichtverh&#228;ltnissen (Tag/Nacht), Wetter- und Umweltbedingungen, sich 
schnell &#228;ndernden Gefechtslagen, extremem Zeitdruck sowie trotz m&#246;glicher Gegenma&#223;nahmen des
Kontrahenten (einschlie&#223;lich Tarnen, T&#228;uschen, Blenden von Sensoren). Da auch Menschen bei dieser
komplexen Aufgabe regelm&#228;&#223;ig Fehler unterlaufen, wird man in der Praxis sicherlich auch bei AWS keine
Unfehlbarkeit bei der Identifikation legitimer Ziele erwarten k&#246;nnen. Die eigentliche Problematik ist
jedoch anders gelagert, denn f&#252;r die Entscheidung, ob Objekte oder Personen legitime milit&#228;rische Ziele
darstellen, reicht deren zuverl&#228;ssige Identifizierung bei Weitem nicht aus. Hierf&#252;r sind ein umfassenderes
Lagebild erforderlich sowie die Einsch&#228;tzung von Verhaltensweisen und letzten Endes der Intentionen des
Gegners. Aufgrund dessen sind Zweifel angebracht, dass softwarebasierte Systeme wie AWS in
absehbarer Zeit in der Lage sein werden, dem Unterscheidungsgebot des HVR Gen&#252;ge zu leisten.
&#8250; Prinzip der Verh&#228;ltnism&#228;&#223;igkeit: Das Unterscheidungsgebot stellt zwar hohe Anforderungen an
milit&#228;rische Angriffe, allerdings bedeutet dies nicht, dass generell von Angriffen abgesehen werden muss, wenn
dabei Zivilisten bzw. zivile Einrichtungen in Mitleidenschaft gezogen werden k&#246;nnten. Derartige
Kollateralsch&#228;den d&#252;rfen in Kauf genommen werden, allerdings nur in einem Umfang, der nicht exzessiv ist in
Relation zu dem konkreten und direkten milit&#228;rischen Nutzen der Operation. Die Anwendung dieser Norm
geh&#246;rt zu den schwierigsten Aufgaben im Kontext des HVR, denn es gilt, h&#246;chst unterschiedliche, im Kern
inkommensurable, also nicht vergleichbare Kategorien &#8211; milit&#228;rische und humanit&#228;re Ziele &#8211;
gegeneinander abzuw&#228;gen. Deswegen bestehen begr&#252;ndete Zweifel, ob softwaregest&#252;tzte Systeme wie AWS in der
Lage sein k&#246;nnten, die erforderlichen Abw&#228;gungen zur Verh&#228;ltnism&#228;&#223;igkeit eines Angriffs
zufriedenstellend und zuverl&#228;ssig durchzuf&#252;hren. Der daf&#252;r erforderliche Umfang an Kontext- und Weltwissen sowie
die F&#228;higkeit, die Aktionen von Menschen zu interpretieren und zu verstehen, d&#252;rften auf absehbare Zeit
nicht im Rahmen ihrer M&#246;glichkeiten liegen.
&#8250; Vorsorgeprinzip: Das Vorsorgeprinzip verpflichtet den Angreifer, dasjenige Mittel zu w&#228;hlen, das der
Zivilbev&#246;lkerung bzw. zivilen Objekten den geringsten Schaden zuf&#252;gt. Dar&#252;ber hinaus besteht die
Verpflichtung, die Zivilbev&#246;lkerung vor Angriffen zu warnen, die sie tangieren k&#246;nnen, soweit dem kein
gewichtiger Grund entgegensteht. Der Ma&#223;stab dieser Verpflichtung ist subjektiv, da die konkret in der
Situation verf&#252;gbaren Optionen abgewogen werden m&#252;ssen. Streitkr&#228;fte, die hochpr&#228;zise Wirkmittel zur
Verf&#252;gung haben, sind somit einem h&#246;heren Standard unterworfen als schlechter ausger&#252;stete Gegner ohne
diese M&#246;glichkeiten. Die Abw&#228;gungen gem&#228;&#223; dem Vorsorgeprinzip w&#228;ren von den hier beschriebenen
Prinzipien des HVR wohl am ehesten durch AWS erf&#252;llbar.
Die ethische Debatte
Im vorliegenden Bericht wird die weitverzweigte ethische Debatte &#252;ber AWS dargestellt. Im Fokus stehen drei
ma&#223;gebliche Diskussionsstr&#228;nge, die aus unterschiedlichen Blickwinkeln die Frage behandeln, ob und ggf.
inwiefern die eigentliche Bestimmung von AWS, n&#228;mlich die autonome Anwendung t&#246;dlicher Gewalt, moralisch
zul&#228;ssig ist.
Im Zentrum der ethischen Debatte &#252;ber AWS stehen Fragen, die die Implikationen dieser neuen
Waffentechnologie f&#252;r die Ethik des Krieges betreffen. Einer der wichtigsten theoretischen Bezugspunkte dabei ist die 
sogenannte Lehre vom gerechten Krieg, die antike Wurzeln hat und deren Ma&#223;st&#228;be Eingang in das HVR
gefunden haben. Ziel ist es, Kriterien zu finden, unter denen die Anwendung milit&#228;rischer Gewalt als Mittel der
Konfliktl&#246;sung im &#228;u&#223;ersten Fall gerechtfertigt werden kann. Bef&#252;rworter autonomer Waffentechnologie sehen
die Chance, dass AWS sogar zu einer Verbesserung der humanit&#228;ren Situation gegen&#252;ber dem Status quo
beitragen k&#246;nnten. So w&#228;ren sie diesem Ansatz zufolge dank ihrer &#252;berlegenen sensorischen und
datenverarbeitenden F&#228;higkeiten insgesamt besser als menschliche K&#228;mpferinnen und K&#228;mpfer in der Lage, zwischen
Kombattanten und unbeteiligten Zivilisten zu unterscheiden (Unterscheidungsgebot) und Waffengewalt so pr&#228;zise
und gleichzeitig zur&#252;ckhaltend einzusetzen, dass unverh&#228;ltnism&#228;&#223;ige Kollateralsch&#228;den ausgeschlossen w&#228;ren
(Verh&#228;ltnism&#228;&#223;igkeitsgebot). Kritiker wenden dagegen ein, dass diese Annahme auf der falschen Pr&#228;misse
beruhe, aus den komplexen und interpretationsbed&#252;rftigen Anforderungen an eine ethische Kriegsf&#252;hrung &#8211; wie 
sie beispielsweise in den v&#246;lkerrechtlichen Geboten zum Ausdruck kommen &#8211; lie&#223;en sich eindeutige
Verhaltensregeln ableiten, die eins zu eins in Programmcodes transferiert werden k&#246;nnen. Von der Frage, ob sich
ethische und rechtliche Erw&#228;gungen maschinell angemessen implementieren lassen, h&#228;ngt ab, ob ein ethisch
vertretbarer Einsatz von AWS prinzipiell m&#246;glich erscheint. Nicht zuletzt entscheidet sich dies auf der
technischen Ebene. Klar ist: Zum jetzigen Zeitpunkt ist der Entwicklungsstand in den relevanten Bereichen (KI,
Robotik, Sensorik etc.) bei Weitem noch nicht ausgereift genug, um ethisch vertretbare bzw. v&#246;lkerrechtskonforme
AWS zu konstruieren. Ob und inwiefern dies in der Zukunft anders sein wird, muss zum jetzigen Zeitpunkt
offenbleiben.
Im zweiten Diskussionsstrang stehen &#220;berlegungen im Vordergrund, die genuin ethischer Art sind, also
weniger auf empirischen Erw&#228;gungen beruhen und stattdessen st&#228;rker von normativen Vorgaben und
begrifflichen Festlegungen geleitet sind. Zur Debatte steht dabei der grunds&#228;tzliche Zweck derartiger Waffensysteme: 
die F&#228;higkeit, autonom &#252;ber Leben und Tod zu entscheiden. Dabei wird vor allem &#252;ber die Frage diskutiert, ob
und inwiefern die T&#246;tung von Menschen durch autonome Systeme mit der Menschenw&#252;rde vereinbar ist. Mit
der Idee der Menschenw&#252;rde, die in Deutschland und in vielen anderen freiheitlich-demokratischen
Gesellschaften als besonders sch&#252;tzenswerter Grundwert gilt, ist eine zentrale Verpflichtung verbunden: Der Mensch
darf nicht zum Objekt gemacht werden. In der Debatte wird das Argument vorgebracht, dass der Einsatz letaler
Gewalt durch AWS ethisch grunds&#228;tzlich inakzeptabel sei, weil er genau dies impliziere: Die Opfer w&#252;rden
entw&#252;rdigt, indem sie in einem rein technischen Prozess zu Zielobjekten degradiert w&#252;rden, ohne dass dabei
die Aussicht auf Achtung ihrer W&#252;rde best&#252;nde. Das Argument bringt die starken moralischen Vorbehalte zum
Ausdruck, die gegen&#252;ber einer solchen Dehumanisierung des Krieges bestehen. Es ist jedoch theoretisch und 
begrifflich sehr voraussetzungsreich und seine Reichweite entsprechend umstritten.
Schlie&#223;lich ist mit Blick auf AWS auch die Frage der Verantwortung von besonderer Brisanz. Denn je
autonomer Waffensysteme agieren, je weniger sie also direkter menschlicher Steuerung unterliegen, desto
weniger eindeutig lassen sich ihre Aktionen einem menschlichen Akteur zuordnen. Das wird immer dann
problematisch, wenn solche Systeme zivile und unverh&#228;ltnism&#228;&#223;ige milit&#228;rische Sch&#228;den anrichten, was selbst bei
v&#246;lkerrechtskonformen AWS nie ganz auszuschlie&#223;en w&#228;re. Wer tr&#228;gt dann die Verantwortung? Diese Frage
wird virulent, da es wenig sinnvoll erscheint, die Maschinen selbst zur Rechenschaft zu ziehen, zumindest
solange sie nicht &#252;ber Handlungsverm&#246;gen im menschlichen Sinne verf&#252;gen. Eine sich m&#246;glicherweise
abzeichnende Verantwortungsl&#252;cke hat zum einen rechtliche Implikationen, denn je mehr sich Waffensysteme
menschlicher Steuerung entziehen, desto schwieriger wird es, menschliche Entscheidungstr&#228;ger f&#252;r die von diesen
Systemen begangenen Vergehen oder Verbrechen zivil- oder strafrechtlich zur Rechenschaft zu ziehen. Zum
anderen stellen sich aber auch moralische Fragen, da die Entscheidung, einen anderen Menschen zu t&#246;ten, eine der
moralisch schwerwiegendsten ist, die man treffen kann. Insofern ist die Vorstellung, dass niemand f&#252;r eine
solche Entscheidung (bzw. die anschlie&#223;ende Tat) die moralische Verantwortung zu &#252;bernehmen h&#228;tte,
durchaus beunruhigend. Durch den Einsatz von AWS k&#246;nnte so eine Situation entstehen, in der sich das technisch
vermittelte T&#246;ten von Menschen wie ein zuf&#228;lliges Naturereignis oder ein Unfall ausnimmt. Es g&#228;be dann
niemanden mehr, der dies mit seinem Gewissen vereinbaren m&#252;sste, was als Grund daf&#252;r ins Feld gef&#252;hrt wird,
dass AWS ethisch verwerflich w&#228;ren und nicht eingesetzt werden sollten.
Insgesamt zeigt sich, dass, obwohl autonome Waffensysteme in ethischer Hinsicht durchaus kontrovers
diskutiert werden, die Zweifel an ihrer Zul&#228;ssigkeit und Legitimit&#228;t deutlich &#252;berwiegen. Allerdings ist es nicht 
Anspruch der Ethik, kategorische Urteile &#252;ber die ethisch-moralische Bedenklichkeit oder Unbedenklichkeit 
einer Technologie zu f&#228;llen. Die Ethik vermag die mit dem technologischen Wandel verbundenen normativen
Unsicherheiten nicht aufzul&#246;sen, sie kann sie nur reflektieren und zu kl&#228;ren versuchen.
Handlungsm&#246;glichkeiten zur pr&#228;ventiven R&#252;stungskontrolle
Pr&#228;ventive R&#252;stungskontrolle dient der Identifikation und Ausarbeitung von r&#252;stungskontrollpolitischen
Regulierungsans&#228;tzen f&#252;r zuk&#252;nftige, bisher nichtstationierte Waffensysteme, mit dem konkreten Ziel, der
destabilisierenden Wirkung von potenziellen R&#252;stungswettl&#228;ufen und den Gefahren milit&#228;rischer
Eskalationsmechanismen bereits im Vorfeld zu begegnen. Dies ist ein breitangelegter Prozess, der darauf abzielt, die m&#246;glicherweise
problematischen Konsequenzen technologischer Entwicklungen fr&#252;hzeitig zu erkennen, f&#252;r die in politischer
Verantwortung stehenden Entscheidungstr&#228;ger beurteilbar zu machen und durch Institutionen und Verfahren
auf nationaler und internationaler Ebene in ihren Risiken zu begrenzen.
Die Bem&#252;hungen um eine Regulierung von AWS speisen sich aus zwei zentralen Argumentationsstr&#228;ngen.
Der eine behandelt die Frage, ob der Einsatz von AWS mit den Prinzipien des humanit&#228;ren V&#246;lkerrechts
vereinbar w&#228;re. Die internationale Debatte dazu hat innerhalb der CCW eine ad&#228;quate Plattform gefunden. Der
andere schlie&#223;t die Auswirkungen der Entwicklung, Stationierung oder des Einsatzes von AWS f&#252;r die
internationale Sicherheit ein. Die zentrale Frage hier ist, wie mit m&#246;glichen sicherheitspolitischen Implikationen, wie
z. B. R&#252;stungsdynamiken, zwischenstaatlichen Spannungen oder strategischen Instabilit&#228;ten, umgegangen
werden kann. Der Austausch zu dieser Frage befindet sich ganz am Anfang und hat bisher noch kein geeignetes
internationales Forum gefunden.
F&#252;r beide F&#228;lle bestehen f&#252;r die deutsche Politik vielf&#228;ltige Ans&#228;tze, um angesichts schnell
voranschreitender technologischer Entwicklungen sowie verst&#228;rkter milit&#228;rischer R&#252;stungsbem&#252;hungen m&#246;gliche Risiken
einzuhegen und damit zur Krisenstabilit&#228;t und internationalen Sicherheit beizutragen.
Die M&#246;glichkeiten innerhalb der CCW aussch&#246;pfen
Um das von Deutschland und einer Reihe anderer Staaten und NGOs erkl&#228;rte Ziel einer &#196;chtung von
Waffensystemen zu erreichen, die dem Menschen die Entscheidungsgewalt &#252;ber Leben und Tod entziehen, ist es
ratsam, dass Deutschland sein bisheriges Engagement im Rahmen der CCW aktiv fortf&#252;hrt. Die gemeinsam mit 
Frankreich ergriffene Initiative, als ersten Schritt eine politische Erkl&#228;rung und das Bekenntnis zum HVR-
konformen Einsatz von AWS anzustreben, wurde international vielbeachtet.
Da f&#252;r Beschl&#252;sse im Rahmen der CCW die Einstimmigkeit aller Vertragsstaaten erforderlich ist, ist bei
den derzeit weit auseinanderliegenden Positionen der einzelnen Staaten abzusehen, dass es sich bei der
Konsensfindung um einen Prozess handelt, der einen langen Atem erfordert. Es ist daher zu erwarten, dass zun&#228;chst
nur kleine Schritte als Minimalkonsens vereinbart werden k&#246;nnen, z. B. in Form von eher weichen vertrauens-
und sicherheitsbildenden Ma&#223;nahmen.
Diese eher auf langfristige Erfolge angelegte Vorgehensweise birgt allerdings die Gefahr, dass mit
verstreichender Zeit die Entwicklung, Verbreitung oder sogar der Einsatz von AWS stark vorangeschritten sein
k&#246;nnte. Damit w&#252;rden Tatsachen geschaffen, die eine Einigung im Rahmen der CCW erschweren k&#246;nnten.
Engagement &#252;ber die CCW hinaus verbreitern
Insbesondere wenn die Fortschritte im Rahmen der CCW als zu langsam und/oder nicht hinreichend bewertet
werden, ist anzuraten, auch weitere M&#246;glichkeiten ins Auge zu fassen, mit denen der internationale Dialog
gest&#228;rkt, Vertrauen aufgebaut und dem Sicherheitsbed&#252;rfnis aller Staaten Rechnung getragen wird. Im
Gegensatz zur CCW w&#228;re hierf&#252;r ein Konsens aller relevanter Staaten keine Voraussetzung. Daher k&#246;nnten Initiativen
von ad hoc zu bildenden Koalitionen von Staaten, NGOs und internationalen Organisationen getragen werden,
die ein gemeinsames Ziel verfolgen und m&#246;glichst z&#252;gig auf eine Ausformulierung und Verabschiedung
entsprechender Regeln zur Einhegung der Risiken von AWS hinarbeiten. Dies k&#246;nnte eine Sogwirkung entfalten,
sodass sich sukzessive immer mehr Staaten dieser Vorgehensweise anschlie&#223;en. F&#252;r solch ein Vorgehen bietet 
der Prozess bis zur Umsetzung der Ottawa-Konvention2 in internationales Recht Orientierung, der im Ergebnis
zum v&#246;lkerrechtlichen Verbot von Antipersonenminen f&#252;hrte. Allerdings w&#228;re ein Abkommen zu AWS, dem
die r&#252;stungstechnisch f&#252;hrenden Staaten nicht beitreten w&#252;rden, sicherlich kaum hilfreich. Die
Wahrscheinlichkeit f&#252;r eine breite Partizipation m&#246;glichst aller relevanter Staaten k&#246;nnte ggf. durch die Wahl weniger
ambitionierter Ma&#223;nahmen und Verbotstatbest&#228;nde erh&#246;ht werden.
Noch weitreichender w&#228;re die M&#246;glichkeit, unilateral voranzugehen und einen Verzicht auf den Einsatz,
die Beschaffung und/oder die Entwicklung von AWS verbindlich und nachpr&#252;fbar zu erkl&#228;ren. Diese Strategie 
k&#246;nnte sich auf die in Bezug auf die Achtung der Menschenw&#252;rde vorgebrachten ethischen Argumente st&#252;tzen
und der herausgehobenen Rolle der Menschenw&#252;rde im Katalog der Grundrechte Rechnung tragen. Die
Glaubw&#252;rdigkeit solchen Handelns w&#252;rde gest&#228;rkt, wenn sie mit anderen Politikbereichen in Einklang gebracht wird.
Zu nennen sind insbesondere die Forschungsf&#246;rderung sowie die Exportpolitik. Auch hier w&#228;re ein Leitmotiv,
auf die Vorbildfunktion und eine eintretende Sogwirkung zu setzen, mit dem Ziel, dass weitere Staaten diesem
Vorgehen folgen.
Zur St&#228;rkung des internationalen Dialogs zu bislang vernachl&#228;ssigten Themen (u. a. R&#252;stungsdynamiken,
zwischenstaatliche Spannungen bzw. strategische Instabilit&#228;ten) sind diverse Formate bilateraler und
multilateraler Foren denkbar, die ggf. als Nukleus f&#252;r konkretere und verbindlichere &#220;bereink&#252;nfte dienen k&#246;nnten. Das
Design der Diskursformate k&#246;nnte sich an erfolgreiche Beispiele aus anderen Politikbereichen anlehnen. So
begann etwa das Wiener &#220;bereinkommen zum Schutz der Ozonschicht als relativ weiche Vereinbarung und 
gewann Verbindlichkeit erst im Laufe der Zeit vor allem durch das Montreal-Protokoll3 und dessen Zus&#228;tze.
Als geeignete Plattform f&#252;r den internationalen Austausch kommt z. B. auch die OECD (Organisation for
Economic Co-operation and Development) infrage. So w&#228;re etwa ein AWS-Forum analog zur BNCT (Working
Party on Biotechnology, Nanotechnology and Converging Technologies) denkbar, die sich f&#252;r eine
verantwortbare Entwicklung dieser Technologien einsetzt.
2 &#220;bereinkommen &#252;ber das Verbot des Einsatzes, der Lagerung, der Herstellung und der Weitergabe von Antipersonenminen und
&#252;ber deren Vernichtung
3 Montreal-Protokoll &#252;ber Stoffe, die zu einem Abbau der Ozonschicht f&#252;hren
Ein weiteres wesentliches Handlungsfeld ist die Revitalisierung der konventionellen R&#252;stungskontrolle.
Dies w&#228;re angesichts der heute vorherrschenden sicherheitspolitischen Lage auch losgel&#246;st vom Thema AWS
von enormer Bedeutung f&#252;r eine Entspannung. Allerdings sind die Bedingungen hierf&#252;r aktuell alles andere als
einfach &#8211; u. a. angesichts der Aufk&#252;ndigung des Mittelstrecken-Nuklearstreitkr&#228;fte-Vertrags (INF-Vertrag;
Intermediate-Range Nuclear Forces Treaty) zum Verbot landgest&#252;tzter Kurz- und Mittelstreckenraketen zum
August 2019 und der Ank&#252;ndigung von US-Pr&#228;sident Trump im April 2019, den Vertrag &#252;ber den Waffenhandel
(Arms Trade Treaty &#8211; ATT) nicht weiter zu unterst&#252;tzen. Die Ausarbeitung und die Umsetzung von
R&#252;stungskontrollvereinbarungen bedeuten einen langen und steinigen Weg. Die diffizile Aufgabe besteht darin,
nachvollziehbar und glaubw&#252;rdig eine rote Linie zu definieren, um erlaubte bzw. gew&#252;nschte milit&#228;rische
Anwendungen der k&#252;nstlichen Intelligenz und Robotik von nicht statthaften zu unterscheiden. Von enormer Bedeutung 
f&#252;r den Erfolg von R&#252;stungsbeschr&#228;nkungen ist die Konzeption verl&#228;sslicher Verifikationsmechanismen. Da
Autonomie eine im Wesentlichen softwarebasierte Funktion ist, sind viele traditionelle Mittel der Verifikation
(z. B. Inspektionen milit&#228;rischer Einrichtungen zur Feststellung der Einhaltung numerischer Obergrenzen f&#252;r
bestimmte Waffentypen) hierf&#252;r kaum geeignet.
Eine wichtige Rolle k&#246;nnte die Ausarbeitung von Exportregeln f&#252;r kritische Technologien spielen. Mit 
dem ATT, dem Wassenaar Arrangement4 (nachfolgend auch als Wassenaar-Abkommen bezeichnet) sowie dem
MTCR (Missile Technology Control Regime) existieren bereits Instrumentarien, die durch die Kontrolle der
Ausfuhr von G&#252;tern und Technologien die Proliferation bestimmter Waffen einschr&#228;nken. Teilweise fallen
bereits Komponenten und Technologien, die auch f&#252;r AWS genutzt werden k&#246;nnten, unter diese Abkommen oder
sie k&#246;nnten relativ leicht einbezogen werden. Gleichzeitig besteht die Schwierigkeit, dass die kritischen
Technologien, insbesondere auf dem Gebiet der KI, ganz wesentlich im kommerziellen Sektor entwickelt werden
und eine klare zivile und milit&#228;rische Doppelnutzbarkeit aufweisen (&#187;dual use&#171;). Die Herausforderung besteht 
darin, Regularien zu schaffen, die unerw&#252;nschte milit&#228;rische Nutzungen einschr&#228;nken, aber den zivilen Nutzen
der Technologien unangetastet lassen.
Fazit
Die zunehmende Nutzung von automatisierten oder zuk&#252;nftig autonomen Waffensystemen k&#246;nnte einen
Paradigmenwechsel darstellen, der die Kriegsf&#252;hrung im 21. Jahrhundert revolutionieren w&#252;rde. AWS werfen
zahlreiche Fragen auf, sowohl was ihre &#220;bereinstimmung mit den Prinzipien des humanit&#228;ren V&#246;lkerrechts angeht
als auch die Auswirkungen, die ihre Verbreitung und ihr Einsatz entfalten k&#246;nnten, gerade auch in Bezug auf
potenzielle R&#252;stungsdynamiken, die internationale Sicherheit sowie regionale und strategische Stabilit&#228;t. Die
internationale Staatengemeinschaft hat begonnen, sich dieser Themen anzunehmen.
Derzeit existiert ein Fenster von M&#246;glichkeiten, um mit einem international abgestimmten, zielgerichteten
Vorgehen die m&#246;glichen Gefahren einzuhegen, die AWS mit sich bringen k&#246;nnten. Dieses Fenster schlie&#223;t sich
sukzessive mit fortschreitender technologischer Entwicklung sowie der kontinuierlichen Integration autonomer
Funktionen in Waffensysteme aller Art. Damit werden Strukturen gefestigt und Fakten geschaffen, die
regulierende Eingriffe erschweren oder sogar verhindern. Dieses Fenster von M&#246;glichkeiten zu nutzen, ist keine
einfache Aufgabe, denn die Schwierigkeiten, die sich bei der R&#252;stungskontrolle im Hinblick auf AWS stellen, sind
gro&#223;. Im Lichte der Implikationen, mit denen die internationale Gemeinschaft durch autonome Waffensysteme
zuk&#252;nftig konfrontiert werden k&#246;nnte, erscheint es dringend geboten, diese Herausforderungen unverz&#252;glich 
anzugehen und L&#246;sungen zu entwickeln. Diesbez&#252;gliche politische und diplomatische Initiativen erfordern
einen langen Atem und einen breiten Diskurs unter Einbezug von Wissenschaft und Zivilgesellschaft.
Wassenaar Arrangement zu Exportkontrollen f&#252;r konventionelle R&#252;stungsg&#252;ter und Dual-Use-G&#252;ter (Waren, Software und
Technologie)
4 
        
 
 
  
 
   
  
   
  
    
  
     
  
     
      
   
   
    
    
  
  
   
     
  
  
    
    
  
   
   
     
     
       
   
 
     
 
 
      
       
  
     
   
        
    
  
  
   
    
 
1
Einleitung
Beeindruckende technologische Fortschritte im Bereich der k&#252;nstlichen Intelligenz erm&#246;glichen eine F&#252;lle
neuer Anwendungen, die im Begriff sind, s&#228;mtliche Wirtschafts- und Lebensbereiche zu durchdringen und
grundlegend zu transformieren. Diese Entwicklung macht auch vor dem Milit&#228;rsektor nicht Halt. Weltweit
laufen intensive Forschungs- und Entwicklungst&#228;tigkeiten, die darauf abzielen, den Grad der Autonomie
milit&#228;rischer Systeme sowie die milit&#228;rische Nutzung der KI zu steigern. Bereits heute sind unbemannte
Waffensysteme im Einsatz, die &#252;ber hochautomatisierte und autonome Funktionen verf&#252;gen, z. B. zur Navigation, zur
Zielerkennung oder zur Steuerung des Zielanflugs von Raketen. Bislang liegen jedoch die Zielauswahl, die
Angriffsentscheidung und schlie&#223;lich die Freigabe des Waffeneinsatzes noch in der Verantwortung eines
menschlichen Kommandeurs bzw. Operators.
Ein autonomes Waffensystem w&#228;re in der Lage, auch diese Schritte selbstt&#228;tig und ohne oder mit nur
minimaler menschlicher Mitwirkung durchzuf&#252;hren. Aus milit&#228;rischer Sicht ist dies vor allem aus zwei
Gr&#252;nden attraktiv: Erstens ben&#246;tigt ein autonomes System keine Kommunikationsverbindung mit einer Basisstation,
zweitens erlaubt es schnellere Reaktionszeiten in Gefechtssituationen, da keine Verz&#246;gerungen durch die
Laufzeiten einer Daten&#252;bertragung und durch die Entscheidungsfindung bzw. die Reaktionszeiten eines
menschlichen Operators auftreten. Die Steigerung der Autonomie von Waffensystemen steht daher in allen technologisch
fortgeschrittenen L&#228;ndern auf der Agenda und hat eine weltweite Debatte ausgel&#246;st.
Mit fortschreitender Autonomie von Waffensystemen wird vor allem eine zentrale Frage virulent: Kann es
ethisch vertretbar, politisch verantwortbar und (v&#246;lker)rechtlich erlaubt sein, die Entscheidung &#252;ber Leben und
Tod von Menschen an Maschinen zu delegieren? Oder anders gefragt: Wie muss die menschliche Kontrolle
bzw. Steuerung von Waffensystemen beschaffen sein, damit diese Waffen im Einklang mit ethischen,
politischen bzw. (v&#246;lker)rechtlichen Prinzipien eingesetzt werden d&#252;rfen?
Vor diesem Hintergrund hat der Ausschuss f&#252;r Bildung, Forschung und Technikfolgenabsch&#228;tzung das
B&#252;ro f&#252;r Technikfolgen-Absch&#228;tzung beim Deutschen Bundestag (TAB) 2017 beauftragt, ein TA-Projekt zum
Thema AWS durchzuf&#252;hren. Ziel der Untersuchung war es, technologische, milit&#228;risch-strategische,
v&#246;lkerrechtliche, ethische und nicht zuletzt r&#252;stungskontrollpolitische Aspekte von AWS aufzuarbeiten, um
Orientierungs- und entscheidungsrelevantes Wissen in diesem schwierigen und hochaktuellen Themenfeld
bereitzustellen. Gleichzeitig konnte so das Monitoring des Themenfelds &#187;Neue Technologien und R&#252;stungskontrolle&#171;
fortgesetzt werden, das 2001 vom Ausschuss f&#252;r Bildung, Forschung und Technikfolgenabsch&#228;tzung beschlossen
worden war. Bislang erschienen in dieser Reihe &#187;Milit&#228;rische Nutzung des Weltraums und M&#246;glichkeiten der
R&#252;stungskontrolle im Weltraum&#171; (TAB 2003) sowie &#187;Stand und Perspektiven der milit&#228;rischen Nutzung
unbemannter Systeme&#171; (TAB 2011).
Aufbau des Berichts
Zu Beginn werden Definitionsans&#228;tze f&#252;r AWS im &#220;berblick dargestellt (Kap. 2). Dabei zeigt sich, dass eine
klare und allgemein akzeptierte Abgrenzung von autonomen zu semiautonomen und hochautomatisierten
Waffensystemen bis heute nicht m&#246;glich ist.
In Kapitel 3 werden die technischen Grundlagen f&#252;r Autonomie behandelt. Es wird ein &#220;berblick &#252;ber den
technologischen Stand bei autonomen Funktionen aktueller milit&#228;rischer Systeme gegeben. Anschlie&#223;end
werden die M&#246;glichkeiten und Begrenzungen beleuchtet, die sich beim aktuellen Entwicklungsstand von KI und
maschinellem Lernen ergeben.
Im Anschluss werden in Kapitel 4 die Verbreitung und der Entwicklungsstand von unbemannten (semi)
autonomen Waffensystemen zu Land, zu Wasser und in der Luft beschrieben. Daraufhin folgt eine Analyse der
Forschungs- und Entwicklungstrends mit Bezug zu AWS. Ziel ist, eine Vorstellung von Entwicklungspfaden
zu vermitteln sowie Anhaltspunkte &#252;ber potenzielle F&#228;higkeiten von zuk&#252;nftigen AWS zu geben.
Auf Grundlage der erwarteten F&#228;higkeiten werden in Kapitel 5 m&#246;gliche milit&#228;rische Missionen und
denkbare Einsatzszenarien f&#252;r AWS aufgezeigt, die wiederum als Basis daf&#252;r dienen, sicherheitspolitische
Implikationen von AWS zu untersuchen (Kap. 6). Dabei stehen Fragen im Mittelpunkt, wie sich mit AWS die
Kriegsf&#252;hrung ver&#228;ndern k&#246;nnte bzw. welche Auswirkungen auf die globale und regionale Stabilit&#228;t sowie auf
R&#252;stungsdynamiken zu erwarten sind.
In Kapitel 7 wird der Rahmen beschrieben, den das humanit&#228;re V&#246;lkerrecht jeglichem legitimem
Waffeneinsatz setzt. Darauf aufbauend wird herausgearbeitet, welche Bedingungen und Grenzen daraus f&#252;r m&#246;gliche
Eins&#228;tze von AWS folgen.
Die ethische Debatte um AWS wird in Kapitel 8 anhand dreier Diskussionsstr&#228;nge dargestellt. Der erste
behandelt die m&#246;glichen Auswirkungen von AWS auf die Ethik der Kriegsf&#252;hrung, insbesondere im Hinblick
auf humanit&#228;re Folgen. Im zweiten wird der Frage nachgegangen, ob es mit der Menschenw&#252;rde zu vereinbaren
ist, die Entscheidung &#252;ber Leben und Tod an Maschinen zu &#252;bertragen. Im dritten schlie&#223;lich wird die Frage 
aufgegriffen, ob AWS eine Verantwortungsl&#252;cke schaffen, die mit rechtlichen sowie moralischen
Rechenschaftspflichten nicht in Einklang zu bringen ist.
In Kapitel 9 wird zun&#228;chst ein &#220;berblick &#252;ber R&#252;stungs- und Exportkontrollabkommen mit Relevanz f&#252;r
AWS gegeben. Im Anschluss werden die Diskussionen der internationalen Staatengemeinschaft &#252;ber die
m&#246;gliche Regulierung von AWS behandelt, die derzeit im Rahmen der CCW gef&#252;hrt werden. Zum Abschluss
werden Handlungsm&#246;glichkeiten aufgezeigt, mittels derer m&#246;gliche Risiken von AWS eingehegt werden k&#246;nnen.
Das Thema AWS ist hochaktuell und die Befassung damit w&#228;hrend der Projektdurchf&#252;hrung sowohl in 
der wissenschaftlichen als auch in der politischen Sph&#228;re deutlich Fahrt aufgenommen. Diese Dynamik hatte
zur Folge, dass einige bedeutsame Entwicklungen erst kurz vor Abschluss dieses Berichts Ende 2018 zu
verzeichnen waren. Dies betrifft in erster Linie die inhaltlichen Fortschritte, die beim Expertentreffen erzielt
wurden, das im Rahmen der CCW vom 25. bis 29. M&#228;rz 2019 in Genf stattfand. Aber auch der Abschlussbericht
des International Panel on the Regulation of Autonomous Weapons (iPRAW 2018b), die Ver&#246;ffentlichung der
KI-Strategie des US-Verteidigungsministeriums (DOD 2019) sowie diverse Initiativen der Trump-
Administration zu KI (U.S. President 2019) und zur R&#252;stungskontrolle (Aufk&#252;ndigung des INF-Vertrags sowie des ATT) 
sind hier zu nennen. Auch wenn es aus Zeitgr&#252;nden nicht mehr m&#246;glich war, alle diese Entwicklungen in der
erforderlichen Tiefe und Breite abzuhandeln, wurden relevante Aspekte in Form von Fu&#223;noten und Textk&#228;sten
kenntlich gemacht.
Zusammenarbeit mit Gutachtern und Danksagung
Zur fachlichen Fundierung dieses Berichts wurden drei Gutachten vergeben:
&#8250; Technologien f&#252;r autonome Waffensysteme &#8211; Stand und Perspektiven. J&#252;rgen Altmann, Mark Gubrud,
K&#246;ln
&#8250; Sicherheitspolitische Implikationen und M&#246;glichkeiten der R&#252;stungskontrolle autonomer Waffensysteme.
Christian Alwardt, Lina-Marieke Hilgert, G&#246;tz Neuneck, Johanna Polle, Institut f&#252;r Friedensforschung und 
Sicherheitspolitik an der Universit&#228;t Hamburg, Hamburg
&#8250; Ethische Fragestellungen im Kontext autonomer Waffensysteme. Dr. Bernhard Koch, Dr. Bernhard Rinke,
Institut f&#252;r Theologie und Frieden, Hamburg
Zahlreiche Elemente dieser Gutachten flossen in den vorliegenden Bericht ein. Die Analysen von Alwardt et al.
(2017) bilden eine wesentliche Grundlage f&#252;r die Kapitel 4 bis 6 sowie 9. Die Arbeiten von Altmann und
Gubrud (2017) liegen haupts&#228;chlich Kapitel 4 zugrunde, liefern aber auch f&#252;r andere Kapitel Informationen und
Materialien. Kapitel 8 zu den ethischen Fragestellungen profitiert wesentlich von der Expertise von Koch und 
Rinke (2017).
An dieser Stelle sei den Gutachterinne und Gutachtern f&#252;r die Bereitschaft zur Kooperation und
Kommunikation sowie die herausragende Qualit&#228;t der vorgelegten Gutachten herzlich gedankt. Die Verantwortung f&#252;r
die Auswahl, Strukturierung und Verdichtung des Materials sowie dessen Zusammenf&#252;hrung mit eigenen
Recherchen und Analysen tragen die Verfasser. Dank geb&#252;hrt auch Brigitta-Ulrike Goelsdorf f&#252;r die sorgf&#228;ltige 
redaktionelle Bearbeitung und Layoutgestaltung des Berichts.
        
 
 
  
    
    
  
   
  
  
 
    
 
   
   
    
 
   
  
    
   
    
    
  
   
 
  
 
  
     
   
    
    
    
  
    
     
 
     
  
  
     
   
    
        
      
 
2
2.1
Abgrenzung des Untersuchungsgegenstands
International werden diverse Ans&#228;tze verfolgt, um AWS zu definieren und von anderen Waffensystemen
abzugrenzen. Eine pr&#228;zise, allgemein akzeptierte Definition existiert bis heute nicht. Um den
Untersuchungsgegenstand des vorliegenden Berichts zu umrei&#223;en, ist eine solche Definition jedoch nicht notwendig. F&#252;r die hier
vorgenommene Analyse milit&#228;risch-strategischer, sicherheitspolitischer und ethischer Fragestellungen reicht es
aus, die hierf&#252;r jeweils relevanten Charakteristika der Waffensysteme plausibel zu beschreiben. Ob diese
Systeme unter eine bestimmte strenge Definition autonomer Waffensysteme fallen w&#252;rden oder nicht, kann
dahingestellt bleiben.
Bei der Erstellung dieses Berichts wurde daher mit einer deskriptiven Charakterisierung gearbeitet, nach
der ein AWS Auftr&#228;ge ohne bzw. mit lediglich schwacher externer menschlicher Kontrolle selbstst&#228;ndig
ausf&#252;hrt und die F&#228;higkeit besitzt, in einer komplexen, dynamischen Umgebung auf unvorhersehbare Ereignisse 
zielgerichtet reagieren zu k&#246;nnen. Nach Kenntnis und Einsch&#228;tzung des TAB existieren solche Waffensysteme
heute noch nicht. Aus der Analyse gegenw&#228;rtig existierender hochautomatisierter Waffensysteme und
wissenschaftlich-technologischer Entwicklungstrends k&#246;nnen jedoch konkrete Vorstellungen und Erwartungen
entwickelt werden, wie zuk&#252;nftige AWS aussehen und wann sie Realit&#228;t werden k&#246;nnten. Da dies ein sich sehr
dynamisch entwickelndes Themengebiet ist, muss allerdings damit gerechnet werden, dass diese Vorstellungen
auch rasch von der Realit&#228;t &#252;berholt werden k&#246;nnten. Daher sollten von diesem Bericht auch keine endg&#252;ltigen
Bewertungen erwartet werden, vielmehr ist eine kontinuierliche Beobachtung des Gebiets anzuraten.
Die Frage der Definition von AWS birgt allerdings eine erhebliche Brisanz, da sie oftmals in einen direkten
Zusammenhang mit m&#246;glichen Vereinbarungen zur R&#252;stungskontrolle gestellt wird (Kap. 9.2). Hier wird oft
implizit oder explizit angenommen, dass die Definition von AWS gleichzeitig den Rahmen setzt, welche
Systeme ggf. zu regulieren oder gar zu verbieten sind. Daher spiegelt die Haltung der Staaten und anderer Akteure
in definitorischen Fragen regelm&#228;&#223;ig deren Eigeninteressen und Verhandlungspositionen wider. Kontroversen 
um Definitionen stehen vielfach stellvertretend f&#252;r tiefere inhaltliche Gegens&#228;tze bzw. werden
instrumentalisiert, um den Verhandlungsprozess zu hemmen. Aus diesem Grund werden im Folgenden definitorische
Ans&#228;tze, Beispiele und Argumente vorgestellt und analysiert.
Definitorische Ans&#228;tze
F&#252;r die Definition von AWS existieren verschiedene Herangehensweisen. Sie gr&#252;nden erstens auf den
(technologischen) Charakteristika des Waffensystems, zweitens legen den verbliebenen Grad menschlicher Kontrolle
zugrunde, drittens orientieren sich an Funktionen oder am intendierten Einsatzspektrum des Waffensystems
oder viertens kombinieren mehrere dieser Kriterien. Jeder dieser Ans&#228;tze weist inh&#228;rente St&#228;rken, aber auch
Schw&#228;chen auf (UNIDIR 2017, S. 19 ff.):
1. Der technologiezentrierte Ansatz entspricht der konventionell in r&#252;stungskontrollpolitischem
Zusammenhang &#252;blichen Herangehensweise und w&#228;re somit als Grundlage entsprechender Vereinbarungen gut
geeignet. Allerdings birgt er das Problem, dass die genauen technischen Spezifikationen zuk&#252;nftiger AWS
in gewissem Umfang spekulativ bleiben m&#252;ssen. Au&#223;erdem k&#246;nnte eine Definition anhand
technologischer Kriterien durch wissenschaftlich-technische Fortschritte obsolet gemacht werden. Au&#223;erdem
existieren gravierende Abgrenzungsprobleme. Falls beispielsweise defensive AWS (beispielsweise nach Art
von &#187;MANTIS&#171;) als akzeptabel erachtet w&#252;rden, offensive dagegen nicht, w&#228;re dies durch eine
technologiezentrierte Definition nicht abzubilden, da das gleiche System lediglich abh&#228;ngig von den
Operationsbedingungen einmal defensiv und einmal offensiv wirken k&#246;nnte.
2. Der Ansatz, dessen Fokus auf den Grad menschlicher Kontrolle liegt, hat den Vorzug, dass er wesentliche
Anforderungen des humanit&#228;ren V&#246;lkerrechts aufgreift (Kap. 7) und gleichzeitig an den breitgeteilten
Grundsatz ankn&#252;pft, dass Menschen die Letztkontrolle &#252;ber den Einsatz t&#246;dlicher Waffengewalt behalten
sollen. Andererseits entsteht das Problem, dass Test, Evaluation bzw. Verifikation des Grads menschlicher
Kontrolle schwierig sind, weil hier im Prinzip der Mensch als Teil des Systems mitevaluiert werden
m&#252;sste.
        
 
 
  
    
    
   
  
   
     
      
      
  
  
     
  
 
  
   
   
 
    
     
     
  
      
  
  
    
 
  
 
     
  
     
   
    
    
  
   
                                              
  
       
2.2
3. Der an den Funktionen orientierte Ansatz basiert auf dem Konzept sogenannter kritischer Funktionen auf.
Damit sind die Zielauswahl (einschlie&#223;lich Detektion, Identifikation, Verfolgung, Priorisierung) und -
bek&#228;mpfung (d. h. der Waffeneinsatz) gemeint. Ein System, das diese Funktionen selbstt&#228;tig ausf&#252;llen kann,
w&#228;re demnach ein AWS. Dieser Ansatz hat den Vorzug, dass er recht einfach ist. Allerdings ist er sehr
breit angelegt, da zum einen auch existierende hochautomatisierte Systeme (ein Beispiel w&#228;re die
radarsuchende Drohne &#187;Harpy&#171;) damit erfasst w&#252;rden und zum anderen insbesondere f&#252;r die Zielauswahl
verschiedene r&#228;umlich verteilte Subsysteme (einschlie&#223;lich Global Positioning System &#8211; GPS) genutzt
werden, die dem AWS zugerechnet werden m&#252;ssten.
4. Eine sequenzielle Kombination der drei Ans&#228;tze hat das Potenzial, zu einer umfassenden pr&#228;zisen
Definition zu f&#252;hren. Nach einer Festlegung der angemessenen bzw. notwendigen menschlichen Rolle sowie der
Bestimmung der kritischen Funktionen k&#246;nnten technologische Fragen zielgerichtet angegangen werden.
Erfolgversprechend ist ein solches Vorgehen allerdings nur, wenn die Definitionsfrage klar getrennt
behandelt werden kann von der Frage, welche Kategorien von AWS m&#246;glicherweise reguliert oder gar
verboten werden sollen. Anderenfalls w&#252;rden absehbar strategische und politische Motive die Diskussion
bestimmen und eine Einigung unwahrscheinlich machen.
Autonom, semiautonom oder automatisiert?
Ein Knackpunkt bei den verschiedenen Definitionen ist die Frage, wie autonome von semiautonomen und 
(hoch)automatisierten Waffensystemen abgegrenzt werden sollen. Um sich dieser Problematik anzun&#228;hern,
m&#252;ssen zun&#228;chst einige zentrale Begriffe gekl&#228;rt werden.
Relativ unproblematisch sind die Begriffsbestandteile Waffe und System zu definieren. Eine Waffe ist ein
Ger&#228;t oder System, das eine Form von Materie, Energie (und m&#246;glicherweise Information beispielsweise bei 
Cyberwaffen, die jedoch im vorliegenden Bericht ausgeklammert bleiben) freisetzen oder lenken soll, die
funktionsunf&#228;hig machende, sch&#228;digende oder zerst&#246;rende Wirkungen auf Personen oder Dinge haben. Vor allem
im Zusammenhang mit der CCW wird oft von &#187;lethal5 autonomous weapons systems&#171; (LAWS/letale autonome
Waffensysteme) gesprochen, um deren t&#246;dliche Wirkung zu betonen. Die Verwendung des Begriffs System
betont, dass nicht nur das Wirkmittel bzw. der Tr&#228;ger des Wirkmittels betrachtet werden sollen, sondern der
Blick ge&#246;ffnet wird f&#252;r alle Komponenten, die f&#252;r das Funktionieren der Waffe von Bedeutung sind, also auch 
beispielsweise Startvorrichtungen, Steuerungs- und Navigationssysteme. Was im konkreten Fall zum AWS
hinzugerechnet und was abgegrenzt wird, ist allerdings stark vom Kontext abh&#228;ngig.
Nicht nur die &#246;ffentliche Debatte, sondern auch die Fachdiskussion krankt oftmals daran, dass der
verwendete Autonomiebegriff nicht hinreichend expliziert und/oder schwammig gehalten wird. Dies kann zu
gravierenden Missverst&#228;ndnissen f&#252;hren, etwa wenn kategorische Aussagen getroffen werden wie: &#187;Autonome
Waffensysteme m&#252;ssen verboten werden&#171;, aber nicht ausgef&#252;hrt wird, in welcher Bedeutung der Begriff autonom
genau verwendet wird. Dessen Bedeutungsspektrum ist sehr weit: Auf der einen Seite kann er im Sinne der
Philosophie der Aufkl&#228;rung als M&#246;glichkeit verstanden werden, selbstbestimmt in Freiheit und Vernunft nach
eigenen Gesetzen zu handeln (Kasten 2.1). Die Realisierung von Waffensystemen, die in diesem Sinne autonom
sind, ist nach allgemeinem Verst&#228;ndnis mindestens in absehbarer Zeit nicht zu erwarten. M&#246;glicherweise
bleiben solche terminator&#228;hnlichen Waffen auch dauerhaft der Dom&#228;ne der Science-Fiction vorbehalten.
Allerdings wird im englischsprachigen Fachdiskurs &#187;lethal&#171; oft nicht im Sinne von t&#246;dlich, sondern von physisch zerst&#246;rend
verwendet. Urspr&#252;nglich sollte der Zusatz &#187;lethal&#171; wohl nur Cyberwaffen ausschlie&#223;en.
5
Kasten 2.1 Der Autonomiebegriff aus philosophischer Sicht
Autonomie ist ein Schl&#252;sselbegriff der Philosophie und der Aufkl&#228;rung, der ma&#223;geblich durch die
moralphilosophischen Schriften Immanuel Kants gepr&#228;gt wurde (z. B. Pauen 2011). Gem&#228;&#223; Kant bedeutet Autonomie
im Kern die F&#228;higkeit zum moralischen Urteil, zu dem nur vernunftbegabte, freie Wesen imstande sind, die
sich die Gesetze ihres sittlichen Handelns selbst aufzuerlegen verm&#246;gen. Dazu reicht es nicht aus, nur in
einer Art Wirkkausalit&#228;t zu einer Bewegung oder dergleichen veranlasst zu sein; vielmehr geh&#246;rt dazu
wesentlich der intentionale Bezug auf das, was durch das Handeln bewirkt werden soll. Autonomie in diesem
Sinne ist gleichbedeutend mit der F&#228;higkeit, sein Handeln aus eigenem freiem Willen zu initiieren und mithin
f&#252;r seine Taten verantwortlich zu zeichnen.
Ein im Gegensatz dazu deutlich einfacherer Ansatz, Autonomie zu definieren, besteht darin, den Begriff
operationell zu fassen, d. h. als Eigenschaft eines (maschinellen) Vollzugs. Demnach sind Systeme dann
autonom, wenn sie Aktionen in komplexen Umwelten bzw. sich ver&#228;ndernden Umgebungen ohne st&#228;ndige
&#220;berwachung und Kontrolle insofern unabh&#228;ngig vom Menschen ausf&#252;hren k&#246;nnen. Aus
moralphilosophischer Sicht besteht der gro&#223;e und nicht zu &#252;berbr&#252;ckende Gegensatz zur kantischen Autonomiedefinition
darin, dass f&#252;r Kant Autonomie auf die Handlungsurheberschaft zielt, w&#228;hrend die operationelle
Autonomiedefinition dies gerade nicht voraussetzt. Auch bei den intelligentesten Robotern ist nicht davon auszugehen,
dass sie &#252;ber (Selbst-)Bewusstsein und einen freien Willen verf&#252;gen und dementsprechend, dass sie
verantwortlich handeln k&#246;nnen (Neuh&#228;user 2014).
Zwar findet sich in der Literatur auch das Argument, dass Maschinen irgendwann mindestens genauso
intelligent wie Menschen und insofern zumindest prinzipiell zu wahrer Autonomie und
Handlungsurheberschaft f&#228;hig sein werden (Bostrom 2014). Voraussetzung daf&#252;r w&#228;re allerdings, dass es gel&#228;nge, allgemeine 
menschliche Intelligenz (inklusive Bewusstsein) k&#252;nstlich nachzubilden (sogenannte starke KI) und nicht
nur Teilaspekte intelligenten Verhaltens zu simulieren (schwache KI; vgl. TAB 2016, S. 102 ff.). Dieses
Szenario ist zum jetzigen Zeitpunkt jedoch (noch) rein hypothetisch-spekulativer Natur.
Quelle: Koch/Rinke 2017, S. 34 f. u. 156 f.
Auf der anderen Seite des Spektrums kann Autonomie in rein funktionaler Hinsicht bedeuten, dass ein System
bestimmte Aufgaben ohne Einwirkung von au&#223;en erledigen kann. In dieser Bedeutung m&#252;sste aber auch eine
einfache Landmine autonom genannt werden. Auch sind Waffensysteme, die selbstst&#228;ndig navigieren bzw.
Ziele anhand bestimmter Kriterien suchen, identifizieren und auch bek&#228;mpfen k&#246;nnen, Stand der Technik.
Beispielsweise ist die israelische Drohne &#187;Harpy&#171; bereits seit vielen Jahren im Einsatz. Sie kann ein Gebiet
abfliegen, nach vorgegebenen Radarsignaturen suchen, deren Quelle lokalisieren sowie diese ansteuern und mit ihrem
Sprengkopf zerst&#246;ren. Nach dem eben beschriebenen funktionalen Verst&#228;ndnis von Autonomie w&#228;re somit
&#187;Harpy&#171; ein Paradebeispiel f&#252;r ein autonomes Waffensystem. Allerdings verf&#252;gt diese Drohne nur &#252;ber ein
geringes Ausma&#223; an implementierter Rechenleistung und folgt einem relativ starr vorgegebenen
Programmablauf. Daher wird sie h&#228;ufig als automatisiertes und nicht als autonomes Waffensystem bezeichnet. Eine
Landmine, die nach einer einfachen Wenn-dann-Logik operiert (in der Art: &#187;Wenn ein Gewicht h&#246;her als x kg
einwirkt, l&#246;se den Z&#252;nder aus&#171;), w&#228;re nach diesem Sprachgebrauch ein automatisches System.
Kasten 2.2 Anmerkung zum Sprachgebrauch
In der Diskussion um AWS wird regelm&#228;&#223;ig mit Begriffen operiert, die urspr&#252;nglich aus philosophischen,
psychologischen oder p&#228;dagogischen Zusammenh&#228;ngen stammen. Der zentrale Begriff &#187;autonom&#171; ist hier
an erster Stelle zu nennen, aber auch beispielsweise &#187;handeln&#171;, &#187;Entscheidungen treffen&#171;, &#187;Intelligenz&#171; oder
&#187;lernen&#171; geh&#246;ren in diese Kategorie. In ihrer Ursprungsbedeutung bezeichnen sie Zuschreibungen, die nur
in Bezug auf Menschen (und ggf. hochentwickelte Tiere) sinnvoll getroffen werden k&#246;nnen.
Bei deren Verwendung in Bezug auf technische Artefakte besteht die Gefahr, dass die durch die Begriffe
transportierten Konnotationen dazu f&#252;hren, dass den Artefakten gewollt oder ungewollt menschliche
Eigenschaften zugesprochen werden und/oder dass aus sprachlich naheliegenden, aber inhaltlich fragw&#252;rdigen
Analogien irref&#252;hrende Schlussfolgerungen gezogen werden.
Da es sinnvoll erscheint, den &#252;blichen Sprachgebrauch in der Fachdiskussion aufzugreifen, war auch im
vorliegenden Bericht die Verwendung der entsprechenden Begrifflichkeiten nicht ganz zu vermeiden. Beim
Lesen dieses Berichts &#8211; wie auch in der gesamten popul&#228;ren und Fachliteratur zu AWS &#8211; sollte man sich
dieser Problematik jedoch stets bewusst sein.
Die Begriffe automatisch, automatisiert und autonom befinden sich in dieser Reihenfolge auf einem Kontinuum
ansteigender Komplexit&#228;t, und eine trennscharfe Abgrenzung zwischen ihnen ist kaum m&#246;glich. Zur
Unterscheidung wurde beispielsweise vorgeschlagen, dass automatische Systeme in einer strukturierten,
vorhersagbaren Umgebung operieren, autonome dagegen auch in einer komplexen, unstrukturierten und sich
ver&#228;ndernden Umwelt sich zurechtfinden (Heyns 2013, S. 8); oder aber, dass die Aktionen automatisierter Systeme
vorhersehbar sind, diejenigen von autonomen Systemen jedoch nicht in jedem Einzelfall, sondern nur in ihrer
Gesamtheit (MOD 2017b, S. 13). Das US-Verteidigungsministerium definiert so: &#187;Autonomie ist die F&#228;higkeit
einer Entit&#228;t, unabh&#228;ngig verschiedene Vorgehensweisen zu entwickeln und auszuw&#228;hlen um Ziele zu
erreichen, basierend auf dem Wissen und Verst&#228;ndnis der Entit&#228;t &#252;ber die Welt, sich selbst und die Situation&#171;.6 
Solche Beschreibungen bieten etwas Orientierung, allerdings bleibt ein gewisser Graubereich, und es k&#246;nnen
immer Beispiele gefunden werden, f&#252;r die eine eindeutige Zuordnung nicht m&#246;glich ist.
Dar&#252;ber hinaus existieren diverse Ans&#228;tze, unterschiedliche Grade an Automatisierung bzw. Autonomie
zu definieren. Beispielsweise hat das US-amerikanische National Institute of Standards and Technology (NIST
2007 u. 2008) f&#252;r unbemannte Systeme eine Skala von 0 (ferngesteuert) bis 10 (vollst&#228;ndige intelligente
Autonomie) entwickelt. F&#252;r die Einordnung eines Systems auf dieser Skala wird in drei Kategorien &#8211; Komplexit&#228;t 
der Mission, Schwierigkeit der Umwelt sowie Mensch-Roboter-Interaktion &#8211; eine Bewertung vergeben, die
zum Gesamtergebnis aggregiert werden. In einem weiteren, eher schematischen Abgrenzungsansatz wird in
Abh&#228;ngigkeit von der Komplexit&#228;t der Einsatzumgebung und dem Grad der menschlichen Kontrolle &#252;ber das
System zwischen sieben Automatisierungslevels unterschieden, die das Spektrum von automatischen zu
autonomen Waffensystemen abdecken sollen (Alwardt/Kr&#252;ger 2016).
Ein wesentlich ausgefeilterer Versuch zur Abgrenzung autonomer Waffensysteme wurde im Rahmen des
Projekts &#187;Multidimensional Autonomy Risk Assessment&#171; (MARA) unternommen (Dickow et al. 2015a). Das
resultierende formelbasierte Instrument erlaubt es, jedes heutige bzw. in der Entwicklung befindliche
Waffensystem &#252;ber die Bestimmung von 15 Einflussgr&#246;&#223;en numerisch zu charakterisieren (MARA-Score), um in
einem direkten Vergleich vieler so eingestufter Systeme eine Bewertung hinsichtlich des Grades autonomer
F&#228;higkeiten vornehmen zu k&#246;nnen.
In einer Analyse solcher Ans&#228;tze, verschiedene Grade von Autonomie zu definieren, kam ein
Beratungsgremium des US-Verteidigungsministeriums allerdings zu dem Schluss, dass diese wenig hilfreich und teilweise
kontraproduktiv sind, da sie zu sehr auf eine Charakterisierung von Eigenschaften des technischen Systems
abzielen und dabei die viel bedeutsamere Art der Zusammenarbeit von Mensch und Maschine bei der
Missionsausf&#252;hrung aus dem Auge verlieren (DSB 2012, S. 4).
In diesem Zusammenhang wurde angeregt, ob es nicht produktiver sein kann, den Fokus der zuk&#252;nftigen
Diskussion auf &#187;Autonomie in Waffensystemen&#171; zu lenken anstatt wie bisher auf &#187;autonome Waffensysteme&#171;
als Waffenkategorie. Dies w&#252;rde implizieren, den vorherrschenden plattform- oder systemzentrierten Ansatz 
durch einen funktionalen Blickwinkel zu ersetzen (Boulanin/Verbruggen 2017, S. 118).
Im Original: &#187;Autonomy is defined as the ability of an entity to independently develop and select among different courses of action
to achieve goals based on the entity&#8217;s knowledge and understanding of the world, itself, and the situation.&#171; (DOD 2017b, S.17)
6 
        
 
 
   
     
     
      
   
        
  
  
   
    
  
  
 
    
   
  
   
 
   
 
    
    
  
  
   
 
      
     
    
   
  
     
       
      
   
 
                                              
    
      
    
  
       
  
 
                
            
2.3
Unterdessen ist zu beobachten, dass &#8211; angesichts der beschriebenen definitorischen Defizite bez&#252;glich
Autonomie einerseits und einer ablehnenden Haltung der &#214;ffentlichkeit gegen&#252;ber intelligenten Killerrobotern
andererseits &#8211; vor allem in milit&#228;rischen und sicherheitspolitischen Fachkreisen der Begriff AWS zunehmend
vermieden wird. Stattdessen ist h&#228;ufiger die Rede von Systemen, die automatisierte oder teilautonome
Funktionen aufweisen und im engen Verbund mit dem Menschen operieren. Schlagw&#246;rter, die in diesem
Zusammenhang verwendet werden, lauten &#187;manned-unmanned teaming&#171; (MUM-T) oder &#187;human-machine teaming&#171;
(DOD 2017b, S. 31 ff.; MOD 2018; Sadowski 2016; TARDEC 2017, S. 12 ff.).
Die Definition des US-Verteidigungsministeriums
Eine zentrale Rolle in der internationalen Debatte nimmt die Definition von AWS ein, die das US-
Verteidigungsministerium im Rahmen einer formellen Richtlinie zu Autonomie in Waffensystemen 2012 vorgestellt
hat. Dies war die erste Definition mit offiziellem Charakter, auf die seitdem immer wieder Bezug genommen
wird. Demnach ist ein autonomes Waffensystem ein Waffensystem, &#187;das nach seiner Aktivierung Ziele
ausw&#228;hlen und bek&#228;mpfen kann ohne weitere Einwirkung durch einen menschlichen Bediener. Dies schlie&#223;t von
Menschen &#252;berwachte AWS ein, die es menschlichen Bedienern erlauben, das System im Betrieb zu
&#252;berstimmen&#171;.7 Im Gegensatz dazu wird als semiautonomes Waffensystem definiert als ein Waffensystem, &#187;das nach
seiner Aktivierung daf&#252;r vorgesehen ist, lediglich einzelne Ziele oder spezifische Gruppen von Zielen, die von
einem menschlichen Bediener ausgew&#228;hlt wurden, zu bek&#228;mpfen&#171;.8 
Solange menschliche Kontrolle &#252;ber die Entscheidung, einzelne Ziele oder Gruppen von Zielen auszuw&#228;hlen,
gewahrt bleibt, sollen Waffensysteme auch dann semiautonom sein, wenn sie bestimmte Funktionen autonom
ausf&#252;hren k&#246;nnen. Es folgt eine Auflistung einiger dieser Funktionen (die nicht als abschlie&#223;end verstanden
werden soll):
&#8250; Erfassen, Verfolgen und Identifizieren m&#246;glicher Ziele
&#8250; menschliche Bediener auf m&#246;gliche Ziele aufmerksam machen
&#8250; ausgew&#228;hlte Ziele priorisieren
&#8250; Bestimmung des Zeitpunkts zu feuern
&#8250; Steuerung des Endanflugs auf ausgew&#228;hlte Ziele9 
Diese Unterscheidung zwischen vollautonomen und semiautonomen Waffensystemen ist nicht &#252;berzeugend, da 
aus technischer Sicht ein so definiertes semiautonomes Waffensystem alle F&#228;higkeiten h&#228;tte, um auch
vollst&#228;ndig autonom funktionieren zu k&#246;nnen. Beim entscheidenden Kriterium, n&#228;mlich, dass ein Mensch die Auswahl
des Ziels trifft, wird nicht weiter ausgef&#252;hrt, was das genau bedeutet (Gubrud 2015). Unter anderem wird nicht
definiert, wer als Bediener des AWS bezeichnet werden soll und welche Funktion und konkreten Aufgaben
dieser erf&#252;llt. Dies wirft die Frage auf, ob nicht beispielsweise auch ein Programmierer, der den Datensatz 
ausw&#228;hlt, anhand dessen das System die Zielerkennung trainiert (zur Bedeutung dieser Begrifflichkeit
Kap. 3.3.1), als Bediener aufgefasst werden kann (Roff 2015). Dies f&#252;hren Altmann und Gubrud (2017, S. 66)
zu der Schlussfolgerung: &#187;Es f&#228;llt schwer, ein milit&#228;risches Robotik- oder KI-Projekt zu identifizieren, das ein
AWS genannt werden k&#246;nnte, aber nicht auch halbautonom genannt werden k&#246;nnte. Es scheint, dass es keine 
bedeutsame Unterscheidung zwischen &#8250;halbautonomen&#8249; und &#8250;autonomen&#8249; Waffensystemen gibt.&#171;
7 Im Original: &#187;A weapon system that, once activated, can select and engage targets without further intervention by a human operator.
This includes human-supervised autonomous weapon systems that are designed to allow human operators to override operation of
the weapon system.&#171; (DOD 2012, S.13 f.)
8 Im Original: &#187;A weapon system that, once activated, is intended to only engage individual targets or specific target groups that have
been selected by a human operator.&#171; (DOD 2012, S.13 f.)
9 Im Original: &#187;This includes: Semi-autonomous weapon systems that employ autonomy for engagement-related functions including,
but not limited to, acquiring, tracking, and identifying potential targets; cueing potential targets to human operators; prioritizing
selected targets; timing of when to fire; or providing terminal guidance to home in on selected targets, provided that human control
is retained over the decision to select individual targets and specific target groups for engagement.&#171; (DOD 2012, S.13 f.)
        
 
 
  
      
 
  
   
     
    
   
  
   
   
     
     
  
   
     
   
  
 
  
  
 
 
 
 
 
 
  
 
     
   
  
    
    
  
       
   
     
     
 
  
                                              
         
2.4
&#220;berlegungen dieser Art haben dazu gef&#252;hrt, dass sich die Diskussion von technischen Charakteristika 
und anderen Versuchen, AWS zu definieren, tendenziell wegbewegt und stattdessen eher der Frage zuwendet,
welche Qualit&#228;t die Kontrolle von Waffensystemen durch Menschen aufweist bzw. aufweisen sollte.
Die Qualit&#228;t menschlicher Kontrolle &#252;ber AWS
&#187;Human in the loop&#171; (Mensch in der Entscheidungsschleife) ist ein zentraler Begriff, der h&#228;ufig im Kontext 
von Fragen zur Rolle des Menschen beim Einsatz von AWS und zum Ausma&#223; menschlicher Kontrolle
verwendet wird. Der Begriff setzt auf dem Konzept auf, dass Handlungsm&#246;glichkeiten im Zuge einer milit&#228;rischen
Operation in einer Entscheidungsschleife &#187;beobachten, orientieren, entscheiden, handeln&#171; entwickelt und
ausgef&#252;hrt werden. Um die Folgen der Handlung einsch&#228;tzen und darauf reagieren zu k&#246;nnen, wird sodann dieser
Zyklus erneut, ggf. mehrfach, durchlaufen.10 
Die Art und Weise, wie menschliche Kontrolle &#252;ber ein unbemanntes Waffensystem ausge&#252;bt wird, wird
h&#228;ufig in drei Kategorien eingeteilt: &#187;human in the loop&#171;, &#187;human on the loop&#171; und &#187;human out of the loop&#171;
(Mensch in, auf und au&#223;erhalb der Entscheidungsschleife; Tab. 2.1). Ein Waffensystem, bei dem der Mensch
au&#223;erhalb der Schleife ist, ist somit vollautonom zu nennen. Aber auch eines mit einem Menschen auf der
Schleife kann, wenn die Einflussm&#246;glichkeiten des Menschen nur gering sind, praktisch als vollautonomes
Waffensystem bezeichnet werden (HRW 2012, S. 2).
Tab. 2.1 Menschliche Rolle bei Zielauswahl und -bek&#228;mpfung
Bezeichnung menschliche Rolle
&#187;human in the loop&#171; Mensch gibt Befehl
Mensch in der Schleife
&#187;human on the loop&#171; Mensch &#252;berwacht und kann ggf.
Mensch auf der Schleife &#252;berstimmen
&#187;human out of the loop&#171; ohne (bzw. nur geringe) menschliche Mitwir-
Mensch au&#223;erhalb der Schleife kung
Quelle: HRW 2012, S. 2
Etwas differenzierter geht der Robotiker Noel Sharkey (2016) vor, indem er die Zielauswahl und die
Entscheidung f&#252;r einen Angriff getrennt betrachtet und je nachdem, welche Rollen hierbei Mensch und Maschine
spielen, f&#252;nf Kategorien definiert (Tab. 2.2). Kategorie 1 und 2 entsprechen der Zuschreibung &#187;in der Schleife&#171;,
wobei man argumentieren kann, dass Kategorie 1 im Zeitalter von computergest&#252;tzten Bildaufbereitungs- und 
Bilderkennungsverfahren in der Praxis nur noch eine untergeordnete Rolle spielt. Die Kategorien 3 und 4
entsprechen &#187;auf der Schleife&#171;, wobei hier der Unterschied zwischen einer aktiv zustimmenden und einer
passiveren, blo&#223; &#252;berstimmenden Rolle des Menschen liegt.
Eine der St&#228;rken des Konzepts &#187;in, auf oder au&#223;erhalb der Schleife&#171; und der Hauptgrund f&#252;r seine gro&#223;e
Verbreitung ist, dass es f&#252;r Laien intuitiv verst&#228;ndlich erscheint. Allerdings ist es bei Weitem nicht differenziert
genug, um das komplexe Verh&#228;ltnis von Menschen, die mit immer intelligenter werdenden Maschinen
interagieren, im Detail zu beschreiben.
Im englischen Sprachraum ist vom OODA-Loop (&#187;observe&#171;, &#187;orient&#171;, &#187;decide&#171;, &#187;act&#171;) die Rede (Wikipedia 2009b).10
Tab. 2.2 Rolle von Mensch und AWS bei Zielauswahl und Angriffsentscheidung
Kategorie Zielauswahl Angriffsentscheidung
1
2
3
4
5
Mensch
AWS schl&#228;gt vor,
Mensch w&#228;hlt aus
AWS
AWS
AWS
Mensch
Mensch
AWS, Mensch muss
zustimmen
AWS, Mensch kann ggf.
&#252;berstimmen
AWS
&#187;in der Schleife&#171;
&#187;auf der Schleife&#171;
&#187;au&#223;erhalb der Schleife&#171;
Quelle: Sharkey 2016
Hinzu kommt, dass es auf verschiedenen Ebenen multiple, teils ineinander verschlungene Schleifen gibt, die
zum Gro&#223;teil unter Mitwirkung komplexer technischer Systeme durchlaufen werden: von der Formulierung von
Kriegszielen auf eher milit&#228;rstrategischer Ebene, deren Priorisierung, der Missionsplanung und Vorauswahl
von Zielen bis hin zur Wahl der Wirkmittel sowie Durchf&#252;hrungsplanung auf taktischer Ebene. Welche davon
gemeint ist, wenn konstatiert bzw. gefordert wird, dass immer ein &#187;Mensch in der Schleife&#171; ist bzw. sein m&#252;sste,
ist vielfach unklar. Daher ist das Konzept der &#187;Schleife&#171; nicht ausreichend, um ein Mindestma&#223; an menschlicher
Kontrolle &#252;ber Waffensysteme zu definieren.
Wie schwierig es ist, dies pr&#228;zise zu fassen, wird in Kapitel 9 erl&#228;utert, wo die internationalen
Verhandlungen zu autonomen Waffensystemen im Rahmen der CCW beleuchtet werden. Dort spielt das Konzept der
menschlichen Kontrolle eine zentrale Rolle. Die konzeptionellen Schwierigkeiten spiegeln sich auf sprachlicher
Ebene in dem Bem&#252;hen, sich auf konkrete sprachliche Formulierungen zu einigen, z. B. in Forderungen nach 
&#187;meaningful human control&#171; oder einem &#187;appropriate level of human judgement&#171;.
Kasten 2.3 AWS in Platos H&#246;hle
Laut der Erkenntnistheorie, einem Teilgebiet der Philosophie, haben weder Menschen noch Maschinen einen
direkten Zugang zu Objekten in der realen Welt, sondern sie kennen sie nur vermittelt durch ihren
Wahrnehmungsapparat. Das hei&#223;t, die Repr&#228;sentationen, die in den K&#246;pfen und Datenr&#228;umen entstehen, sind die 
Wirklichkeit, wie wir (bzw. die Maschinen) sie erfahren. Und da uns die Welt nur mittelbar zug&#228;nglich ist,
besteht immer auch die M&#246;glichkeit, dass wir uns &#252;ber ihre Beschaffenheit irren k&#246;nnen. Aus diesem
Problem gibt es keinen Ausweg, wie bereits Plato mit seinem H&#246;hlengleichnis (siehe z. B. Wikipedia 2003a)
eindrucksvoll gezeigt hat. Die menschliche (wie die maschinelle) Wahrnehmung ist begrenzt, genau wie die
der in der H&#246;hle Gefangenen, die lediglich die Schatten an der H&#246;hlenwand sehen k&#246;nnen und diese Schatten 
f&#252;r die Wirklichkeit ansehen.
Im Kontext von AWS ist dies &#228;u&#223;erst relevant und durchaus keine m&#252;&#223;ige Metaphysik: Wenn die Ziele,
die ein AWS angreift, immer den Realweltobjekten oder -personen entspr&#228;chen, die Menschen anzugreifen
beabsichtigten, g&#228;be es wenig Grund zur Besorgnis. Tats&#228;chlich sind jedoch diese &#187;Ziele&#171; lediglich interne
Repr&#228;sentationen des Systems, die aus Sensordaten mit rechnergest&#252;tzten Verfahren erzeugt werden. Dabei 
muss das System mit Unsch&#228;rfen und Unsicherheiten umgehen, oft in der Form, dass vermeintlich kleine
Restgr&#246;&#223;en vernachl&#228;ssigt werden. Aus: &#187;Diese Infrarotsignatur entspricht zu 95 % der eines Panzers vom
Typ X&#171; wird dann: &#187;Dies ist ein gegnerischer Panzer&#171;. Um einem AWS mitzuteilen, welche Ziele es
angreifen soll, m&#252;ssen ihm zudem Zielspezifikationen &#252;bermittelt werden &#8211; durch einen menschlichen Bediener
oder durch ein anderes System. Auch dies ist ein Prozess, der vielf&#228;ltige Gelegenheiten f&#252;r Fehler bietet.
Da am Ende der Kette die Einwirkung von Waffengewalt auf Objekte bzw. Personen in der realen Welt 
steht, bleibt nur die &#8211; wie ausgef&#252;hrt &#8211; m&#246;glicherweise tr&#252;gerische Hoffnung, dass es eine gen&#252;gend starke
Kopplung gibt zwischen der Modellwelt des Systems, das Gewalt anwendet, und der realen Welt, in der die 
Gewaltanwendung geschieht.
Quelle: Altmann/Gubrud 2017, S. 48 ff.; Gubrud 2015
        
 
 
  
 
  
    
    
 
    
  
   
      
   
   
  
   
  
     
    
   
 
    
 
    
   
  
   
     
  
   
    
 
  
   
   
 
  
    
      
   
   
     
  
 
3
3.1
Technische Grundlagen von Autonomie
Obwohl die aktuell zu beobachtende gesteigerte Leistungsf&#228;higkeit unbemannter Systeme auch erheblich durch
die j&#252;ngsten Fortschritte in hardwarebasierten Technologiefeldern (Leichtbau, Miniaturisierung von
Komponenten, neue Materialien, Antriebstechnik, Energieversorgung, Computer- und kommunikationstechnische
Hardware, Sensoren, Aktuatoren) bedingt ist (einen &#220;berblick zu diesen Technologiefeldern gibt TAB 2011),
wird der Schritt zu autonomen Systemen entscheidend durch Software getrieben (&#187;autonomy is primarily a 
software endeavour&#171;) (DSB 2012, S. 22).
Nach einem Blick auf Autonomie aus informationstechnischer Sicht werden im Folgenden autonome
Funktionen in aktuellen milit&#228;rischen Systemen n&#228;her betrachtet. F&#252;r die weitere Entwicklung von Autonomie
in Waffensystemen sind computer- und datengest&#252;tzte Verfahren wesentlich, die unter dem Schlagwort
k&#252;nstliche Intelligenz subsumiert werden k&#246;nnen. Hierzu geh&#246;ren insbesondere moderne Verfahren des maschinellen
Lernens, deren M&#246;glichkeiten und Grenzen hinsichtlich ihrer Leistungsf&#228;higkeit thematisiert werden.
Autonomie aus informationstechnologischer Sicht
Aus informationstechnologischer Sicht ist Autonomie die F&#228;higkeit eines rechnergest&#252;tzten Systems,
selbstst&#228;ndig aus Daten, die sowohl durch Programmierung vorgegeben als auch aus der Umwelt mittels Sensoren 
gewonnen sein k&#246;nnen, zielgerichtete Pl&#228;ne und Aktionen zu generieren. Hierf&#252;r m&#252;ssen Wahrnehmung,
Entscheidungsfindung sowie Ausf&#252;hrung von Aktionen integriert werden (Boulanin/Verbruggen 2017, S. 7 ff.).
Wahrnehmung besteht aus den beiden Schritten Datensammeln mittels diverser Sensoren (z. B. optische,
akustische, chemische) sowie Zusammenf&#252;hrung und Auswertung dieser Daten mittels Analysesoftware. Die
Identifizierung von Objekten erfolgt meist durch Mustererkennung bzw. Abgleich mit vorgegebenen Profilen.
Dies kann an Bord eines AWS oder aber durch externe Systeme erfolgen.
Die Entscheidungsfindung durch das Kontrollsystem eines autonomen Systems kann relativ simpel sein
und beispielsweise auf einfachen Wenn-dann-Regeln beruhen, wie bei einer Landmine, die explodiert, wenn
das auf ihr lastende Gewicht einen vorbestimmten Schwellenwert &#252;berschreitet. Bei ausgefeilteren Systemen
ist ein Datensatz hinterlegt, der die Umwelt repr&#228;sentiert nebst Regeln, wie diese sich mit oder ohne Einwirkung
durch das AWS ver&#228;ndert (Weltmodell). Hinzu kommt eine Belohnungsfunktion, die angibt, inwieweit ein
gew&#252;nschter Zustand realisiert worden ist (z. B. die energieeffizienteste Route zu einem Ziel), sowie ein System,
das Handlungsalternativen generiert und diese mit der Belohnungsfunktion abgleicht, um die beste M&#246;glichkeit 
zu ermitteln. Solche Systeme k&#246;nnen &#228;u&#223;erst flexibel sein. Insbesondere ist es nicht erforderlich, alle
Eventualit&#228;ten, mit denen das System im Betrieb konfrontiert werden k&#246;nnte, in der Designphase des Systems
vorzugeben.
Aktionen k&#246;nnen durch physische Aktuatoren (z. B. Motoren) vermittelt werden. Dies sind quasi die
Muskeln, die es den Endeffektoren (dies k&#246;nnen R&#228;der, Beine, Fl&#252;gel, aber auch Waffen sein) erlauben,
physikalische Kraft auf die Umgebung auszu&#252;ben. Es sind allerdings auch virtuelle Aktuatoren denkbar, z. B.
Softwareprogramme, die bestimmte Aktionen durchf&#252;hren wie beispielsweise das Blockieren eines Schadcodes bei
einem Virenschutzprogramm.
Ganz wesentlich f&#252;r autonomes Agieren sind Lern- und Adaptationsmechanismen, die ein System bef&#228;higen,
sich an ver&#228;ndernde Umgebungen anzupassen bzw. neue Verhaltensweisen zu erwerben, damit es robust und 
unfallfrei auf neue Situationen oder pl&#246;tzliche St&#246;rungen reagieren kann (TAB 2016, S. 102).
Autonome Funktionen aktueller milit&#228;rischer Systeme
Autonome Funktionen werden derzeit bereits in diversen milit&#228;rischen Systemen genutzt. Im Folgenden wird
der Stand der Technik bei bereits eingesetzten Systemen sowie in FuE-Projekten in vier relevanten Bereichen
n&#228;her beleuchtet: Mobilit&#228;t, Zielerkennung bzw. Zielbestimmung, Informationsgewinnung sowie F&#228;higkeit zur
Zusammenarbeit.
3.2
3.2.1 Autonomie f&#252;r Mobilit&#228;t
Eine Funktion im Bereich Mobilit&#228;t, die heutzutage routinem&#228;&#223;ig autonom ausgef&#252;hrt werden kann, ist das
Verfolgen eines vorausfahrenden (bzw. -fliegenden oder -gehenden) Fahrzeugs oder eines Soldaten (oft &#187;follow
me&#171; genannt). Technisch &#228;hnlich ist dies dem &#187;homing in&#171;, d. h. dem Anflug auf ein vorbestimmtes Ziel, das
von vielen modernen Flugk&#246;rpern selbstst&#228;ndig durchgef&#252;hrt werden kann. F&#252;r beides ist die Voraussetzung,
dass das entsprechende Ziel bzw. das F&#252;hrungsfahrzeug zuverl&#228;ssig identifiziert und getrackt wird. Eine zweite
Funktion, bei der die Autonomisierung bereits weit fortgeschritten ist, ist der Start bzw. die Landung von
Flugger&#228;ten. Augenscheinlich ist der Stand der Technik in diesem Bereich so weit, dass menschliche Piloten
hinsichtlich Pr&#228;zision und Zuverl&#228;ssigkeit von autonomen Systemen &#252;bertroffen werden (Boulanin/Verbruggen 
2017, S. 23). Mit der &#187;X-47B&#171; (Northrop Grumman 2015) wurden autonome Starts und Landungen auf einem
Flugzeugtr&#228;ger auf hoher See demonstriert. Moderne UAVs, wie der &#187;Global Hawk&#171;, fliegen in allen Phasen
autonom &#8211; bis hin zur Fortbewegung auf dem Rollfeld (&#187;taxiing&#171;) (Williams 2006, S. 4). Sense-and-avoid-
Systeme zur Vermeidung von Kollisionen mit anderen Flugobjekten sind u. a. f&#252;r die Erlaubnis, in zivilem
Luftraum zu fliegen, unerl&#228;sslich. Derzeit st&#246;&#223;t die Technik bei un&#252;bersichtlichen Situationen mit einer Vielzahl
von zu beachtenden Objekten noch an Grenzen. An einer Verbesserung dieser Systeme wird intensiv geforscht.
Die wichtigste F&#228;higkeit f&#252;r die selbstst&#228;ndige Mobilit&#228;t eines AWS ist Navigation, d. h. die Bestimmung
des gegenw&#228;rtigen Aufenthaltsorts, die Planung einer Route im Einklang mit der Mission sowie die Verfolgung
dieser Route. Im einfachsten Fall bewegt sich das System von einem vorgegebenen Punkt zum n&#228;chsten (&#187;
waypoint navigation&#171;). Neuere Systeme k&#246;nnen ohne vorgegebene Wegmarken selbstst&#228;ndiger planen, jedoch mit 
vom Operator vorgegebenen Parametern (z. B. maximale Geschwindigkeit, Flugh&#246;he).
3.2.2 Autonomie f&#252;r Zielerkennung/Zielbestimmung
Die simpelste Form der Zielerkennung wird seit den 1970er Jahren in automatisierter Zielerkennungssoftware
implementiert. Sie beruht auf der Detektion von Signalen (z. B. Radar, optisch, akustisch) und dem Vergleich
mit abgespeicherten Signaturen. Werden mehrere Ziele detektiert, k&#246;nnen sie mit vordefinierten Kriterien (je
nach Einsatzbedingungen) priorisiert werden. Solche Systeme k&#246;nnen das v&#246;lkerrechtliche
Unterscheidungsgebot lediglich auf primitive Weise ber&#252;cksichtigen (indem nur Ziele identifiziert werden, die der Signatur
entsprechen, die charakteristisch f&#252;r das intendierte milit&#228;rische Ziel ist). Dar&#252;ber hinausgehende
Anforderungen des HVR k&#246;nnen durch sie nicht erf&#252;llt werden (u. a. die Einsch&#228;tzung, ob sich das Ziel &#187;hors de combat&#171; 
befindet oder ob unverh&#228;ltnism&#228;&#223;ig gro&#223;e zivile Verluste zu bef&#252;rchten sind, Kap. 7.2).
Derzeit wird an KI-gest&#252;tzten Systemen zur Zielerkennung gearbeitet, um die Limitationen bestehender
Software zu umgehen; so z. B. im Programm &#187;Target Recognition and Adaption in Contested Environments&#171;
der Defense Advanced Research Projects Agency (DARPA o. J.a).11 Aus zwei Gr&#252;nden sind die Fortschritte in
diesem Bereich noch eher moderat: Zum einen werden hierf&#252;r gro&#223;e und qualitativ hochwertige Datens&#228;tze
ben&#246;tigt, die alle m&#246;glichen Variationen der Einsatzbedingungen abdecken, um die Systeme zu trainieren. Diese
Daten sind in der erforderlichen Menge und Qualit&#228;t schwierig zu erzeugen bzw. zu beschaffen. Zum anderen
bestehen erhebliche Schwierigkeiten hinsichtlich der Vorhersagbarkeit und Verl&#228;sslichkeit der durch KI-
Systeme generierten Klassifikationen.
3.2.3 Autonomie f&#252;r Informationsgewinnung
Die automatisierte Detektion von Objekten und Ereignissen ist etablierter Stand der Technik, sofern die Aufgabe
klar strukturiert ist: z. B. die automatische Erkennung von Minen oder Sprengfallen (&#187;improvised explosive
device&#171; &#8211; IED) oder das Aufsp&#252;ren von Eindringlingen in ein Sperrgebiet (z. B. das UGS &#187;Mobile Detection
Assessment and Response System&#171; &#8211; MDARS). Auch die Ermittlung des Ursprungsorts von Gewehr/
Gesch&#252;tzfeuer kann mit hoher Pr&#228;zision erfolgen.
Weit verbreitet, insbesondere bei Unterwassersystemen und zunehmend auch bei fliegenden Systemen, ist
die F&#228;higkeit, aus Sensordaten (z. B. optische, Ultraschall, Radar) dreidimensionale Karten der Umgebung zu
erstellen. Moderne Systeme zur Raketenabwehr wie das israelische &#187;Iron Dome&#171; k&#246;nnen Bedrohungsanalysen
11 Die DARPA ist eine dem US-Verteidigungsministerium unterstellte Agentur f&#252;r Forschungsprojekte im Verteidigungsbereich.
anhand vorgegebener Kriterien autonom durchf&#252;hren und anhand des vorausberechneten Einschlagortes der
Raketen bzw. Granaten angepasste Gegenma&#223;nahmen vorschlagen. Wenn die Bedrohung als gering
eingesch&#228;tzt wird, weil beispielsweise keine milit&#228;rische oder zivile Einrichtung im Detonationsgebiet liegt, kann zur
Einsparung von Munition auch darauf verzichtet werden, den Flugk&#246;rper zu bek&#228;mpfen (Raytheon Missiles &amp;
Defense o. J.a).
In offeneren Situationen sind die autonomen F&#228;higkeiten zur Objekterkennung in heute genutzten
Systemen dagegen noch eher rudiment&#228;r. So kann beispielsweise das UAV &#187;ScanEagle&#171;, das zur Detektion von
milit&#228;risch relevanten Objekten auf See verwendet wird, aktuell lediglich Wasser von Nichtwasser
unterscheiden. Eine n&#228;here Bestimmung oder Unterscheidung von Klassen von Objekten ist nicht m&#246;glich (Boulanin/
Verbruggen 2017, S. 28). Die meisten heute genutzten Systeme verf&#252;gen nicht &#252;ber die F&#228;higkeiten zur
weitergehenden Analyse der gewonnenen Sensordaten. Diese m&#252;ssen &#252;ber eine breitbandige Datenverbindung an das
Lagezentrum &#252;bermittelt werden, wo die Auswertung &#252;berwiegend durch menschliche Fachkr&#228;fte
vorgenommen wird, was bei der riesigen Menge an typischerweise anfallendem Bild- bzw. Videomaterial eine gro&#223;e
Herausforderung ist.
Eine Entwicklung der j&#252;ngeren Zeit sind Big-Data-Analysen zum Auffinden von Mustern in gro&#223;en,
heterogenen Datenbest&#228;nden. Aufgrund der hohen Anforderungen an die Rechnerleistung werden derartige
Analysen &#252;blicherweise nicht an Bord von mobilen Systemen durchgef&#252;hrt, sondern in station&#228;ren Rechenzentren.
Berichten zufolge sollen beispielsweise die USA die Metadaten von 55 Mio. pakistanischen Mobiltelefonen
ausgewertet haben, um aufgrund bestimmter Muster Al-Kaida-Mitglieder bzw. -Kuriere zu identifizieren, zu
lokalisieren und auf dieser Grundlage Drohnenangriffe zu planen (Robbins 2016).
3.2.4 Autonomie f&#252;r die F&#228;higkeit zur Zusammenarbeit
Hierunter fallen alle F&#228;higkeiten, die es einem System gestatten, in Verbindung mit einem anderen technischen
System (&#187;machine-machine teaming&#171;) oder einem menschlichen Operator (&#187;human-machine teaming) zu
operieren.
Eine bereits in der Praxis genutzte Basisfunktion des &#187;machine-machine teaming&#171; ist die F&#228;higkeit,
Informationen zu teilen: Die Systeme sind vernetzt und nutzen Sensor- und andere Daten gemeinsam, arbeiten
ansonsten aber unabh&#228;ngig voneinander. Deutlich anspruchsvoller sind Formen der kollaborativen Autonomie,
d. h., mehrere Systeme koordinieren ihre Aktionen, um ein gemeinsames Ziel zu verfolgen. Hierf&#252;r ist eine 
&#252;bergeordnete Steuerung des kollektiven Systems (&#187;system of systems&#171;) notwendig. Experten gehen davon aus,
dass dies in den n&#228;chsten Jahren realisiert werden wird und zu neuen operativen F&#228;higkeiten autonomer Systeme
(Stichwort: Schwarmtaktiken) f&#252;hren kann (Arquilla/Ronfeldt 2000; Scharre 2014b).
Einige Ans&#228;tze zur Kollaboration werden aktuell entwickelt, die sich derzeit vorwiegend im Versuchs-
und Demonstrationsstadium befinden:
&#8250; Koordinierte Bewegung in unterschiedlichen Formationen wurde beispielsweise in Flugtests des US-
amerikanischen UAV &#187;UTAP-22&#171; der Kratos Defense &amp; Security Solutions, Inc. demonstriert (White 2015).
&#8250; In einigen FuE-Projekten wird sich mit kollaborativen gro&#223;r&#228;umigen Aufkl&#228;rungsmissionen
beispielsweise durch eine gro&#223;e Anzahl kleiner, kosteng&#252;nstiger UAVs, u. a. im Projekt &#187;Perdix&#171;, befasst. Bei einer
Demonstration dieses Projekts gelang es, 103 Mikrodrohnen aus einem fliegenden Kampfjet heraus
freizusetzen, die anschlie&#223;end als koordinierter Schwarm diverse Man&#246;ver durchf&#252;hrten (DOD 2017c).
&#8250; Anti-Access-and-Area-denial-Man&#246;ver (A2/AD) wurden beispielsweise im US-amerikanischen Projekt 
&#187;Control Architecture for Robotic Agent Command and Sensing&#171; (&#187;CARACaS&#171;) demonstriert: In einem
Feldversuch kooperierte eine Flotte von 13 Booten, &#252;berwacht von lediglich einem menschlichen
Bediener, um ein gegnerisches Boot zu identifizieren, zu stellen und einzukreisen (Tucker 2016).
&#8250; Entwickelt werden auch Funktionen f&#252;r koordinierte Angriffe, bei denen z. B. Ziele zwischen mehreren
beteiligten AWS untereinander verteilt werden bzw. das Vorgehen abgestimmt wird. Beispielsweise soll
der f&#252;r Eins&#228;tze gegen Schiffe in Entwicklung befindliche US-amerikanische Antischiffsflugk&#246;rper (&#187;long 
range anti-ship missile&#171; &#8211; LRASM) &#252;ber solche F&#228;higkeiten verf&#252;gen (Lockheed Martin o. J.).
        
 
 
   
  
    
   
   
   
  
    
    
  
       
   
  
   
   
 
   
    
   
      
 
  
  
    
   
     
 
    
 
  
   
 
   
  
       
 
      
      
   
   
         
 
     
 
  
                                              
     
     
3.3
F&#252;r &#187;human-machine teaming&#171; relevante F&#228;higkeiten sind die bereits erw&#228;hnten koordinierten Bewegungen,
wie das &#187;follow me&#171; oder der Formationsflug. Dar&#252;ber hinaus k&#246;nnen heutige Systeme relativ einfache
vorprogrammierte Man&#246;ver ausf&#252;hren, etwa das Markieren von Zielen, die Aufkl&#228;rung und Bewertung der Wirkungen
von ausgef&#252;hrten Schl&#228;gen oder das Abfeuern von Waffen auf Befehl des Piloten. Dar&#252;ber hinausgehende, in 
Richtung echte (Peer-to-peer-)Kooperation gehende F&#228;higkeiten sind noch in einem relativ fr&#252;hen
Entwicklungsstadium. Die Hauptschwierigkeit, die hierf&#252;r &#252;berwunden werden muss, sind die Beschr&#228;nkungen, denen
die Mensch-Maschine-Kommunikation unterliegt. Daher sind verbesserte visuelle bzw. taktile Interfaces
Gegenstand der Forschung. Die K&#246;nigsdisziplin, an der Maschinen bislang noch scheitern, ist das Verst&#228;ndnis von
nat&#252;rlich gesprochener Sprache &#252;ber simple Befehlsphrasen hinaus. Trotz beeindruckender Fortschritte in der
Computerlinguistik beispielsweise beim Umsetzen von gesprochener in geschriebene Sprache oder bei
&#220;bersetzungen sind Maschinen nach wie vor nicht in der Lage, einem Gespr&#228;ch zwischen Menschen zu folgen und
auf abstrakter Ebene zu verstehen, wovon die Rede ist (Krischke 2018).
K&#252;nstliche Intelligenz als Schl&#252;sseltechnologie
F&#252;r die Fortentwicklung der allermeisten zuvor beschriebenen F&#228;higkeiten in Richtung weitergehender
Autonomie werden derzeit prim&#228;r Methoden angewandt, die unter dem Oberbegriff k&#252;nstliche Intelligenz subsumiert 
werden k&#246;nnen. Der Begriff KI ist nicht unproblematisch, da damit computergest&#252;tzte Verfahren
vermenschlicht und Konnotationen transportiert werden, die f&#252;r eine rationale Diskussion der M&#246;glichkeiten und
Begrenzungen der technologischen Leistungsf&#228;higkeit jetzt und in absehbarer Zukunft nicht hilfreich sind.12 Wegen
der gro&#223;en Verbreitung und Anschlussf&#228;higkeit an die aktuelle wissenschaftliche und gesellschaftliche Debatte
wird der Begriff in dem hier vorliegenden Bericht dennoch verwendet. Dass auch einige Informatiker sich mit
dem Begriff etwas unwohl f&#252;hlen, illustriert das Bonmot: &#187;Sobald etwas funktioniert, h&#246;ren wir auf, es &#8250;KI&#8249; zu
nennen, sondern sagen stattdessen &#8250;Software&#8249;&#171; (Buchanan/Miller 2017).
Der Begriff KI stammt aus der Informatik und Robotik und bezeichnet Forschungsrichtungen, die darauf
abzielen, menschliche F&#228;higkeiten durch computergest&#252;tzte Verfahren nachzuahmen. Dies betrifft
Basisf&#228;higkeiten wie z. B. das Lesen von Handschriften, das Verstehen gesprochener Sprache, das Erkennen von Objekten 
bzw. Personen auf Bildern und Videos bis hin zu spezielleren Aufgaben wie das Meistern logikbasierter Spiele
wie Schach und Go, das &#220;bersetzen von Texten in andere Sprachen, das Entwerfen von Handelsstrategien an
Aktienm&#228;rkten, das Prognostizieren von R&#252;ckfallquoten bei H&#228;ftlingen und nicht zuletzt das Fahren von
Automobilen im &#246;ffentlichen Stra&#223;enverkehr.
Kasten 3.1 Schwache und starke KI
Im Bereich der KI wird oft zwischen schwacher KI (enger KI) und starker KI (genereller KI) unterschieden.
Bei schwacher KI wird sich mit relativ eng gesteckten, spezialisierten Aufgaben in einer bestimmten
Anwendungsdom&#228;ne besch&#228;ftigt. Alle heutigen Anwendungen sind der schwachen KI zuzuordnen. Starke KI w&#252;rde
menschen&#228;hnliche Intelligenz &#252;ber ein breites Spektrum von Dom&#228;nen bedeuten oder sogar menschliche
kognitive F&#228;higkeiten &#252;bertreffen. Ein mit starker KI ausgestattetes System w&#228;re in der Lage, vorab
unbekannte Aufgaben in unbekannten Umgebungen zu l&#246;sen.
Ob starke KI jemals durch technische Systeme zu erreichen ist, ist in Expertenkreisen umstritten.
Umfragen zufolge ist eine Mehrheit der KI-Forscher der Ansicht, dass dies durchaus m&#246;glich ist (UNIDIR 2018b,
S. 7). Eine offene Frage ist, ob daf&#252;r eine Form von Bewusstsein, Selbstwahrnehmung bzw.
Empfindungsverm&#246;gen erforderlich ist (z. B. TAB 2016, S. 103 f.).
Die Entwicklung von KI vollzog sich bisher in zwei Wellen (GAO 2018, S. 16 f.; Launchbury o. J.): Die
Anf&#228;nge der KI-Forschung, beginnend in den 1950er Jahren, beruhten auf regelbasierten Expertensystemen. Diese 
waren in der Lage, eng definierte Probleme zu l&#246;sen, wie z. B. die Ermittlung der H&#246;he der geschuldeten
Einkommensteuer. Sie verf&#252;gten jedoch im Allgemeinen nicht &#252;ber die F&#228;higkeit zum eigenst&#228;ndigen Lernen und 
konnten nicht ad&#228;quat mit unvorhergesehenen Situationen umgehen.
Aus diesen Gr&#252;nden verwendet beispielsweise das iPRAW (iPRAW 2017b) diesen Begriff nicht, sondern propagiert die Nutzung
des Terminus &#187;computational methods&#171; (computergest&#252;tzte Verfahren).
12
Seit den 1990er Jahren ist die zweite Welle in vollem Gange. Deren Grundlagen sind fortgeschrittene
Methoden der Statistik und vor allem die rechnergest&#252;tzte Auswertung riesiger Datenmengen (Big Data) sowie
das sogenannte Maschinenlernen (&#187;machine learning&#171;, &#187;deep learning&#171;). Erfolgreich sind diese Methoden
insbesondere beim Klassifizieren und Gruppieren von Daten, z. B. bei der Erkennung von Handschriften. Den 
Systemen ist es allerdings nicht m&#246;glich, Ergebnisse in einen weiteren Kontext zu stellen bzw. zu interpretieren.
Eine Richtung aktueller Forschung hat zum Ziel, eine dritte Welle der KI zu begr&#252;nden, die Vorteile der
ersten und zweiten Welle kombinieren und gleichzeitig deren Defizite auffangen soll (Launchbury o. J.).
Allerdings stehen die angestrebten F&#228;higkeiten des Verst&#228;ndnisses abstrakterer Zusammenh&#228;nge, der
Konzeptualisierung, des logischen Folgerns sowie der Herstellung von Erkl&#228;rbarkeit noch relativ weit am Anfang.
Aufgaben, die sich nach dem heutigen Erkenntnisstand f&#252;r die L&#246;sung durch KI-basierte Verfahren
besonders eignen, weisen einige spezifische Charakteristika auf:
&#8250; Es ist ein m&#246;glichst perfektes mathematisches Modell oder eine Simulation der Aufgabe verf&#252;gbar.
&#8250; F&#252;r das Fortschreiten auf das angestrebte Ziel hin existieren kurzfristige Indikatoren.
&#8250; Es ist eine gro&#223;e Menge an Daten vorhanden, z. B. mit Beispielen, wie die Aufgabe von Menschen
bew&#228;ltigt wurde.
&#8250; Die Aufgabe erfordert kein Modell der Welt, in die die Aufgabe eingebettet ist (kein &#187;common sense&#171;)
(Brundage et al. 2018, S. 13).
F&#252;r etliche Alltagsaufgaben werden KI-Systeme bereits eingesetzt. Hierzu geh&#246;ren etwa Empfehlungssysteme,
die beispielsweise Konsumenten Produkte und Dienstleistungen auf Grundlage fr&#252;herer Entscheidungen
vorschlagen, Suchmaschinen und Spamfilter, Systeme zur Spracherkennung und pers&#246;nliche Assistenzsysteme 
(z. B. &#187;Alexa&#171;, &#187;Cortana&#171;, &#187;Google Assistant&#171;, &#187;Siri&#171;), die Erkennung von Personen auf Fotos, die Erkennung 
von Handschriften, Fremdsprachen&#252;bersetzung, Betrugserkennung aus Nutzungsdaten von Kreditkarten und 
anderen Bezahlsystemen (Royal Society 2017, S. 21 ff.).
3.3.1 Maschinelles Lernen
Die beeindruckenden Fortschritte, die im Feld der KI in j&#252;ngster Zeit erzielt worden sind, sind ganz
entscheidend durch Durchbr&#252;che bei Methoden des maschinellen Lernens (ML) erm&#246;glicht worden. Dabei handelt es
sich allgemein gesprochen um Algorithmen, die sich datengetrieben verbessern k&#246;nnen (Royal Society 2017).
Es gibt verschiedene Verfahren, die f&#252;r ML angewendet werden. Die wichtigsten sind (Royal Society 2017,
S. 20):
&#8250; &#187;Supervised learning&#171;: Hierbei werden die gew&#252;nschten Outputkategorien vorgegeben (beispielsweise f&#252;r
die Erkennung von handgeschriebenen Zahlen die Ziffern 0 bis 9). Die Input- bzw. Trainingsdaten (z. B. 
Bilder von handgeschriebenen Ziffern) sind korrekt (in der Regel von Menschen) gem&#228;&#223; den Kategorien
gelabelt (&#187;dieses Bild zeigt eine 4&#171;). Im Lernprozess soll der Algorithmus die Kategorisierungen
nachvollziehen, um diese dann anschlie&#223;end auf unbekannte Daten &#252;bertragen zu k&#246;nnen.
&#8250; &#187;Unsupervised learning&#171;: Anhand der statistischen Eigenschaften der Inputdaten werden Outputkategorien
vom Algorithmus selbst entwickelt (z. B. Einteilung von Konsumenten in Marktsegmente anhand
vorliegender Daten zu Kaufentscheidungen). Die Kategorien sind im Allgemeinen nicht intuitiv f&#252;r Menschen
verst&#228;ndlich, hierf&#252;r ist eine Interpretationsleistung erforderlich. Im genannten Beispiel k&#246;nnte ein Mensch
diese Interpretation vornehmen und etwa Marktsegmenten charakteristische Gemeinsamkeiten zuordnen
und diese benennen (z. B. Gruppe 1: technoaffin, Gruppe 2: komfortbetont etc.).
&#8250; &#187;Reinforcement learning&#171;: Ein als Belohnung interpretierbarer Parameter wird so eingestellt, dass er umso
gr&#246;&#223;er wird, je n&#228;her der Output einem gew&#252;nschten Ergebnis kommt. Der Lernprozess besteht darin, den
Output systematisch so zu ver&#228;ndern, dass die Belohnung maximiert wird. Ein Anwendungsbeispiel ist das
Finden des besten Zugs in einer Schachpartie. Die &#187;Belohnung&#171; ist in diesem Beispiel ein Indikator daf&#252;r,
wie vorteilhaft die entstehende Stellung eingesch&#228;tzt wird.
Ein zentrales Unterscheidungsmerkmal bei den verschiedenen ML-Verfahren besteht darin, ob das Lernen
offline oder online geschieht. Bei einem offline lernenden System wird nach Abschluss des Trainings der
entstandene Algorithmus einschlie&#223;lich aller interner Parameter fixiert (eingefroren). Im Praxiseinsatz &#228;ndern sich
diese Parameter nicht mehr. Im Gegensatz dazu setzen online lernende (selbstlernende) Systeme das Training
auch im laufenden Betrieb fort. Ein wesentlicher Vorteil selbstlernender Systeme besteht darin, dass sie in der
Praxis Erfahrungen sammeln und so im Prinzip immer besser werden k&#246;nnen. Andererseits steht der gravierende
Nachteil, dass selbstlernende Systeme nichtintendierte Verhaltensweisen lernen k&#246;nnten und die
Vorhersagbarkeit ihres Verhaltens im Laufe der Zeit gemindert w&#252;rde. Daher kam beispielsweise die Ethik-Kommission
Automatisiertes und vernetztes Fahren (Ethik-Kommission 2017, S. 30) zu dem Schluss: &#187;Solange bei
selbstlernenden Systemen keine hinreichende Sicherheit besteht, dass diese Situationen richtig einsch&#228;tzen bzw.
Sicherheitsanforderungen einhalten k&#246;nnen, sollte eine Entkoppelung der selbstlernenden Systeme von
sicherheitskritischen Funktionen vorgeschrieben werden. Ein Einsatz von selbstlernenden Systemen ist beim
gegenw&#228;rtigen Stand der Technik daher nur bei nicht unmittelbar sicherheitsrelevanten Funktionen denkbar&#171;. Da der
Waffeneinsatz durch ein AWS offenkundig eine sicherheitskritische Funktion im Sinne der Ethik-Kommission
darstellen w&#252;rde, liegt der Analogieschluss nahe, dass selbstlernende Systeme f&#252;r kritische Funktionen von
AWS ausgeschlossen werden m&#252;ssen. Nicht nur ethisch, auch v&#246;lkerrechtlich w&#228;ren selbstlernende AWS
problematisch (Kap. 7). So ist vollkommen unklar, wie die Pr&#252;fpflicht nach Artikel 36 ZP I der Genfer Konvention,
ob ein neu entwickeltes Waffensystem v&#246;lkerrechtskonform ist, bei solchen Systemen umgesetzt werden k&#246;nnte
(IKRK 2018a, S. 15 ff.).
3.3.2 Was k&#246;nnen KI-Systeme heute leisten?
Diverse Aufgaben, die vor wenigen Jahren noch als schwierig oder sogar unm&#246;glich f&#252;r Maschinen eingesch&#228;tzt
wurden, k&#246;nnen von heutigen KI-Systemen bew&#228;ltigt werden. In etlichen spezifischen Bereichen &#252;bertreffen
die Leistungen von KI-gest&#252;tzten Verfahren bereits heute menschliche F&#228;higkeiten, beispielsweise bei der
Erkennung von Bildern (He et al. 2015), Handschriften (Markoff 2015) oder Stimmen (Xiong et al. 2016).
Regelm&#228;&#223;ig gehen Erfolgsmeldungen durch die Medien, dass ein KI-System in dieser oder jener Aufgabe
menschliche F&#228;higkeiten &#252;bertroffen habe. So verk&#252;ndeten beispielsweise im Januar 2018 die Alibaba Group Holding 
Limited (Najberg 2018) sowie unabh&#228;ngig davon die Microsoft Corporation (Linn 2018), dass sie einen
Algorithmus entwickelt h&#228;tten, der in einem standardisierten Test zum Leseverst&#228;ndnis (dem Stanford Question
Answering Dataset, SQuAD) besser als Menschen abgeschnitten habe.
Im Folgenden werden einige der herausragenden Beispiele f&#252;r die F&#228;higkeiten aktueller KI-Systeme
dargestellt.
Schach, Shogi, Go
Seit Jahren bieten strategische Brettspiele wie Schach, Shogi (japanisches Schach) und Go eine ideale B&#252;hne,
um die Leistungsf&#228;higkeit von Computerprogrammen in intellektuellen Aktivit&#228;ten, die als typisch menschlich
angesehen werden, unter Beweis zu stellen. Programme zu entwickeln, die besser als Menschen spielen, galt
lange als eine der Grand Challenges der KI. Ein erster Meilenstein wurde 1997 erreicht, als Schachweltmeister
Garry Kasparov gegen ein auf dem Supercomputer &#187;Deep Blue&#171; laufendes Programm verlor. Allerdings st&#252;tzte 
sich dieses Programm in erheblichem Umfang auf menschliches Expertenwissen, das u. a. aus umfangreichen
Bibliotheken mit Er&#246;ffnungsz&#252;gen und Parametern f&#252;r die Beurteilung von Stellungen bestand (Campbell et al.
2002).
Im Vergleich zu Schach galt Go noch bis vor wenigen Jahren als Dom&#228;ne, in der die menschliche
&#220;berlegenheit &#252;ber Computerprogramme auf lange Zeit gesichert schien. Ein Hauptgrund daf&#252;r ist, dass bei Go viel
mehr Zugm&#246;glichkeiten bestehen, sodass die bei klassischen Schachprogrammen angewandte Brute-Force-
Methode (Durchprobieren s&#228;mtlicher M&#246;glichkeiten im Rahmen der vorhandenen Rechenkapazit&#228;ten und
Bewertung der so entstehenden Stellungen) an ihre Grenzen st&#246;&#223;t.13 Daher war es eine wissenschaftliche Sensation,
als 2016 das auf ML-Methoden (in erster Linie &#187;reinforcement learning&#171;) basierende Programm &#187;AlphaGo&#171;
13 Betrachtet man alle theoretisch m&#246;glichen Spielverl&#228;ufe, so &#252;bertrifft Go Schach um den Faktor 10237 (eine Eins mit 237 Nullen)
(Wikipedia 2008).
(Silver et al. 2016) den zu dieser Zeit unbestritten st&#228;rksten menschlichen Go-Spieler Lee Sedol &#252;berzeugend
schlug (Wikipedia 2016).
Die mit &#187;AlphaGo&#171; angesto&#223;ene Entwicklung wurde unmittelbar mit einem weiteren Entwicklungsschritt,
dem &#187;AlphaZero&#171;, fortgesetzt, der zwar deutlich weniger &#246;ffentliche Aufmerksamkeit erregte, aus
technologischer Sicht aber fast noch beeindruckender ist. Dabei handelt es sich um einen Algorithmus, der sowohl Schach
als auch Shogi und Go bew&#228;ltigt hat. Lediglich mit den Regeln des Spiels gef&#252;ttert, steigerte er anschlie&#223;end
selbstst&#228;ndig durch Spielen gegen sich selbst seine Spielst&#228;rke. Bereits nach wenigen Stunden &#252;bertraf er das
f&#252;r Menschen erreichbare Niveau in allen drei Spielen und nach 24 Stunden schlug er das jeweils beste
verf&#252;gbare konventionelle Programm. &#187;AlphaZero&#171; wird als bedeutender Schritt hin zu einem generalisierten
Algorithmus gesehen, der jedes Strategiespiel lernen kann (Campbell 2018; Kasparov 2018; Silver et al. 2017).
Am Beispiel &#187;AlphaZero&#171; kann gut ein wesentlicher Unterschied zwischen Menschen und Algorithmen
beim Lernen deutlich gemacht werden. Wenn &#187;AlphaZero&#171; ein neues Spiel lernt, startet er jedes Mal von null:
Der Algorithmus kann nicht davon profitieren, dass in der Vergangenheit bereits ein anderes Spiel gelernt 
wurde. Menschen k&#246;nnen im Lernprozess generalisiertere Erkenntnisse gewinnen, die etwa durch
Analogieschl&#252;sse f&#252;r neue Aufgaben genutzt werden k&#246;nnen.
Computerspiele und Simulationen
Ein weiteres Erfolgsbeispiel daf&#252;r, dass ML-basierte Algorithmen menschliche F&#228;higkeiten erreichen bzw.
&#252;bertreffen k&#246;nnen, sind Computerspiele. Dies wurde beispielsweise f&#252;r eine breite Palette klassischer Atari-
Spiele (z. B. Pong, Space Invaders) gezeigt. Dem Algorithmus wurden lediglich die Bildschirmpixel und der
Spielstand &#252;bermittelt. Spielstrategien wurden anschlie&#223;end per &#187;reinforcement learning&#171; vom Algorithmus
selbst entwickelt (Mnih et al. 2015).
In letzter Zeit ist die Frage in den Mittelpunkt des Interesses ger&#252;ckt, wie unterschiedliche Algorithmen
zur Erreichung eines gemeinsamen Ziels miteinander bzw. mit Menschen kooperieren k&#246;nnen. Bemerkenswert
sind hier erste Ans&#228;tze, die u. a. beim &#228;u&#223;erst popul&#228;ren Multiplayer-Echtzeit-Strategiespiel DOTA II14 erprobt
wurden. So gelang es einem Team aus f&#252;nf Bots, genannt &#187;OpenAI Five&#171;, ein Team aus f&#252;nf ehemaligen
Profispielern zu schlagen, obwohl ihre Reaktionszeit k&#252;nstlich auf menschliches Niveau herabgesetzt wurde
(Hutson 2018).
Im Hinblick auf die praktische Umsetzung von AWS ist die Entwicklung eines KI-basierten autonomen
Steuersystems f&#252;r Flugsimulatoren bemerkenswert. Das System simuliert ein gegnerisches Kampfflugzeug im
Luft-zu-Luft-Kampf und scheint selbst f&#252;r erfahrene Kampfpiloten kaum bezwingbar zu sein (Ernest et al. 2016; 
Reilly 2016).
Poker
Eine v&#246;llig andere Dom&#228;ne als Strategiespiele wie Schach ist Poker. Hier liegen buchst&#228;blich nicht alle
Informationen auf dem Tisch. Daher m&#252;ssen Entscheidungen unter unsicheren Annahmen getroffen werden und der
Zufall spielt eine wesentliche Rolle. Gute Pokerspieler besitzen daher viel Intuition und ein gutes Verst&#228;ndnis
von Psychologie und Verhalten der Gegner. F&#252;r eine bestimmte Variante des Pokers (&#187;heads up no limit Texas
hold &#8217;em&#171;) wurde ein ML-Algorithmus &#187;DeepStack&#171; entwickelt, der professionelle Pokerspieler bezwingen
konnte. Da Entscheidungen in der realen Welt oft unter unsicheren Annahmen getroffen werden m&#252;ssen,
versprechen sich die Entwickler von &#187;DeepStack&#171;, dass ihre Methoden weit &#252;ber das Pokern hinaus
Anwendungsperspektiven finden k&#246;nnen (Morav&#269;&#237;k et al. 2017; Riley 2017).
Jeopardy!
In der aus den USA stammenden Quizshow &#187;Jeopardy!&#171; werden den Kandidaten Antworten aus diversen
Kategorien vorgegeben. Die Aufgabe besteht darin, schneller als die Konkurrenten eine zur Antwort passende 
Frage zu formulieren. Damit ein Computerprogramm diese Aufgabe bew&#228;ltigen kann, muss es als Erstes den
Sinn von in nat&#252;rlicher Sprache gestellten Fragen erfassen und sodann in einem Wissensbestand die relevanten
Benannt nach der 1. Folge &#187;Defense of the Ancients&#171;.14
Fakten auffinden. Eine solche semantische Suchmaschine ist das Programm &#187;Watson&#171; (Wikipedia 2010a), das 
bereits 2011 gegen zwei menschliche Jeopardy!-Champions antrat und mit gro&#223;em Abstand gewann.
Inzwischen werden von &#187;Watson&#171; abgeleitete Programme in einer Vielzahl von Feldern angewandt, u. a.
zur Beantwortung von Onlinekundenanfragen, zur Weitergabe von Erfahrungswissen &#228;lterer Ingenieure an
Nachwuchskr&#228;fte oder zur Steuerung eines intelligenten Bew&#228;sserungssystems im Weinbau (IBM o. J.). Auch
in der Medizin werden enorme Anwendungspotenziale f&#252;r angepasste Watson-Algorithmen erwartet.
Allerdings gab es in diesem Bereich auch R&#252;ckschl&#228;ge. So wurde ein Experiment abgebrochen, bei dem &#187;Watson&#171; 
Vorschl&#228;ge zur personalisierten Therapie von Krebspatienten entwickeln sollte, da gravierende Fehler auftraten,
die im Ernstfall Patienten empfindlich geschadet h&#228;tten (Meier 2018).
3.3.3 Begrenzungen, Schwierigkeiten und Risiken bei KI-Systemen
Den beschriebenen Erfolgen von KI-Systemen stehen betr&#228;chtliche technische und operationelle Begrenzungen,
Schwierigkeiten und Risiken gegen&#252;ber, von denen einige im Folgenden dargestellt werden.
Abh&#228;ngigkeit von gro&#223;en Datenbest&#228;nden
Die Verf&#252;gbarkeit gro&#223;er und f&#252;r die konkrete Fragestellung hochwertiger Datenbest&#228;nde (z. B. beim &#187;
supervised learning&#171; korrekt gelabelte Daten) ist von entscheidender Bedeutung f&#252;r die Qualit&#228;t der Resultate von 
ML-Prozessen, oft wichtiger als die verwendeten Algorithmen selbst (Banko/Brill 2001). So hat beispielsweise
das Unternehmen Waymo LLC (eine Tochter des Google-Mutterkonzerns Alphabet Inc.) 2018 bereits Daten 
von &#252;ber 16 Mio. km autonomer Fahrten auf &#246;ffentlichen Stra&#223;en gesammelt. Dar&#252;ber hinaus wurde
angek&#252;ndigt, &#252;ber 60.000 selbstfahrende Minibusse auf die Stra&#223;e zu bringen (Hu 2018). Die gro&#223;e Datenmenge ist 
essenziell, um autonomen Fahrzeugen ad&#228;quates Verhalten auch bei selten auftretenden Ereignissen
antrainieren zu k&#246;nnen.
Wie dies im analogen Fall eines gel&#228;ndeg&#228;ngigen autonomen Milit&#228;rfahrzeugs geschehen k&#246;nnte, damit
alle Eventualit&#228;ten, denen das System im Betrieb ausgesetzt sein k&#246;nnte, in den entsprechenden Trainingsdaten
abgebildet sind, ist gegenw&#228;rtig unklar.
Begrenzte &#220;bertragbarkeit von einem auf andere Bereiche
Erfolge von heutigen KI-Systemen in einem bestimmten Bereich k&#246;nnen nicht ohne Weiteres auf einen anderen
Bereich &#252;bertragen werden, auch wenn dieser aus menschlicher Sicht analog zum ersten Bereich erscheint. Dies
liegt an einer charakteristischen Eigenschaft von KI-Systemen, die katastrophales Vergessen genannt wird. Das
bedeutet, dass ein Algorithmus, der gelernt hat, eine bestimmte Aufgabe zu bew&#228;ltigen, und anschlie&#223;end f&#252;r
eine neue Aufgabe trainiert wird, seine Kompetenz zur L&#246;sung der ersten Aufgabe verliert. Dies verhindert,
dass KI-Systeme kontinuierlich F&#228;higkeiten hinzugewinnen k&#246;nnen, und stellt eine wesentliche Begrenzung f&#252;r
die Erschlie&#223;ung neuer Aufgabenfelder vor allem durch ML-basierte KI dar. An der &#220;berwindung dieses
Problems wird derzeit intensiv geforscht (Kirkpatrick et al. 2017; UNIDIR 2018b, S. 6).
Eine &#228;hnlich gelagerte Problematik tritt auf, wenn ein KI-System, das f&#252;r eine bestimmte Aufgabe
entwickelt wurde, f&#252;r eine &#8211; nach menschlichen Ma&#223;st&#228;ben &#8211; geringf&#252;gig andere Aufgabe eingesetzt werden soll.
Um den Entwicklungsaufwand zu minimieren, k&#246;nnte man auf die Idee kommen, das bestehende System
geringf&#252;gig zu modifizieren, um es so an die neue Aufgabe anzupassen. Dies kann allerdings zu unerw&#252;nschtem,
nicht vorhersehbarem Verhalten des Systems f&#252;hren, denn vor allem ML-basierte Systeme unterliegen dem
sogenannten CACE-Prinzip (&#187;changing anything changes everything&#171;), gem&#228;&#223; dem jede &#196;nderung an Teilen
des Systems in schwer vorhersehbarer Weise auf das ganze System durchschlagen kann (Danzig 2018, S. 8;
Sculley et al. 2015).
Bias
Algorithmen, ob auf der Basis von ML-Verfahren oder auf andere Weise erzeugt, k&#246;nnen eine komplexe
Wirklichkeit niemals objektiv, neutral und pr&#228;zise abbilden, denn sie reflektieren immer die Auswahlentscheidungen,
die in ihrem Design getroffen werden m&#252;ssen.
Allgemein gesprochen liegt ein Bias vor, wenn die Ergebnisse eines algorithmischen Verfahrens
systematisch von einem gesetzten Standard abweichen (hierzu und zum Folgenden UNIDIR 2018a). Aus diversen
Anwendungsbereichen von ML-gest&#252;tzten Verfahren sind Beispiele bekannt, bei denen ein Bias zu groben Fehlern
und erheblichem Schaden f&#252;r die Betroffenen gef&#252;hrt hat. Dies reicht von durch Algorithmen vorgeschlagene
h&#228;rtere Haftstrafen f&#252;r Angeh&#246;rige bestimmter Ethnien (Fefegha 2018) bis hin zu geringeren Jobchancen f&#252;r
Frauen bei algorithmisch gest&#252;tzten Auswahlverfahren (Dastin 2018).15 Die genaue Untersuchung solcher F&#228;lle
tr&#228;gt zu einem besseren Verst&#228;ndnis bei, wie ein Bias entsteht, welche Auswirkungen er hat und wie er
eingegrenzt werden kann. Dies liefert wertvolle Hinweise, um die m&#246;glichen Effekte von Bias auch bei AWS besser
einsch&#228;tzen zu k&#246;nnen.
Es gibt unterschiedliche Arten von Bias: Ein statistischer Bias liegt vor, wenn der Output eines ML-
Verfahrens die wahre H&#228;ufigkeit eines bestimmten Ereignisses nicht im Rahmen vorgegebener Toleranzen
widerspiegelt. Beispielsweise k&#246;nnte ein System zur Absch&#228;tzung des Kreditausfallrisikos dieses f&#252;r bestimmte
Kundengruppen systematisch zu hoch einsch&#228;tzen. Ein moralischer Bias tritt hingegen auf, wenn der Output eines
ML-Verfahrens etablierten moralischen Normen widerspricht. Beispielsweise k&#246;nnte das System das
Kreditausfallrisiko zwar statistisch korrekt berechnen, aber im Ergebnis bestimmte Kundengruppen, z. B. aufgrund
von Geschlecht oder ethnischer Zugeh&#246;rigkeit, diskriminieren. Dies k&#246;nnte auftreten, selbst wenn die
Information &#252;ber die Zugeh&#246;rigkeit zu dieser Gruppe nicht verwendet wurde, etwa weil diese mit anderen verwendeten
Merkmalen korreliert.
Ein Bias kann auf unterschiedliche Art und Weise entstehen. Auf der einen Seite kann er systembedingt
induziert werden, etwa durch Trainingsdaten, die f&#252;r den intendierten Einsatzzweck nicht repr&#228;sentativ sind,
oder durch falsche Modellannahmen im zugrundegelegten Algorithmus. Repr&#228;sentative Trainingsdaten f&#252;r
AWS, die sich in Gefechtsfeldern bewegen, die sich hochdynamisch ver&#228;ndern und von gegnerischen Aktionen
gepr&#228;gt sind, sind schwer bis unm&#246;glich zu beschaffen. Um herauszufinden, ob sich ein AWS in allen
Einsatzszenarien regelkonform verh&#228;lt, m&#252;sste es in realistischen Umgebungen getestet werden, da bestimmte durch
Bias verursachte Fehler sonst ggf. nicht rechtzeitig entdeckt werden k&#246;nnen.
Auf der anderen Seite k&#246;nnen auch auf der Nutzerseite erhebliche Quellen von Bias entstehen. Wenn etwa 
ein System au&#223;erhalb des intendierten Einsatzkontexts verwendet wird (&#187;transfer context bias&#171;), ist oft eine
&#252;berraschende bzw. fehlerhafte Funktionsweise zu beobachten. Ein Beispiel k&#246;nnen AWS sein, die f&#252;r ein
offenes Terrain entwickelt wurden, aber entgegen der Spezifikation in dicht bebautem, urbanem Gel&#228;nde
eingesetzt werden. Schlie&#223;lich sind auch Fehlinterpretationen durch den Nutzer m&#246;glich (&#187;interpretation bias&#171;).
Beispielsweise k&#246;nnte der Output eines &#220;berwachungssystems, der die &#187;Unsicherheit &#252;ber die Identit&#228;t einer
Person&#171; beschreibt, missverstanden werden als &#187;Wahrscheinlichkeit, dass die Person ein Terrorist ist&#171;. Diese Art
von Fehler k&#246;nnte man eher als Nutzerversagen beschreiben, allerdings spielen auch hier oft technische
Elemente, wie das Design der Interfaces, eine tragende Rolle.
Vorhersagbarkeit und Verl&#228;sslichkeit des Verhaltens
&#220;ber die beschriebenen Auswirkungen von Bias hinaus besitzen KI-Systeme weitere Charakteristika, die die
Vorhersagbarkeit ihres Verhaltens erschweren k&#246;nnen. Im Fall von selbstlernenden Systemen, die w&#228;hrend
ihres Betriebs &#187;im Feld&#171; Erfahrungen sammeln und ihr Verhalten entsprechend anpassen k&#246;nnen, ist
offensichtlich, dass die Vorhersagbarkeit im Laufe der Zeit abnimmt und fr&#252;her oder sp&#228;ter nicht mehr gew&#228;hrleistet 
werden kann.
Aber auch offline lernende ML-Systeme sind bekannt daf&#252;r, dass sie schwer vorhersagbares Verhalten
produzieren k&#246;nnen. So k&#246;nnen etwa ML-Algorithmen, beispielsweise zur Bilderkennung, unter Umst&#228;nden
durch minimale Abweichungen des Inputs dazu gebracht werden, ein zuvor korrekt klassifiziertes Bild v&#246;llig
falsch einzuordnen.
Ein eindr&#252;ckliches Beispiel daf&#252;r zeigt Abbildung 3.1 (Szegedy et al. 2014). Einem korrekt klassifizierten
Bild (linke Spalte) wurde eine kleine St&#246;rung (mittlere Spalte) &#252;berlagert. Obwohl f&#252;r das menschliche Auge 
praktisch nicht von den Ausgangsbildern zu unterscheiden, wurden die resultierenden Bilder (rechte Spalte)
vom ML-Algorithmus f&#228;lschlich als &#187;Vogel Strau&#223;&#171; (Struthio camelus)&#171; klassifiziert. Etliche weitere dieser
&#187;adversarial examples&#171; (zu Deutsch etwa &#187;feindliche Beispiele&#171;) wurden generiert (Goodfellow et al. 2015). 
Im Extremfall reicht bereits die &#196;nderung eines einzigen Pixels aus, um einen ML-Bilderkennungsalgorithmus
Weitere Fallbeispiele werden in Lischka et al. 2017 beschrieben.15
zu t&#228;uschen (Su et al. 2017). Dies k&#246;nnte im Praxiseinsatz problematische oder sogar katastrophale Folgen
haben. Ein drastisches Beispiel w&#228;ren autonome Autos, die etwa Stoppschilder, die mit kleinen Aufklebern
manipuliert wurden, pl&#246;tzlich als Vorfahrtsschilder interpretieren. Dass dies mehr als ein Gedankenspiel ist,
zeigen Versuche mit Verkehrsschildern von Eykholt et al. (Eykholt et al. 2018).
Abb. 3.1 T&#228;uschung eines Algorithmus zur Bilderkennung
Links: korrekt klassifiziertes Bild, Mitte: Differenz zwischen Bild links und rechts, rechts:
falsch klassifiziertes Bild. Alle Bilder in der rechten Spalte wurden als &#187;Vogel Strau&#223;&#171;
(Struthio camelus) klassifiziert.
Quelle: Szegedy et al. 2014, S. 6
Das Generieren von Beispielen, die ML-Algorithmen t&#228;uschen k&#246;nnen, hat mittlerweile einen eigenen
Forschungszweig begr&#252;ndet, das Adversarial Machine Learning. Dieses hat zum Ziel, Schwachstellen von ML-
Algorithmen zu finden, um diese ggf. zu verbessern. Mittlerweile sind diverse Angriffs- und
Verteidigungsstrategien entwickelt worden, um ML-Algorithmen zu kompromittieren bzw. genau davor zu sch&#252;tzen. Die
grundlegende Anf&#228;lligkeit von ML-Algorithmen konnte bisher aber nicht behoben werden (Goodfellow et al. 2017).
Selbst wenn ein ML-Algorithmus aus mathematischer Sicht verl&#228;sslich ist, weil er reproduzierbar auf einen
identischen Input immer denselben Output ausgibt, w&#228;re dies keineswegs ausreichend, um in der Praxis
verl&#228;sslich zu funktionieren. Eine Bedingung hierf&#252;r w&#228;re, dass &#228;hnliche Inputs zu &#228;hnlichen Outputs f&#252;hren. Dies
ist bei ML-Algorithmen vielfach nicht gew&#228;hrleistet, weshalb ihr Verhalten oft als br&#252;chig beschrieben wird
(Pontin 2018).
Blackbox KI
Heutige KI-Verfahren, insbesondere ML-gest&#252;tzte Verfahren, sind h&#228;ufig vom Typ Blackbox. Der Input &#8211;
beispielsweise bei Bilderkennungsverfahren ein Bild &#8211; erzeugt einen Output etwa in der Art: &#187;Mit einem
Korrelationskoeffizienten von 0,93 f&#228;llt der Input in die mit &#8250;Katze&#8249; beschriftete Kategorie&#171;, ohne dass ein Nutzer oder
auch der Entwickler die innere Logik und die einzelnen Schritte, die zu dem Output gef&#252;hrt haben,
nachvollziehen kann.
Wie bedeutsam dies ist, h&#228;ngt vom Kontext ab. F&#252;r ein robustes System, das gut abgegrenzt werden kann,
mag dies irrelevant sein. F&#252;r ein System, das im Kontext eines gr&#246;&#223;eren Systems verwendet wird, k&#246;nnte dies
unl&#246;sbare Schwierigkeiten bei der Fehlersuche und -behebung verursachen (Marcus 2018, S. 11). Besonders in 
Anwendungsbereichen, in denen viel auf dem Spiel steht &#8211; wie in der Medizin, im Finanzsektor und nat&#252;rlich
beim Milit&#228;r &#8211;, ist kaum vorstellbar, dass durch ein System generierte Handlungsvorschl&#228;ge, die nicht
nachvollzogen und begr&#252;ndet werden k&#246;nnen, umgesetzt werden.
Verschiedene Ans&#228;tze sollen hier Abhilfe schaffen und neben dem eigentlichen Output entsprechende
Begr&#252;ndungen mitliefern. Interpretierbare Verfahren sind so transparent gestaltet, dass ein Nutzer, der &#252;ber
hinreichendes technisches Verst&#228;ndnis bzw. Hintergrundwissen verf&#252;gt, die Entstehung des Outputs
nachvollziehen und erkl&#228;ren kann. Im Gegensatz dazu erzeugt ein erkl&#228;rbares Verfahren nicht nur einen Output, sondern
generiert im Zuge der Berechnungen Symbole (z. B. W&#246;rter oder Visualisierungen), die es dem Nutzer erlauben,
Eigenschaften des Inputs dem Output zuzuordnen. Im Beispiel eines Bilderkennungsverfahrens w&#228;re dies etwa:
&#187;Es hat Fell, es hat Schnurrhaare und Krallen: Es ist eine Katze&#171;. Dies ist in Abbildung 3.2 im Kontrast zu 
heutigen Blackboxsystemen illustriert. Ziel dieser erkl&#228;rbaren KI ist es, dem Nutzer zu erm&#246;glichen, die Qualit&#228;t
und die Grenzen des durch die KI generierten Outputs zu bewerten.
Streng genommen liefern aber diese Verfahren keine Erkl&#228;rungen, sondern sie erm&#246;glichen sie lediglich, 
d. h., es wird charakterisiert, wie ein Output zustande gekommen ist, aber nicht warum (Doran et al. 2017;
Gunning o. J. 2017). Ob diese Interpretationsleistung jemals von Maschinen erbracht werden kann, ist
umstritten. Demzufolge ist derzeit noch nicht absehbar, ob diese Ans&#228;tze die Erwartungen erf&#252;llen werden, eine Br&#252;cke
zwischen Maschinensprache und menschlicher Sprache bzw. menschlichem Verst&#228;ndnis zu schlagen (iPRAW
2017b).
        
 
 
  
 
     
   
   
    
    
  
   
  
 
   
   
 
       
       
    
     
         
      
 
  
  
   
KI
Trainingsdaten
erkl&#228;rbare KI
Trainingsdaten
maschinelles
Lernen
neues
maschinelles
Lernen
erlernte
Funktion
Entscheidung
oder
Empfehlung
Aufgabe
Warum dieses?
Warum nicht etwas anderes?
Wann funktion iert es?
Wann funktion iert es nicht?
Wann kann ich dem Ergebnis trauen?
Wie kann ich Fehler korrigieren?
Aufgabe
Erkl&#228;rungsmodul
Modell Interface
Ich verstehe, warum.
Ich verstehe, warum nicht.
Ich wei&#223;, wenn es funktion iert.
Ich wei&#223;, wenn es nicht funktioniert.
Ich wei&#223;, wann ich dem Ergebnis trauen kann.
Ich wei&#223;, warum ein Feh ler aufgetreten ist.
Abb. 3.2 Erkl&#228;rbare KI
Quelle: nach GAO 2018, S. 19
Problem der Verifizierung und Zertifizierung
Bevor milit&#228;rische Systeme in sicherheitskritischen Bereichen in der Praxis stationiert und eingesetzt werden,
m&#252;ssen sie strenge Pr&#252;fprozeduren durchlaufen, damit sichergestellt ist, dass sie wie intendiert funktionieren.
Eine etablierte Methode, um dies nachzuweisen, ist, die Systeme allen denkbaren Umst&#228;nden auszusetzen und
zu &#252;berpr&#252;fen, ob sie ad&#228;quat darauf reagieren. Dies ist f&#252;r KI-Systeme schwierig zu realisieren, insbesondere 
wenn diese (Boulanin/Verbruggen 2017, S. 68 ff.)
&#8250; in zunehmend komplexen und dynamisch sich &#228;ndernden, schwer vorhersehbaren Umgebungen eingesetzt
werden sollen,
&#8250; mit anderen autonomen Systemen oder mit Menschen interagieren,
&#8250; selbstlernende Funktionen aufweisen.
Derzeit gibt es keine Konzepte bzw. Standards, wie solche hochkomplexen autonomen Systeme verifiziert und
zertifiziert werden sollen. Dies betrifft sowohl die Algorithmen bzw. die Software als auch die Integration in
ein cyberphysikalisches System (z. B. ein AWS) (DOD 2015b, S. 2 f.). Aktuelle Ans&#228;tze zur Entwicklung
solcher Standards beruhen auf Modellierung und Simulation. Die Herausforderung besteht dann darin zu
gew&#228;hrleisten, dass diese Modelle bzw. Simulationen die relevanten Eigenschaften der realen Welt hinreichend gut 
abbilden (DSB 2016, S. 29 ff.). Da es sich bei der Kriegsf&#252;hrung oft um ein extrem schwer vorhersehbares
Umfeld handelt, sind hier durchaus Zweifel angebracht, ob dies m&#246;glich ist (Danzig 2018, S. 7).
Emergente Effekte bei KI-Systemen
Die bisher beschriebenen Charakteristika von KI-Systemen waren &#252;berwiegend im engeren Bereich der
Algorithmen angesiedelt. Dar&#252;ber hinaus k&#246;nnen allerdings auf der Systemebene Effekte auftreten, wenn Teile eines
Systems oder verschiedene Systeme untereinander auf komplexe Art und Weise miteinander wechselwirken.
Komplexit&#228;t ist der zentrale Treiber dieser emergenten Effekte. Sie sind h&#228;ufig auch dann schwer vorhersehbar,
wenn die Eigenschaften der wechselwirkenden Komponenten gut bekannt sind (J&#246;nsson 2007). Nicht
ungew&#246;hnlich ist bei emergenten Effekten, dass sie nur gelegentlich und nicht reproduzierbar auftreten (von Tolk 
[2015] &#187;spukhaft&#171; genannt). Bei milit&#228;rischen technologischen Systemen werden emergente Effekte als einer
der Hauptgr&#252;nde f&#252;r den Verlust von Kontrolle angesehen (Danzig 2018, S. 9 f.).
In welcher Art und Weise AWS, die auf gegnerische autonome Systeme treffen, mit diesen interagieren
w&#252;rden, ist eine vollkommen ungekl&#228;rte Frage. Da die genauen Charakteristika der gegnerischen Systeme
vermutlich nicht bekannt sein d&#252;rften und die operationellen Umst&#228;nde des Aufeinandertreffens kaum
vorhersehbar sind, sind solche Begegnungen auch im Vorfeld schwer modellier- bzw. simulierbar.
Auch das Verhalten von Schw&#228;rmen wie etwa bei der sogenannten Schwarmintelligenz ist ein emergenter
Effekt, der den Schwarm als Ganzes zu Aktionen bef&#228;higt, die f&#252;r ein einzelnes Individuum unerreichbar sind.
Dieses Verhalten zu verstehen bzw. zu steuern ist die Zielsetzung der Schwarmrobotik, die ein aktuelles
Forschungsfeld ist (&#350;ahin/Spears 2005).
Mensch-Maschine-Interaktion
Wenn menschliche Bediener nicht zuletzt aus ethischen und/oder juristischen Gr&#252;nden eine wesentliche Rolle
bei der Steuerung (Kontrolle, Aufsicht) von AWS spielen sollen, ist aus technologischer Sicht nicht das Ziel,
ein m&#246;glichst hohes Level von Autonomie im System zu implementieren, sondern einen m&#246;glichst perfekten
Mix aus Autonomie und menschlicher Mitwirkung, um eine effektive, zuverl&#228;ssige und fehlerarme Ausf&#252;hrung
der &#252;bertragenen Aufgaben zu gew&#228;hrleisten. Diesen Mix herzustellen ist eine der gr&#246;&#223;ten Herausforderungen
der Robotik/Automatisierung und nach Ansicht des Technikhistorikers David Mindell schwieriger zu erreichen
als vollst&#228;ndige Autonomie (Mindell 2015a, 2015b).
Ein hoher Grad an Automation f&#252;hrt oft dazu, dass im Normalbetrieb der Bediener kaum etwas zu tun hat,
aber in kritischen Situationen pl&#246;tzlich sehr viel gleichzeitig auf ihn einstr&#246;mt (Geer 2018, S. 12). Die
Schwierigkeit besteht dann darin, mit einer Situation umzugehen, die (Hawley 2017, S. 7) so beschreibt: &#187;
Dreiundzwanzig Stunden und neunundf&#252;nfzig Minuten Langeweile, gefolgt von einer Minute Panik&#171;.
Es existiert eine Vielzahl von Beispielen f&#252;r Fehler mit katastrophalen Auswirkungen, die in der
Vergangenheit im Zuge der Interaktion von menschlichen Bedienern mit hochautomatisierten Systemen geschehen
sind. (Hawley 2017) beschreibt einige davon anhand seiner langj&#228;hrigen Erfahrung mit dem Flugabwehrsystem
&#187;Patriot&#171;, das u. a. 2003 im Irakkrieg f&#228;lschlicherweise ein britisches sowie ein Kampfflugzeug der US-Navy 
abgeschossen hat.
Auch in der zivilen Luftfahrt existieren umfangreiche Erfahrungen mit Problemen bei der Mensch-
Maschine-Schnittstelle. Dabei scheinen die gro&#223;en Flugzeughersteller Boeing und Airbus unterschiedliche
Philosophien zu verfolgen, welche Entscheidungen in bestimmten Situationen durch die Software bzw. den Piloten
gef&#228;llt werden (Danzig 2018, S. 15). Diese Arbeitsteilung wird regelm&#228;&#223;ig nach Unf&#228;llen &#246;ffentlich kritisiert 
und infrage gestellt (z. B. Economist 2019; Tagesschau 2019).
Konzeptionell entsprechen hochautomatisierte Systeme, und damit auch AWS, komplexen, eng
gekoppelten technischen Systemen. In diesem Zusammenhang bedeutet eng gekoppelt, dass ein (Sub-)System ein
anderes ohne Puffer direkt beeinflusst. F&#252;r solche Systeme hat Charles Perrow dargelegt, dass es auch bei h&#246;chster
Sorgfalt bei Design, Bau, Betrieb und Aufsicht unvermeidlich zu unerwarteten und schwerwiegenden Fehlern
kommen muss. Hierf&#252;r hat er den Begriff Normale Katastrophen gepr&#228;gt (Perrow 1989).16 Die
unvorhergesehene Art und Weise, wie sich solche Fehler manifestieren k&#246;nnen, und die fehlende Beobachtbarkeit und
Steuerbarkeit systeminterner Vorg&#228;nge (z. B. der ML-Algorithmen), machen es auch f&#252;r gut ausgebildete und
erfahrene Bediener kaum m&#246;glich, die Quellen solcher Fehler rechtzeitig zu identifizieren und zu beheben.
Entgegen der Intuition kann das Einbauen von Redundanzen, um z. B. die M&#246;glichkeit zu haben, ein fehlerhaftes
(Sub)System durch ein Reservesystem zu ersetzen, durch Erh&#246;hung der Komplexit&#228;t sogar zu zus&#228;tzlichen bzw.
schwerwiegenderen Fehlern f&#252;hren (UNIDIR 2016, S. 6 ff.).
Eine F&#252;lle an Beispielen findet sich in Clearfield/Tilcsik 2018.16
Kasten 3.2 Milit&#228;rische KI-Forschung und zivilgesellschaftlicher Protest: zwei Fallbeispiele
Dass Teile der Zivilgesellschaft der milit&#228;rischen KI-Forschung &#228;u&#223;erst skeptisch gegen&#252;berstehen und
Mittel des Protests bzw. Boykotts einsetzen, um kritisch gesehene Entwicklungen zu stoppen, zeigen zwei
Fallbeispiele.
Projekt &#187;Maven&#171;
Die Menge des insbesondere von US-Drohnen aufgezeichneten Bildmaterials (Fotos und Bewegtbilder)
steigt immer mehr an und ist aktuell dabei, die Analysekapazit&#228;t der f&#252;r diese Aufgabe bislang eingesetzten
menschlichen Analysten zu sprengen (Pellerin 2017). Dieses Problem soll durch den Einsatz von KI-
gest&#252;tzten Verfahren zur Bildanalyse gel&#246;st bzw. abgemildert werden. Das vom US-Verteidigungsministerium
initiierte Projekt &#187;Maven&#171; hat die Zielsetzung, Bildmaterial durch Algorithmen auswerten zu lassen, um
Objekte bzw. Personen zu klassifizieren, zu identifizieren sowie zu verfolgen (tracken).
Seit 2017 ist das Unternehmen Google am Projekt &#187;Maven&#171; beteiligt, u. a. indem die von Google
entwickelte (frei verf&#252;gbare) KI-Software &#187;TensorFlow&#171; an die milit&#228;rische Fragestellung angepasst wurde.
Nachdem Tausende Google-Mitarbeiter sowie mehrere Hundert IT-Spezialisten und Ethiker dagegen
protestiert hatten, entschied das Unternehmen, den bis 2019 laufenden Vertrag mit dem US-
Verteidigungsministerium nicht zu verl&#228;ngern (B&#252;nte 2018; Peitz 2018).
KAIST
Hanwha Systems, ein bedeutender s&#252;dkoreanischer R&#252;stungskonzern, hat im Februar 2018 bekanntgegeben,
eine Kooperation mit der staatlichen Forschungseinrichtung Korean Advanced Institute of Science and
Technology (KAIST) einzugehen, auf deren Grundlage gemeinsam an KI-Technologien f&#252;r milit&#228;rische Systeme
geforscht werden soll (Jun 2018). Als Reaktion darauf haben mehr als 50 f&#252;hrende KI- und Robotikforscher
einen Boykott von KAIST (d. h. einen Stopp jeglicher Kooperationen und Kontakte) angek&#252;ndigt, der so 
lange aufrechterhalten werden soll, bis KAIST versichert, dass die zu entwickelnden Waffen stets unter
menschlicher Kontrolle stehen werden. Daraufhin erkl&#228;rte der Pr&#228;sident von KAIST in einem Brief an die
Unterst&#252;tzer des Boykotts, dass KAIST sich nicht an der Entwicklung von LAWS oder Killerrobotern
beteiligen w&#252;rde. Daraufhin wurde der Boykott beendet (McLean 2018).
4 Verbreitung, Status und Trends unbemannter Waffensysteme
Bem&#252;hungen um unbemannte Waffensysteme17 lassen sich bis zum Ersten Weltkrieg zur&#252;ckverfolgen. Wie der
historische Abriss bei Altmann und Gubrud (2017, S. 22 ff.) zeigt, wurden damals erstmals in gr&#246;&#223;erem Umfang
Minen sowohl zu Land als auch zur See eingesetzt, die mit ihrem automatischen Z&#252;ndmechanismus als sehr
einfache Varianten von AWS verstanden werden k&#246;nnen. Au&#223;erdem wurde bereits, wenn auch nicht besonders
erfolgreich, an der Entwicklung von per Kabel fernsteuerbaren Waffen (z. B. Torpedos und mit Sprengstoff
best&#252;ckten Motorbooten) gearbeitet, zudem kamen erste unbemannte motorisierte Luftfahrzeuge zur
Erprobung. In den folgenden Jahrzehnten waren eine stetige Automatisierung der Waffentechnik sowie etliche
Innovationssch&#252;be insbesondere bei luftgest&#252;tzten unbemannten Systemen zu beobachten. Wichtige Meilensteine 
umfassten die &#187;Fieseler 103&#171;, auch bekannt als &#187;V1&#171;, die als erster Marschflugk&#246;rper gilt und von
Nazideutschland im Zweiten Weltkrieg eingesetzt wurde, sowie die US-amerikanischen Ryan-Firebee-Aufkl&#228;rungsdrohnen,
die ab den 1960er Jahren in gro&#223;er Zahl u. a. im Vietnamkrieg zum Einsatz kamen und als Vorl&#228;ufer heutiger
UAVs gelten. Im Jom-Kippur-Krieg (1973) wurden dann erstmalig bewaffnete Drohnen in gr&#246;&#223;erem Umfang
von Israel gegen feindliche Stellungen eingesetzt; es handelte sich dabei um mit Luft-Boden-Raketen
ausgestattete Firebee-Drohnen, die per Video kontrolliert wurden. Aber erst die Fortschritte in Elektronik und
Computertechnik erm&#246;glichten schlie&#223;lich ausgefeiltere und zunehmend autonome Lenkverfahren (vor allem
Gel&#228;ndefolgeflug), was in den sp&#228;ten 1970er Jahren zur Konstruktion einer neuen Generation von
Marschflugk&#246;rpern f&#252;hrte, die selbstst&#228;ndig Abwehrstelllungen umfliegen sowie Nuklear- oder auch konventionelle Waffen
mit gro&#223;er Genauigkeit ans Ziel bringen konnten.
Heute sind unbemannte Waffensysteme vor allem in Form von bewaffneten Drohnen fester Bestandteil 
der modernen Kriegsf&#252;hrung. Im Folgenden werden basierend auf den Gutachten von Altmann und Gubrud
(2017) sowie Alwardt et al. (2017) die Proliferationstendenzen bei unbemannten Systemen, deren aktuelle
Autonomiegrade sowie die Trends in der autonomiebezogenen FuE &#252;berblicksartig zusammengefasst.
In den letzten 10 Jahren hat die Zahl der staatlichen und nichtstaatlichen Akteure, die unbemannte
Waffensysteme besitzen und teilweise auch bereits eingesetzt haben, stark zugenommen. Diese Entwicklung geht
fast ausschlie&#223;lich auf das Konto von Kampfdrohnen, die mit Abstand zu den am h&#228;ufigsten produzierten und
weitesten verbreiteten UWS geh&#246;ren (dazu und zum Folgenden Alwardt et al. 2017, S. 16 ff.). Zwar w&#228;chst
auch die Bedeutung von unbemannten Boden- und Wasserfahrzeugen (Kap. 4.1.2 u. 4.1.3). Da deren
Einsatzf&#228;higkeiten derzeit jedoch noch weitgehend auf nichtletale Zwecke begrenzt sind (Aufkl&#228;rung, &#220;berwachung,
Logistik etc.) und zur Proliferation kaum &#246;ffentlich zug&#228;ngliche Daten verf&#252;gbar sind, liegt der Fokus der
folgenden Darstellung auf den bewaffneten UAVs. Mit Blick auf die m&#246;glichen Proliferationstendenzen
zuk&#252;nftiger AWS sind dabei vor allem fortschrittliche MALE-Kampfdrohnen (Kap. 4.1.1) von Interesse, die speziell 
f&#252;r eine mittlere Flugh&#246;he (ca. 10 bis 15 km) und lange Reichweiten entwickelt wurden (24 bis 48 Stunden 
Flugdauer).
Noch Anfang der 2000er Jahre waren die USA die einzige Nation, die &#252;ber einen entsprechenden
Drohnentyp verf&#252;gten (&#187;MQ-1 Predator&#171;); der erste Einsatz erfolgte im Oktober 2001 in Afghanistan
(Altmann/Gubrud 2017, S. 81). In den folgenden Jahren bauten die USA ihr Drohnenprogramm massiv aus, was
zur Entwicklung einer leistungsf&#228;higeren MALE-Drohne18 (&#187;MQ-9 Reaper&#171;) und zu unz&#228;hligen bewaffneten
Drohneneins&#228;tzen in Pakistan, Afghanistan sowie dem Nahen Osten f&#252;hrte. Das Bureau of Investigative
Journalism (BIJ) sch&#228;tzt, dass alleine im Zeitraum von Januar 2015 bis Oktober 2018 in Afghanistan 4.978
Drohnenangriffe stattfanden und dabei bis zu 5.346 Personen get&#246;tet wurden.19 Hinzu gekommen sind mindestens
430 Eins&#228;tze in Pakistan (seit 2004), 324 Eins&#228;tze im Jemen (seit 2002) und 100 Eins&#228;tze in Somalia (seit 2007).
Das Vorbild der USA motivierte diverse andere L&#228;nder, selbst Kampfdrohnen zu entwickeln oder
zumindest zu beschaffen (dazu und zum Folgenden Altmann/Gubrud 2017, S. 81). W&#228;hrend 2008 erst 3 Staaten im
Besitz dieses Waffentyp waren (neben den USA noch Gro&#223;britannien und Israel), stieg die Zahl bis 2015 auf
17 Umgangssprachlich oft auch als Drohnen bezeichnet.
18 Dar&#252;ber hinaus f&#252;hrte die U.S. Navy 2009 den unbemannten Hubschrauber &#187;MQ-8 Fire Scout&#171; (Naval Technology o. J.a) in den 
Betrieb ein und die U.S. Army 2012 das unbemannte Kampfluftfahrzeug &#187;MQ-1C Gray Eagle&#171; (GA-ASI o. J.). Beide k&#246;nnen 
selbstst&#228;ndig starten und landen (Alwardt et al. 2017, S.21).
19 https://www.thebureauinvestigates.com/projects/drone-war/charts?show_casualties=1&amp;
show_injuries=1&amp;show_strikes=1&amp;location=afghanistan&amp;from=2015-1-1&amp;to=now (1.9.2020)
15 (New America o. J.b). Es ist davon auszugehen, dass inzwischen mindestens 30 Staaten &#252;ber fortschrittliche
Kampfdrohnen verf&#252;gen (Munich Security Conference 2019, S. 52), davon haben 10 L&#228;nder (einschlie&#223;lich den
USA) Kampfdrohnen bereits unter Waffengebrauch in Kampfhandlungen eingesetzt (New America o. J.c).20 
Betrachtet man das Drohneninventar der einzelnen L&#228;nder genauer, f&#228;llt auf, dass eine gro&#223;e Anzahl von
L&#228;ndern bewaffnete Drohnen importiert hat. Die USA, China sowie Israel stellen die wesentlichen Exportnationen
dar. China soll z. B. bewaffnete Drohnen an &#196;gypten, Irak, Jordanien, Kasachstan, Myanmar, Nigeria, Saudi-
Arabien, Turkmenistan und die Vereinigten Arabischen Emirate verkauft haben (Ewers et al. 2017, S. 12). Die 
USA wiederum exportieren Kampfdrohnen bisher ausschlie&#223;lich an NATO-Partner, n&#228;mlich nach Frankreich,
Gro&#223;britannien und Italien sowie an die Niederlande und Spanien (Cole 2016).
Tab. 4.1 Staatlicher Besitz, Beschaffung und Einsatz von fortschrittlichen Kampfdrohnen
Stand der Proliferation von Kampfdrohnen
Besitz Eigenproduktion China, Georgien, Iran, Israel, Nordkorea, Pakistan,
S&#252;dafrika, T&#252;rkei, Ukraine, USA
Import/Leasing &#196;gyptenc, Aserbaidschanb, Frankreicha, f,
Gro&#223;britannienb, Irakc, Italiena, Kasachstanc, Niederlandea, f,
Nigeriac, Saudi-Arabienc, Spaniena, f, Turkmenistanc,
Vereinigte Arabische Emiratea, c
bisherige Eins&#228;tze unter
Waffengebrauch
Aserbaidschan, Gro&#223;britannien, Irak, Iran, Israel,
Nigeria, Pakistan, T&#252;rkei, USA, Vereinigte Arabische
Emirate
laufender
Beschaffungsprozess
Entwicklung Deutschlande, Frankreiche, Griechenlandd,
Gro&#223;britannien, Indien, Schwedend, Schweizd, Spaniend,
S&#252;dkorea, Taiwan, Vereinigte Arabische Emirate, Russland
Import/Leasing Australiena, g, Deutschlandb, Indienb, Jordanienc,
Poleng, Schweizb
a US-amerikanische Fabrikate
b israelische Fabrikate
c chinesische Fabrikate
d Entwicklung als Teil eines Konsortiums
e Entwicklung als Konsortialf&#252;hrer
f bisher unbewaffnete Kampfdrohne
g unsichere Informationen oder steht noch nicht fest
Wenn keine weiteren Angaben, dann handelt es sich bei den Kampfdrohnen (vorwiegend)
um eigene Entwicklungen. Staaten, die bereits Kampfdrohnen produzieren, werden im Fall 
etwaiger weiterer Entwicklungsprogramme nicht mehr zus&#228;tzlich unter dem Punkt
Entwicklung gelistet.
Quelle: Alwardt et al. 2017, S. 19
Deutschland besitzt bislang keine Kampfdrohnen vom Typ MALE (dazu und zum Folgenden Alwardt et al.
2017, S. 21 f.). Als fortschrittlichstes unbemanntes UAV setzt die Bundeswehr seit 2010 die von Israel geleaste
unbewaffnete Drohne &#187;Heron&#171; der MALE-Kategorie zu Aufkl&#228;rungszwecken in Afghanistan ein und seit
November 2016 auch in Mali. &#220;ber bewaffnungsf&#228;hige Nachfolgemodelle der &#187;Heron&#171; ist in den vergangenen
Jahren kontrovers diskutiert worden. Als m&#246;gliche Alternativen standen zuletzt entweder US-amerikanische 
20 Es gibt bislang keine umfassenden Datensammlungen &#252;ber die weltweiten Drohnenprogramme. Das Programm &#187;World of Drones&#171;
von New America ist eines der wenigen Projekte, das eine Datenliste hierzu anbietet, und wird daher h&#228;ufig &#8211; auch in
wissenschaftlicher Literatur &#8211; zitiert. Dennoch sind diese Daten eher als Richtwerte zu verstehen, da keine umfassenden Quellenangaben
hinterlegt sind (Alwardt et al. 2017, S.16).
        
 
 
 
    
  
    
   
   
    
     
       
        
   
 
       
         
 
  
  
     
     
    
 
   
  
  
   
      
       
 
 
 
 
  
     
  
  
 
     
       
    
    
         
     
 
4.1
&#187;MQ-9 Reaper&#171; oder geleaste israelische &#187;Heron-TP&#171; zur Auswahl. Am 13. Juni 2018 bewilligte der Bundestag 
die Beschaffung von f&#252;nf Drohnen des Typs &#187;Heron TP&#171;, die jedoch vorerst nicht bewaffnet und nur zur
Aufkl&#228;rung eingesetzt werden sollen (BMVg 2018b). &#220;ber eine sp&#228;tere Bewaffnung soll laut Koalitionsvertrag
&#187;nach ausf&#252;hrlicher v&#246;lkerrechtlicher, verfassungsrechtlicher und ethischer W&#252;rdigung&#171; gesondert entschieden 
werden (CDU/CSU/ SPD 2018). Dar&#252;ber hinaus engagiert sich Deutschland in f&#252;hrender Rolle im Rahmen
eines europ&#228;ischen Konsortiums f&#252;r die Entwicklung und Beschaffung einer bewaffnungsf&#228;higen MALE-
Drohne (Bundesregierung 2018e; dazu auch Kap. 4.2.2).
Zwar verf&#252;gen heute auch bereits einzelne nichtstaatliche Gruppierungen (z. B. Hamas, Hisbollah und IS)
&#252;ber unbemannte Waffensysteme in Form einfacher bewaffneter Drohnen (dazu und zum Folgenden Alwardt
et al. 2017, S. 18). Dabei handelt es sich allerdings vorwiegend um Kleinstdrohnen sehr einfacher Bauart. Nach
wie vor sind es also prim&#228;r Staaten, darunter keineswegs nur die gro&#223;en Milit&#228;rm&#228;chte, die die Proliferation
von bewaffneten UAVs vorantreiben und dies auch auf absehbare Zeit tun werden. Es w&#228;re nicht &#252;berraschend,
wenn bei einer Einf&#252;hrung von AWS &#228;hnliche Erfahrungen und ein ebenso schneller Aufwuchs zu erwarten 
w&#228;ren (Altmann/Gubrud 2017, S. 108) &#8211; schlie&#223;lich ist die machtstrategische Bedeutung autonomer Waffen als
sehr gro&#223; einzusch&#228;tzen und mithin die Gefahr eines R&#252;stungswettlaufs nicht von der Hand zu weisen (Kap. 6).
Eine nach L&#228;ndern aufgeschl&#252;sselte &#220;bersicht des Proliferationsstandes von Kampfdrohnen (milit&#228;risch
bewaffnete Drohnen fortschrittlicherer Kategorien), untergliedert nach derzeitigem Besitz, laufender
Beschaffung und dem bisherigen Einsatz findet sich in Tabelle 4.1. Es werden nur bereits stationierte Kampfdrohnen
als Besitz eines Staates gelistet; anhand der vorliegenden Informationen kann jedoch nicht immer mit letzter
Sicherheit abgesch&#228;tzt werden, ob eine Beschaffung nicht inzwischen schon in eine Stationierung gem&#252;ndet ist.
&#220;berblick zu einsatzreifen unbemannten (teil)autonomen Waffensystemen
Autonome Waffensysteme im strengen Sinne des Wortes, also bewaffnete unbemannte Plattformen, die f&#228;hig
sind, im Kampfeinsatz ohne jegliche menschliche Kontrolle zu agieren, gibt es noch nicht. Aber zwischen
Autonomie und Automatisierung besteht ein flie&#223;endes Kontinuum (Kap. 2.2), sodass in verschiedenen
Waffengattungen bereits bewaffnete unbemannte Systeme vorliegen, die &#252;ber einen relativ weitreichenden Grad an
Automatisierung verf&#252;gen und deshalb als Vorl&#228;ufer von AWS klassifiziert werden k&#246;nnen. Im Folgenden wird
anhand bereits verf&#252;gbarer oder in fortgeschrittener Entwicklung befindlicher unbemannter Waffensysteme &#8211;
aufgeschl&#252;sselt nach den Einsatzbereichen Luft, Land und Wasser &#8211; der aktuelle Stand erreichter Autonomie
grob eingeordnet. Der &#220;berblick ist aufgrund der teils nur sp&#228;rlich verf&#252;gbaren Informationen sowie der gro&#223;en
Zahl teilautomatisierter Waffensysteme nicht abschlie&#223;end, sondern soll dazu dienen, einen exemplarischen
Einblick in die aktuellen M&#246;glichkeiten und technischen Gegebenheiten bei unbemannten Waffensystemen zu
geben.
4.1.1 Fliegende Systeme
Nachdem pr&#228;zisionsgelenkte Flugk&#246;rper wie der US-amerikanische &#187;Tomahawk&#171; und ballistische Raketen
jahrzehntelang vorherrschend waren, haben seit Anfang der 2000er Jahre fortschrittliche MALE-Kampfdrohnen
eine weltweite Verbreitung gefunden. Bei diesen handelt es sich um multifunktionale fliegende Plattformen, die
&#252;ber fortgeschrittene Aufkl&#228;rungsfunktionen verf&#252;gen und zus&#228;tzlich mit Lenkflugk&#246;rpern ausgestattet werden 
k&#246;nnen. Inzwischen findet sich in den Arsenalen der f&#252;hrenden Nationen eine beachtliche Vielfalt an
unterschiedlichen Kampfdrohnentypen, die mit einer Vielzahl von Waffen best&#252;ckt sind (Military Factory o. J.a).
Dass bei der Verbreitung unbemannter Waffensysteme Flugger&#228;te vorherrschend sind, h&#228;ngt wesentlich
damit zusammen, dass Navigation, Orientierung und Funkkommunikation in der Luft relativ einfach zu
bewerkstelligen sind &#8211; deutlich einfacher jedenfalls, als dies an Land bzw. auf oder unter Wasser der Fall ist
(Dickow 2015, S. 14). Allerdings sind die autonomen F&#228;higkeiten fliegender unbemannter Systeme insgesamt 
nach wie vor begrenzt:
&#8250; Bei den Kampfdrohnen beschr&#228;nkt sich die Autonomie &#252;blicherweise auf die Flugkontrolle, auf
Navigations- und Aufkl&#228;rungsfunktionen sowie auf die eventuelle R&#252;ckkehr im Falle eines Funkabbruchs; die
Waffenausl&#246;sung geschieht in der Regel per Fernsteuerung und unterliegt somit in letzter Instanz immer noch
menschlicher Kontrolle (Alwardt et al. 2017, S. 19).21 
&#8250; G&#228;ngige Lenkwaffen werden entweder durch aktive menschliche Steuerung (z. B. per Laser-, Radar- oder
Infrarotstrahl) ins Ziel gelenkt, oder sie steuern selbstst&#228;ndig ein vorab definiertes und einprogrammiertes
Ziel an (Altmann/Gubrud 2017, S. 101 f.; Boulanin/Verbruggen 2017, S. 47 ff.). Letzteres wird auch &#187;fire
and forget&#171; genannt. Zur Anwendung kommen dabei Bilderkennung, GPS-Steuerung und
Pr&#228;zisionslenkung &#8211; jedoch in der Regel blo&#223; &#187;als Verst&#228;rkung menschlicher Steuerung [...], da sie klar dazu
beabsichtigt sind, den Flugk&#246;rper zu einem vorher ausgew&#228;hlten Ziel zu lenken&#171; (Altmann/Gubrud 2017, S. 102).
Das milit&#228;rische Interesse an der zunehmenden Integration autonomer Funktionen ist jedoch klar erkennbar,
und es befinden sich bereits Systeme im Einsatz oder in fortgeschrittener Entwicklung, die mit weitreichender
Autonomie ausgestattet sind. Dazu geh&#246;ren zum einen Drohnensysteme, die im Rahmen definierter Parameter
selbstst&#228;ndig auf die Suche nach einem Ziel gehen, um dieses dann autonom zu attackieren &#8211; im Fachjargon
auch als &#187;loitering weapon&#171; bezeichnet (&#187;to loiter&#171;: verweilen, herumlungern) &#8211;, sowie zum anderen taktische 
Lenkflugk&#246;rper, die mit weitentwickelten autonomen Zielerfassungs- bzw. Zielsuchfunktionen ausgestattet 
sind.
Drohnensysteme
Zu den wenigen Drohnensystemen, die &#252;ber relativ weitreichende autonome Angriffsfunktionen verf&#252;gen,
geh&#246;ren drei israelische Systeme: die Drohne &#187;Harpy&#171;, die bereits in den 1990er Jahren entwickelt wurde, deren
Nachfolger &#187;Harop&#171; (2009; Abb. 4.1) sowie &#187;Harpy NG&#171; (2016), bei denen es sich im Wesentlichen um
vergr&#246;&#223;erte und technisch verbesserte Varianten von &#187;Harpy&#171; handelt. Die Drohnen verf&#252;gen &#252;ber eine
Spannweite von 2,1 m (&#187;Harpy&#171;) bzw. 2,5 m (&#187;Harop&#171;) und sind mit Sprengladungen von 32 kg (&#187;Harpy&#171;) bzw.
23 kg (&#187;Harop&#171;) best&#252;ckt. Entwickelt wurden diese Systeme von Israel Aerospace Industries (IAI).
Sowohl &#187;Harpy&#171; als auch &#187;Harop&#171; z&#228;hlen zu den &#187;loitering weapons&#171; und dienen prim&#228;r dem Zweck,
feindliche Flugabwehrsysteme auszuschalten (dazu und zum Folgenden Boulanin/Verbruggen 2017, S. 50 ff.).
Der Start erfolgt &#252;ber an Fahrzeugen befestigte Container. Wie herk&#246;mmliche Drohnen sind sie in der Lage, in 
einem bestimmten Gebiet f&#252;r mehrere Stunden selbstst&#228;ndig zu kreisen, um autonom nach vorab definierten
Zielen (in diesem Falle feindlichen Radarsignalen) zu suchen. Der Unterschied zu herk&#246;mmlichen
Kampfdrohnen ist darin zu sehen, dass sowohl &#187;Harpy&#171; als auch &#187;Harop&#171; nach dem Start fast komplett autonom agieren
k&#246;nnen: Sobald sie ein vorab definiertes Ziel, sprich eine feindliche Radarstation, anhand ihres spezifischen
Signals detektiert haben, gehen sie in den Angriffsmodus &#252;ber und st&#252;rzen sich auf das Zielobjekt, um sich in
dessen N&#228;he zur Explosion zu bringen und es so zu zerst&#246;ren &#8211; in dieser Hinsicht &#228;hneln sie Marschflugk&#246;rpern,
weshalb sie auch Kamikazedrohnen genannt werden. Dieser Selbstzerst&#246;rungseffekt hat zur Folge, dass die
Drohnen hinsichtlich ihrer technischen Ausstattung eher einfach gehalten sind, auch was die
Aufkl&#228;rungsfunktionen betrifft. W&#228;hrend &#187;Harpy&#171; ausschlie&#223;lich autonom agiert (und bei erfolgloser Suche nach einem Ziel
sich selbst zerst&#246;rt) (Defense Update 2006), kann &#187;Harop&#171; zus&#228;tzlich ferngesteuert werden und ist zu diesem
Zweck mit elektrooptischen sowie Infrarotkameras ausgestattet; dies macht es m&#246;glich, auch andere Ziele als
Radarstationen anzugreifen. Im Falle einer erfolglosen Mission kann &#187;Harop&#171; zudem per Fallschirm gelandet
werden (Bumbacher 2016).
Allerdings gibt es auch Stimmen, die spekulieren, die U.S. Air Force k&#246;nnte bereits im Geheimen &#252;ber einsatzf&#228;hige unbemannte 
Kampfdrohnen mit weitreichenden autonomen F&#228;higkeiten verf&#252;gen. So vertritt etwa Rogoway (2016) diese These. Die US-
amerikanischen Entwicklungsprogramme, die diesen Spekulationen zugrunde liegen, werden in Kapitel 4.2.1 vorgestellt.
21
Abb. 4.1 Drohne &#187;Harop&#171;
Quelle: Julian Herzog, Wikimedia Commons, CC-BY-4.0
&#187;Harpy&#171; wurde von Israel an China, Indien, S&#252;dkorea und die T&#252;rkei verkauft, &#187;Harop&#171; an Aserbeidschan,
Indien, die T&#252;rkei und auch an Deutschland (Altmann/Gubrud 2017, S. 87). Bei der Bundeswehr bildet &#187;Harop&#171;
zusammen mit der Aufkl&#228;rungsdrohne &#187;KZO&#171; (&#187;Kleinflugger&#228;t Zielortung&#171;) von Rheinmetall das
Verbundsystem WABEP (Wirkmittel zur abstandsf&#228;higen Bek&#228;mpfung von Einzel- und Punktzielen) (TAB 2011,
S. 42). Da Harop sich beim Einsatz selbst zerst&#246;rt, wird WABEP von der Bundesregierung nicht als
Kampfdrohne eingestuft, sondern als &#187;Wirkmittel (Munition), das dem &#8250;Sch&#252;tzen&#8249; erm&#246;glicht, bis kurz vor dem
Einschlag das Ziel zu beobachten, nachzurichten und notfalls den Angriff abzubrechen&#171; (Bundesregierung 2009,
S. 2).
Ein anderes Beispiel f&#252;r eine mit autonomen Funktionen ausgestattete &#187;loitering weapon&#171; ist ein &#187;Low
Cost Autonomous Attack System&#171; (LOCAAS), das 1998 von der U.S. Air Force (mit Lockheed Martin)
entwickelt wurde (dazu und zum Folgenden Altmann/Gubrud 2017, S. 161 f.). Das Ziel war es, einen kleinen,
kosteng&#252;nstigen Flugk&#246;rper herzustellen, der wie &#187;Harop&#171; eine Hybridform aus Drohne und Lenkflugk&#246;rper
darstellt. Mit einer L&#228;nge von 0,9 m, einer Gesamtmasse von 45 kg und einem Multimodusgefechtskopf
(Penetrator oder Splitter) von 8 kg sollte die Waffe gegen gepanzerte und ungepanzerte Fahrzeuge, anderes Material 
und Personen eingesetzt werden k&#246;nnen, etwa zur Bek&#228;mpfung von mobiler Luftabwehr, mobilen Startger&#228;ten
f&#252;r Boden-Boden-Flugk&#246;rper und langreichweitigen Raketensystemen. Vorgesehen war, dass das von einem
Tr&#228;ger gestartete LOCAAS &#252;ber eine Reichweite von &#252;ber 200 km verf&#252;gt, sich einem vorher definierten
Zielbereich mittels GPS und Tr&#228;gheitsnavigation n&#228;hern und dort ungef&#228;hr eine halbe Stunde lang nach Zielen
suchen kann. Diese sollten mittels Laserradar anhand ihrer dreidimensionalen Geometrie durch Vergleich mit
gespeicherten Zielsignaturen gefunden und erkannt werden. Das System sollte in der Lage sein, zwischen
unterschiedlichen Typen von Zielen sowie zwischen legitimen Zielen und Nichtkombattanten zu unterscheiden
(Global Security o. J.a). F&#252;r die U.S. Navy wurde der Einsatz in Schw&#228;rmen und mit optionaler
Datenverbindung f&#252;r einen Fernsteuerungsmodus ins Auge gefasst. Das Programm wurde Mitte der 2000er Jahre
abgebrochen, offenbar aufgrund von Bedenken hinsichtlich Verl&#228;sslichkeit, Steuerbarkeit und Rechtm&#228;&#223;igkeit (Gubrud
2015).
Lenkflugk&#246;rper
Ein Lenkflugk&#246;rper22, der bereits &#252;ber weitreichende autonome Funktionen verf&#252;gt, ist die britische &#187;
Brimstone&#171; von Matra BAe Dynamics A&#233;rospatiale S.A.S (MBDA) (dazu und zum Folgenden Altmann/Gubrud 2017,
S. 104 f.). Es handelt sich dabei um einen Luft-Bodenflugk&#246;rper mit einer Reichweite von 12 bis 24 km, der
einen Millimeterwellen-Radarsuchkopf (MMW-Suchkopf) nutzt, um Fahrzeuge zu orten und anzufliegen (Mar-
Lenkflugk&#246;rper werden wie Torpedos &#252;blicherweise nicht zu den AWS gez&#228;hlt, da sie nur f&#252;r den einmaligen Einsatz gedacht sind
und es sich also nicht im engeren Sinne um bewaffnungsf&#228;hige Tr&#228;gersysteme bzw. Plattformen handelt. Sie werden hier dennoch
erl&#228;utert, da sie teilweise &#252;ber fortgeschrittene autonome Funktionen verf&#252;gen und deshalb als wichtige Vorstufen von AWS
anzusehen sind.
22
kowitz/Gresham 2012). Die Entwicklung begann 1996, und die ersten Flugk&#246;rper wurden 2005 in Dienst
gestellt. Er ist zum Einsatz gegen gepanzerte Fahrzeuge vorgesehen, mit einem Tandemgefechtskopf, in dem eine
kleine Vorausladung zuerst reaktive Panzerung entfernt und ein zweiter Hohlladungsgefechtskopf schwere
Panzerung durchdringt. In einem Katalog der Royal Air Force (RAF 2003, S. 87) von 2003 wird &#187;Brimstone&#171; als
eine &#187;v&#246;llig autonome, Feuern-und-vergessen-Antipanzerwaffe&#171; angepriesen. Insbesondere wird bemerkt, dass
die Waffe, einmal gestartet, v&#246;llig eigenst&#228;ndig nach gepanzerten Zielen suchen kann, auf Basis von
gespeicherten Zielsignaturen. Nichtpassende Ergebnisse (wie Autos, Busse, Geb&#228;ude) werden zur&#252;ckgewiesen und
mit der Suche und dem Vergleich so lange fortgefahren, bis ein g&#252;ltiges Ziel identifiziert ist. Analog zu den
vorab beschriebenen Drohnensystemen kann der Brimstone-Flugk&#246;rper so programmiert werden, dass er erst
nach Zielen zu suchen beginnt, sobald ein definiertes Zielgebiet erreicht ist (Boulanin/Verbruggen 2017, S. 49 
f.). Dies soll es laut der Royal Air Force erm&#246;glichen, freundliche Kr&#228;fte sicher zu &#252;berfliegen und
Kollateralsch&#228;den zu vermeiden.
2007 bekam MBDA den Auftrag, Brimstone-Flugk&#246;rper zus&#228;tzlich zum MMW-Suchkopf mit einem
halbaktiven Lasersuchkopf (&#187;semi-active laser&#171; &#8211; SAL) nachzur&#252;sten (dazu und zum Folgenden Altmann/Gubrud 
2017, S. 105). Grund daf&#252;r war offenbar, dass sich der vollautonome Modus (&#187;fire and forget&#171;) in
asymmetrischen Konflikten wie dem Afghanistaneinsatz als problematisch herausstellte, da Zivilisten und irregul&#228;re
Kombattanten anhand der MMW-Signaturen kaum voneinander zu unterscheiden waren (Markowitz/Gresham
2012). Bei der neu hinzugef&#252;gten SAL-Zielsuchlenkung muss ein menschlicher Bediener einen Laserstrahl auf
dem Ziel halten und der Bediener kann den Flugk&#246;rper bis einige Sekunden vor dem Aufschlag umlenken. Die
SAL- und MMW-Zielanflugarten k&#246;nnen f&#252;r bessere Genauigkeit kombiniert werden, insbesondere bei
bewegten Zielen. SAL kann auch einzeln benutzt werden, um den Flugk&#246;rper auf Ziele zu lenken, die durch Radar
schwer zu detektieren sind (z. B. nichtmetallische Objekte) (MDBA 2018). Der Dual-Mode-Brimstone wurde
in Afghanistan, Irak, Libyen und auch Syrien eingesetzt (Think Defence o. J.).
Laut Berichten von 2016 (Max 2016) plant die deutsche Luftwaffe die Anschaffung von Dual-Mode-
Brimstone f&#252;r den &#187;Eurofighter&#171;, genauere Ausk&#252;nfte dazu hat die Bundesregierung (2018a) jedoch k&#252;rzlich aus
Geheimhaltungsgr&#252;nden verweigert. Diese Anschaffung w&#228;re insofern brisant, als der Brimstone-
Lenkflugk&#246;rper in seiner Dual-Mode-Variante zwar oft als Mensch-in-der-Schleife-Waffe angepriesen wird, der
Lenkflugk&#246;rper jedoch weiterhin in seinem v&#246;llig autonomen Modus mit MMW-Suchkopfmodus benutzt werden kann
&#8211; und im Gefecht offenbar auch entsprechend eingesetzt wird. So feuerte das britische Milit&#228;r im Libyenkrieg
2011 erstmals eine gr&#246;&#223;ere Salve von ca. 24 Flugk&#246;rpern auf eine Ansammlung gepanzerter Fahrzeuge des
Gaddafi-Regimes ab, wobei mehrere feindliche Fahrzeuge zerst&#246;rt wurden. Laut Pressemitteilung der britischen
Regierung (MOD 2011) wurde dabei nur der MMW-Suchkopf im autonomen Modus benutzt.
Ebenfalls &#252;ber hochentwickelte autonome F&#228;higkeiten bei deutlich h&#246;herer Reichweite soll der
Antischiffsflugk&#246;rper &#187;LRASM&#171; verf&#252;gen (offizielle Bezeichnung &#187;AGM-158C&#171;), dessen Entwicklung von den 
USA seit 2009 vorangetrieben wird (dazu und zum Folgenden Altmann/Gubrud 2017, S. 103). Obwohl er sich 
derzeit noch in der Testphase befindet und nur sp&#228;rliche Informationen verf&#252;gbar sind, eilt ihm bereits der Ruf
voraus, hinsichtlich Autonomie eines der fortschrittlichsten Waffensysteme weltweit zu sein. Hinter dem
Programm &#187;LRASM&#171; steht das strategische Entwicklungsziel, die Reichweite der Antischiffsflugk&#246;rper der U.S.
Navy vor allem im westlichen Pazifik zu vergr&#246;&#223;ern (Roblin 2018). Zu diesem Zweck wurde &#187;LRASM&#171;, der
eine Modifikation des vorhandenen Luft-Boden-Marschflugk&#246;rpers &#187;JASSM-ER&#171; (&#187;joint air-to-surface
standoff missile &#8211; extended range&#171;) darstellt, mit autonomen Funktionen ausgestattet, die es erlauben sollen,
unabh&#228;ngig von pr&#228;zisen Informationen und funktionierenden Kommunikationsverbindungen auch weitentfernte,
bewegliche Ziele zu identifizieren und zu bek&#228;mpfen. Erreicht werden soll dies mittels KI-basierter
Zielidentifizierung, die mit den Daten eines multimodalen Suchkopfes gespeist wird: basierend auf passiver und aktiver
Radarzielsuchlenkung, elektronischen Unterst&#252;tzungsma&#223;nahmen (&#187;electronic support measures&#171; &#8211; ESM),
Infrarotbildgebung und st&#246;rresistentem GPS (NavalTechnology o. J.b). Au&#223;erdem verf&#252;gt der Flugk&#246;rper &#252;ber 
einen Datenlink, &#252;ber den die Zielvorgaben w&#228;hrend des Fluges aktualisiert werden k&#246;nnen. Das intelligente
Lenksystem von &#187;LRASM&#171; soll f&#228;hig sein, seinen Weg autonom zu ver&#228;ndern und sich mit Flugk&#246;rpern in 
einer Salve oder einem Schwarm zu koordinieren. W&#228;hrend sie zum endg&#252;ltigen Angriff im Sinkflug
&#252;bergehen, sollen die Flugk&#246;rper elektronische Gegengegenma&#223;nahmen (&#187;electronic counter-countermeasures&#171;)
nutzen k&#246;nnen, um feindliches Radar zu t&#228;uschen (Roblin 2018). Die erhebliche Gr&#246;&#223;en- und Gewichtsb&#252;rde durch
die Hinzuf&#252;gung dieser Komponenten hat jedoch zur Folge, dass die Reichweite von &#187;LRASM&#171; mit ca. 600 km
erheblich geringer ist als die des &#187;JRASSM-ER&#171; (930 km Reichweite), wenngleich immer noch um ein
Vielfaches h&#246;her als die von &#187;Brimstone&#171; oder des norwegischen &#187;Naval Strike Missile&#171; (NSM).23 
LRASM wurde bereits mehrfach erfolgreich getestet und offenbar wurden im Mai 2019 die ersten
Exemplare an die US-Luftwaffe ausgeliefert (Defense Industry Daily 2019).
4.1.2 Bodensysteme
Milit&#228;rische Roboter, die am Boden operieren, sind grunds&#228;tzlich mit deutlich komplexeren Anforderungen 
konfrontiert als ihre fliegenden Pendants (Altmann/Gubrud 2017, S. 83 f.; Dickow 2015, S. 14): Sie haben
keinen freien Luftraum vor sich, sondern teils unwegsames, un&#252;bersichtliches Gel&#228;nde. Das erschwert die
Navigation, vor allem abseits von Stra&#223;en, und je nach Bodenbeschaffenheit auch die Fortbewegung. Diese
Einschr&#228;nkungen gelten jedoch nur f&#252;r Fahrzeuge und nicht f&#252;r station&#228;re Waffensysteme, sodass es prinzipiell
einfacher ist, Letztere mit autonomen Funktionen auszustatten.
Station&#228;re Systeme
Bodengest&#252;tzte Waffensysteme, die &#252;ber keine eigene Fortbewegungsf&#228;higkeit verf&#252;gen, haben in aller Regel
ein stark eingegrenztes Funktions- und Einsatzspektrum, das meist defensiver Natur ist. In ihrer einfachsten
Variante handelt es sich um Sprengfallen oder Minen, die &#8211; sofern sie direkt gegen Personen gerichtet sind
(Antipersonenminen) &#8211; gem&#228;&#223; der Ottawa-Konvention24 von 1997 inzwischen v&#246;lkerrechtlich ge&#228;chtet sind.
Das Problem bei Antipersonenminen ist, dass sie nicht zwischen Kombattanten und Nichtkombattanten
unterscheiden k&#246;nnen, wof&#252;r der automatische Z&#252;ndmechanismus verantwortlich ist. Mittels der Nutzung von KI 
und geeigneter Sensorik k&#246;nnten Antipersonenminen im Prinzip jedoch so weiterentwickelt werden, dass eine 
entsprechende Diskriminierung m&#246;glich wird und dementsprechend der Grund f&#252;r die v&#246;lkerrechtliche &#196;chtung
entfallen w&#252;rde.
Ein Schritt in diese Richtung stellt das &#187;M7 Spider Network Command Munition System&#171; (&#187;M7 Spider&#171;)
dar, ein vernetztes Antipersonensystem, das im Kern aus sechs sternf&#246;rmig angeordneten Granatwerfern kurzer
Reichweite besteht (&#187;munition control unit&#171; &#8211; MCU) (dazu und zum Folgenden Altmann/Gubrud 2017, S. 96 
ff.).25 Jeder dieser Granatwerfer ist mit einem Stolperdraht verbunden, dessen Aktivierung jedoch nicht direkt
zur Waffenausl&#246;sung f&#252;hrt; stattdessen wird ein Signal an eine Fernsteuereinheit gesendet (&#187;remote control 
unit&#171; &#8211; RCU), &#252;ber die mehrere MCUs aus der Distanz kontrolliert werden k&#246;nnen (ALT 2011, S. 296 f.). 
Aufgrund dieser Konfiguration ist zwar grunds&#228;tzlich sichergestellt, dass sich immer noch ein Mensch in der
Schleife befindet. Da die Entfernung zwischen RCU (und damit dem menschlichen Bediener) und MCU bis zu
1,5 km betragen kann, ist allerdings fraglich, ob in Gefechtssituationen ein f&#252;r die Waffenbedienung
ausreichendes Situationsverst&#228;ndnis garantiert ist.
Ebenfalls der Personenabwehr dienen robotische Wachpostenkanonen, wie sie von mindestens zwei
Nationen &#8211; von Israel (entlang des Gazastreifens) sowie S&#252;dkorea (entlang der demilitarisierten Zone) &#8211; stationiert
worden sind.26 Diese fest installierten Systeme sind mit Schusswaffen ausgestattet und verwenden keine
Explosivstoffe, weshalb sie nicht als Landminen gelten. Obwohl vermutlich sowohl die israelischen Sentry-Tech-
Stationen als auch der s&#252;dkoreanische &#187;SGR-A1&#171; &#252;ber alle technischen Voraussetzungen f&#252;r einen
vollautomatischen Betrieb verf&#252;gen, werden sie &#8211; zumindest laut offiziellen Verlautbarungen &#8211; nur unter menschlicher
Aufsicht betrieben (Altmann/Gubrud 2017, S. 99 f.; Cavanaugh 2016).
23 Der &#187;NSM&#171; ist ebenfalls eine Antischiffswaffe und verf&#252;gt bei einer Reichweite von etwa 185 km &#252;ber autonome F&#228;higkeiten, die
dem &#187;LRASM&#171; vergleichbar sind. F&#252;r genauere Informationen siehe Boulanin/Verbruggen (2017, S.50).
24 &#220;bereinkommen &#252;ber das Verbot des Einsatzes, der Lagerung, der Herstellung und der Weitergabe von Antipersonenminen und
&#252;ber deren Vernichtung
25 &#187;M7 Spider&#171; geht auf das &#187;intelligent munition system&#171; (IMS) zur&#252;ck, das ab 2006 im Rahmen des inzwischen eingestellten
Programms &#187;Future-Combat-Systems&#171; (FCS) (Gubrud/Altmann 2013) der U.S. Army entwickelt wurde. Die Idee hinter IMS war, ein
vernetztes, intelligentes Minensystem zu schaffen, bestehend aus Bodensensoren und Munitionsverteilern, die entweder autonom
oder auf Befehl ausgel&#246;st werden k&#246;nnen (Altmann/Gubrud 2017, S.97). Das IMS sollte sowohl Antipersonengranaten als auch
Antipanzerstreumunition umfassen. Offiziell wurde die Entwicklung des IMS zusammen mit dem FCS 2009 beendet, woraufhin 
Textron Systems die Entwicklung von &#187;M7 Spider&#171; in Eigenregie weiterf&#252;hrte.
26 2016 wurde vermeldet, dass auch die T&#252;rkei begonnen hat, automatische Kanonent&#252;rme, hergestellt von der Firma Aselsan, an der
Grenze zu Syrien zu installieren (Yeni &#350;afak 2016). &#220;ber die technische Ausstattung sowie die F&#228;higkeiten dieser Systeme ist
jedoch fast nichts bekannt (Altmann/Gubrud 2017, S.100).
Hinsichtlich seiner autonomen F&#228;higkeiten gilt der von Samsung Techwin entwickelte &#187;SGR-A1&#171; als
besonders fortgeschritten. Er verf&#252;gt &#252;ber eine Tageslicht- sowie eine Infrarotkamera, mittels derer er potenzielle
Ziele auf eine Distanz von bis zu 4 km aufsp&#252;ren und &#252;berwachen k&#246;nnen soll. Da der Roboter prim&#228;r f&#252;r den 
Einsatz in der demilitarisierten Zone entwickelt wurde, in der sich kein Mensch aufhalten darf, und es sich
zudem um ein station&#228;res System handelt, sind nur rudiment&#228;re Diskriminierungsf&#228;higkeiten erforderlich und
auch implementiert worden. Mittels Mustererkennung soll der Roboter nicht nur f&#228;hig sein, Menschen von
anderen Objekten zu unterscheiden, sondern auch einen sich ergebenden Soldaten erkennen zu k&#246;nnen; au&#223;erdem
soll er &#252;ber ein Sprachinterface verf&#252;gen, &#252;ber das sich n&#228;hernde Personen gewarnt und ggf. identifiziert werden
k&#246;nnen (Global Security o. J.b). Ausger&#252;stet mit einem leichten Maschinengewehr ist der Roboter so zumindest
prinzipiell in der Lage, Angreifer automatisch zu bek&#228;mpfen, auch wenn ein derartiges Einsatzszenario wie 
bereits erw&#228;hnt offiziell dementiert wird (Rabinoff 2010). Auch wo und in welcher Zahl die Systeme eingesetzt
werden, ist bis zum heutigen Tag noch nicht klar (Altmann/Gubrud 2017, S. 100). &#220;ber experimentelle
Stationierungen entlang der koreanischen demilitarisierten Zone sowie im Irak und in Afghanistan (zum Schutz
s&#252;dkoreanischer Milit&#228;rpr&#228;senz) wurde berichtet, aber &#252;ber Anzahl und eventuelle dauerhafte Stationierungen
wurde nichts &#246;ffentlich gemacht (Cavanaugh 2016; Rabinoff 2010).27 
Bei den hier beschriebenen robotischen Wachpostenkanonen handelt es sich im Prinzip um Sonderformen
fernbedienbarer Waffenstationen (&#187;remote weapon station&#171; &#8211; RWS), die auf Plattformen, verschiedenen
Fahrzeugen oder auch Schiffen bereits weitverbreitet zum Einsatz kommen (Cavanaugh 2016). Das Spektrum der
Bewaffnung von RWS-Systemen ist breitgef&#228;chert und reicht von leichten Maschinengewehren &#252;ber
Granatwerfer bis hin zu Kurzstreckenflugk&#246;rpern. Um aus der Ferne bedient werden zu k&#246;nnen, sind RWS
&#252;blicherweise mit hochentwickelten optoelektrischen Sensorsystemen ausgestattet. Dank moderner Computertechnik 
ist es heutzutage m&#246;glich, und dies wird auch angestrebt, grundlegende Waffenfunktionen wie Zielerfassung, -
verfolgung und sogar Feuerleitung zunehmend zu automatisieren. Dies ist nicht nur zur Personenabwehr an
umstrittenen Grenzverl&#228;ufen n&#252;tzlich, sondern vor allem und mehr noch zur Luftabwehr im Nahbereich. Die
Bedienung entsprechender Abwehrsysteme erfordert n&#228;mlich so kurze Reaktionszeiten, dass Menschen damit
typischerweise &#252;berfordert sind. Aus diesem Grund sind Nahbereichsflugabwehrsysteme wie das deutsche
&#187;MANTIS&#171; (Rheinmetall Defence o. J.), das US-amerikanische Phalanx &#187;CIWS&#171; (Raytheon Missiles &amp;
Defense (o. J.b) oder das israelische &#187;Iron Dome&#171; (RAFAEL o. J.a) mit weitreichenden automatisierten
Funktionen ausgestattet, die sie in die Lage versetzen, auch kleine, schnellbeweglich Ziele mittels Radar zu
identifizieren, anhand des Radarquerschnitts zu klassifizieren und schlie&#223;lich per Maschinenkanone oder Abfangrakete
zu bek&#228;mpfen &#8211; alles prinzipiell ohne menschliches Zutun (Dickow 2015, S. 9).
Das von der Rheinmetall Defence entwickelte &#187;MANTIS&#171; dient u. a. dem Schutz von Feldlagern vor
Raketen-, Artillerie- und M&#246;rserangriffen und wird derzeit von der Bundeswehr in Mali eingesetzt (BMVg 2018c). 
Es ist modular aufgebaut und besteht aus einer Bedien- und Feuerleitzentrale (BFZ), mehreren Radareinheiten
und 35-mm-Gesch&#252;tzen (Abb. 4.2). Das Radar ist auf die Erkennung kleiner Objekte optimiert und soll in der
Lage sein, Ziele in der Gr&#246;&#223;e eines Tennisballs auf 20 km Entfernung zu erkennen; die maximale Reichweite
des Gesch&#252;tzes wiederum betr&#228;gt 5 km (Wikipedia 2009a). Auch wenn &#187;MANTIS&#171; als hochautomatisiertes
System gilt, das nach Scharfstellung prinzipiell vollst&#228;ndig eigenst&#228;ndig agieren k&#246;nnte, sehen die Einsatzregeln
der Bundeswehr derzeit vor, dass die Waffenausl&#246;sung unter der Kontrolle eines menschlichen Bedieners zu
stehen hat (DBWV 2016).
Von der s&#252;dkoreanischen DoDamm Systems Ltd. stammt das Konkurrenzsystem &#187;Super aEgis II&#171;. Hierbei handelt es sich ebenfalls
um einen station&#228;ren Maschinengewehrturm, der menschliche Ziele auf mehr als 2 km Entfernung erkennen und bek&#228;mpfen k&#246;nnen
soll (Blain 2010). Das Ger&#228;t ist daf&#252;r ebenfalls mit zahlreichen Kameras ausgestattet und soll theoretisch &#8211; wie &#187;SGR-A1&#171; auch &#8211;
&#252;ber einen vollautonomen Einsatzmodus verf&#252;gen, wenngleich in der Regel f&#252;r die Waffenausl&#246;sung eine menschliche Eingabe
vorgesehen ist (Parkin 2015). &#187;Super aEgis II&#171; soll sich im Mittleren Osten bereits an verschiedenen Orten im Einsatz befinden.
27
Abb. 4.2 Gesch&#252;tz des Flugabwehrsystems &#187;MANTIS&#171;
Quelle: Frank Vincentz, Wikimedia Commons, CC-BY-SA-3.0
Bodenfahrzeuge
Bei unbemannten Bodenfahrzeugen (UGVs) sind autonome Funktionen aus den eingangs genannten Gr&#252;nden 
erheblich schwieriger zu realisieren als bei station&#228;ren Waffensystemen: Sowohl die Anforderungen an die
Umgebungserfassung als auch an die Verhaltensplanung sind wesentlich h&#246;her, ebenso die mechanischen
Herausforderungen vor allem bei der Fortbewegung in unwegsamem Terrain (Dickow 2015, S. 14). Deshalb liegt der
Entwicklungsstand bei den robotischen UGVs insgesamt deutlich hinter dem von UAVs wie auch dem von
station&#228;ren Bodensystemen zur&#252;ck. Die milit&#228;rischen UGV-Systeme, die sich bereits im Einsatz befinden, sind
in aller Regel nicht bewaffnet und haben vor allem unterst&#252;tzende Funktion: Sie dienen z. B. Transportzwecken
(wie das SMSS28 von Lockheed Martin) oder der Aufkl&#228;rung und Minenr&#228;umung (wie der in gro&#223;er St&#252;ckzahl
produzierte Kleinroboter &#187;PackBot&#171; [Army Technology o. J.b] der formaligen iRobot Corp.). Sowohl das
SSMS wie auch der &#187;PackBot&#171; wurden von den USA in Afghanistan eingesetzt. Eine der wenigen bewaffneten
Ausnahmen bei den komplett ferngesteuerten UGVs ist das von Qinetiq North America seit 2008 angebotene
Modular &#187;Advanced Armed Robotic System&#171; (MAARS), ein 167 kg schwerer Kleinroboter, der optional mit
Sprenggranaten und Maschinengewehr ausger&#252;stet werden kann und speziell f&#252;r Erkundungsmissionen
entwickelt wurde (Altmann/Gubrud 2017, S. 84; Qinetiq North America 2018).
Gleichwohl wird intensiv an bewaffneten UGVs gearbeitet, die mit komplexeren autonomen F&#228;higkeiten
ausgestattet sind. Was Navigation und Umgebungserfassung angeht, gibt es dabei naturgem&#228;&#223; gro&#223;e
Schnittmengen mit der zivilen Forschung zum autonomen Fahren. So spielte die DARPA eine Schl&#252;sselrolle beim
Ansto&#223; entsprechender Entwicklungen, indem sie drei wegweisende Wettbewerbe f&#252;r autonome Fahrzeuge
durchf&#252;hrte: 2004 und 2005 die &#187;Grand Challenges&#171; in einer W&#252;ste sowie 2007 den &#187;Urban Challenge&#171; in einer
st&#228;dtischen Umgebung (Siciliano et al. 2009). Ziel war es, die prinzipielle Machbarkeit des autonomen Fahrens
zu demonstrieren, was auch grunds&#228;tzlich gelang (Berger/Rumpe 2008) und vor allem der zivilen Forschung 
sichtbaren Aufschwung gab.29 
Inzwischen sind aber auch aus dem milit&#228;rischen Bereich erste Entwicklungserfolge zu vermelden (zum
Folgenden Altmann/Gubrud 2017, S. 84 f.; Alwardt et al. 2017, S. 22 f.):
28 Das SMSS z&#228;hlt zu den gr&#246;&#223;ten verf&#252;gbaren UGVs (L&#228;nge: 3,6 m; Gewicht: 1,7 t) und ist in der Lage, schwere Lasten zu transportieren.
Es verf&#252;gt &#252;ber verschiedene Steuerungsmodi: Es kann einer vorab spezifizierten Person automatisch folgen, autonom eine
vordefinierte Route abfahren und auch ferngesteuert werden (Army Technology o.J.a). Laut Lockheed Martin ist die langfristige Vision, auf
Basis des SMSS Varianten von autonomen UGVs aufzubauen, die jeweils unterschiedliche milit&#228;rische Zwecke erf&#252;llen und teils auch
bewaffnet sein sollen (https://lockheedmartin.com/en-us/products/smss.html; 30.12.2019).
29 In Europa findet seit 2006 j&#228;hrlich der &#187;European Land-Robot Trial&#171; (ELROB) statt, die abwechselnd milit&#228;rischen und zivilen
Zielen gewidmet ist (http://www.elrob.org/; 1.9.2020). Anders als die von der DARPA veranstalteten Challenges handelt es sich
nicht um Innovationswettbewerbe im engeren Sinne, sondern um eine reine Leistungsschau im Bereich europ&#228;ischer
Landrobotiksysteme. Getestet wird jeweils anhand realistischer Einsatzszenarien.
Einer der Vorreiter bei UGVs ist Israel, das zur Grenzkontrolle mit einer ganzen Reihe teils bewaffneter
oder zumindest bewaffnungsf&#228;higer UGVs operiert. Dazu geh&#246;ren das gepanzerte Fahrzeug &#187;Guardium&#171; 
(Abb. 4.3), 2008 erstmals erprobt, mit diversen Kameras und Sensoren ausger&#252;stet und prim&#228;r f&#252;r
Patrouillenfahrten an der Grenze zum Gazastreifen eingesetzt, sowie das darauf basierende &#187;AvantGuard&#171;.30 Beide
Fahrzeuge k&#246;nnen optional bewaffnet werden und sollen semiautonom &#8211; sei es einem vorausfahrenden Fahrzeug
folgend oder entlang vordefinierter Routen &#8211; oder ferngesteuert ein begrenztes Gebiet &#252;berwachen k&#246;nnen, z. B. 
um improvisierte Sprengk&#246;rper aufzusp&#252;ren und zu neutralisieren (Army Technology o. J.c). 2017 wurde von
der israelischen Meteor Aerospace Ltd. zudem das UGV &#187;Rambow&#171; vorgestellt (Ahronheim 2017). Es ist mit
einem leisen Elektromotor ausger&#252;stet und verf&#252;gt ebenfalls &#252;ber semiautonome Fahrfunktionen, die optionale
Bewaffnung ist aber auch &#8211; wie bei &#187;Guardium&#171; und &#187;AvantGuard&#171; &#8211; ferngesteuert (Meteor Aerospace o. J.).
Abb. 4.3 Das israelische UGV &#187;Guardium&#171;
Quelle: Israel Defense Forces, Wikimedia Commons, CC-BY-SA-3.0
Seit einigen Jahren zeigt sich Russland bei bewaffneten UGVs besonders aktiv (Hambling 2014b). Die
entsprechenden Entwicklungsbem&#252;hungen passen zum offiziellen Ziel, bis etwa 2025 30 % der russischen Kampfkraft
durch ferngesteuerte und robotische Plattformen bereitzustellen (Eshel 2015). Ein erster Schritt dahin ist bereits
gemacht: 2014 wurde angek&#252;ndigt, dass bewaffnete UGVs, die auch ohne menschliche Beteiligung schie&#223;en
k&#246;nnen sollen, f&#252;r die Bewachung von mehreren Raketenbasen eingesetzt werden (Hambling 2014a). Ferner
wurde 2016 der vom Kalaschnikow Konzern entwickelte Panzer &#187;BAS-01G-Soratnik&#171; pr&#228;sentiert; ausgestattet
mit Maschinengewehr und Panzerabwehrrakete soll dieser in erster Linie dem Infanterieschutz dienen (Military
Factory o. J.b). Daneben gibt es eine ganze Reihe von weiteren russischen Entwicklungen im Bereich
bewaffneter UGVs: Die bemerkenswertesten sind in aufsteigender Gewichtsklasse &#187;Nerekhta&#171; (Army Recognition
2016), &#187;Uran-9&#171; (Army Technology o. J.d) und &#187;Vikhr&#171;. Diese Kettenfahrzeuge k&#246;nnen schwere
Maschinengewehre, Granatwerfer und Panzerabwehrlenkflugk&#246;rper tragen. Der &#187;Uran-9&#171;, der entweder eine
vorprogrammierte Route abfahren kann oder &#252;ber eine Distanz von bis zu 3 km per Fernsteuerung bedient wird, wurde laut
Pressemeldungen bereits im Syrienkrieg eingesetzt, offenbar aber mit entt&#228;uschendem Ergebnis (Brown 2018).
Auch asiatische L&#228;nder wie China und Indien investieren verst&#228;rkt in die milit&#228;rische UGV-Entwicklung.
So veranstaltete China in den Jahren 2014 und 2016 je einen Wettbewerb f&#252;r autonome UGVs nach dem Vorbild
der DARPA, an dem gr&#246;&#223;ere und kleinere unbewaffnete mobile Bodensysteme teilnahmen (Ray et al. 2016,
S. 70). Das kleine, bewaffnete Kettenfahrzeug &#187;Sharp Claw 1&#171; (Masse 120 kg) und ein noch kleinerer &#187;Battle 
Robot&#171; wurden schon 2014 vorgestellt (Lin/Singer 2014; Wong 2014). In Indien wiederum hat die staatliche
Defence Research and Development Organisation (DRDO) laut inoffiziellen Meldungen neben unbewaffneten
Kleinfahrzeugen, u. a. zur Entsch&#228;rfung von Sprengfallen, die zwei bewaffneten UGVs &#187;Rudra&#171; und &#187;Daksh
Warrior&#171; entwickelt (Indian Defense Blog 2017).
Die f&#252;r das &#187;Guardium&#171; von Israel Aerospace Industries und Elbit Systems speziell gegr&#252;ndete Firma G-nius wurde allerdings 2016 
wegen zu weniger internationaler Verk&#228;ufe geschlossen.
30
Nicht zuletzt arbeiten auch europ&#228;ische R&#252;stungsfirmen an UGV-Systemen: Die franz&#246;sische Nexter S.A.
hat k&#252;rzlich den bewaffneten &#187;Optio-X20&#171; vorgestellt, der f&#252;r Beobachtungsaufgaben und zur
Feuerunterst&#252;tzung gedacht ist; der Kleinpanzer soll &#252;ber autonome Navigationsfunktionen und einen ferngesteuerten 20-mm-
Gesch&#252;tzturm verf&#252;gen. Von der deutschen Firma Rheinmetall stammt der &#187;Mission Master&#171;, ein modular
aufgebautes unbemanntes Mehrzweckbodenfahrzeug, das f&#252;r unterschiedliche Aufgaben &#187;mit verschiedenen
Subsystemen&#171; ausgestattet werden kann. Darunter befindet sich auch eine bewaffnete Version f&#252;r den Schutz von
Einsatztruppen (Rheinmetall Group 2018). Die zum Transport von Ausr&#252;stung vorgesehene Version des &#187;
Mission Master&#171; ist bereits seit 2018 einsatzbereit.31 Sie verf&#252;gt &#252;ber die g&#228;ngigen semiautonomen Fahrfunktionen
und kann u. a. Soldaten automatisch folgen.
4.1.3 Systeme zu Wasser
Auf und unter Wasser besteht im Allgemeinen mehr Raum zum Man&#246;vrieren als auf Land, insbesondere auf
hoher See (Altmann/Gubrud 2017, S. 86). Au&#223;erdem sind die Meere arm an Hindernissen und weitgehend
menschenleer, sodass das Risiko von Kollateralsch&#228;den sehr gering ist. All dies w&#252;rde den Unter- sowie
&#220;berwasserbereich eigentlich f&#252;r den Einsatz autonomer Waffensysteme pr&#228;destinieren. Dass bislang kaum
einsatzf&#228;hige seegest&#252;tzte UWS vorliegen, h&#228;ngt u. a. mit den potenziell riesigen Einsatzr&#228;umen zusammen, die vor
allem im submarinen Bereich mit Blick auf Energieversorgung und Kommunikation eine gro&#223;e
Herausforderung darstellen (Dickow 2015, S. 15).
&#220;berwasserfahrzeuge
Ideen f&#252;r unbemannte &#220;berwasserfahrzeuge (USV) existieren seit den 1980er Jahren: Damals kamen Konzepte
auf, Boote fern- oder programmgesteuert f&#252;r Aufkl&#228;rung, Minensuche, T&#228;uschung und als Waffentr&#228;ger zu
nutzen (Altmann/Gubrud 2017, S. 23). In den 2000er Jahren untersuchte und entwickelte die U.S. Navy
verschiedene USV-Prototypen, es war aber die israelische Elbit Systems Ltd., die 2006 mit dem &#187;Silver Marlin&#171;
das erste einsatzf&#228;hige bewaffnete USV vorstellte. Es dient vor allem der &#220;berwachung und Aufkl&#228;rung und 
soll auch bei schwerem Seegang, sowohl bei Tag als auch bei Nacht, &#187;autonom vorgegebene Koordinaten
ansteuern&#171; k&#246;nnen (Masirske 2009). Neben einem Radar und verschiedenen elektrooptischen Sensoren ist das
knapp 11 m lange Boot mit einem stabilisierten Maschinengewehr ausgestattet, das jedoch rein ferngesteuert ist
(Altmann/Gubrud 2017, S. 86). Weiterentwicklungen wie das f&#252;r Anti-U-Boot- und Antimineneins&#228;tze
spezialisierte &#187;Seagull&#171; von Elbit Systems (o. J.), oder das in verschiedenen bewaffneten Varianten erh&#228;ltliche &#187;
Protector&#171; von Rafael Advanced Defense Systems Ltd. (RAFAEL o. J.b) illustrieren Israels Ambitionen im USV-
Bereich (Alwardt et al. 2017, S. 23; Opall-Rome 2016).
Insgesamt ist das Spektrum sowohl an verf&#252;gbaren wie auch in Entwicklung befindlichen USVs aber noch
recht klein, vor allem im Vergleich zu den im n&#228;chsten Kapitel besprochenen Unterwasserfahrzeugen (UUVs),
die eine ungleich h&#246;here milit&#228;rstrategische Bedeutung aufweisen. Die U.S. Navy beispielsweise f&#252;hrte zwar
2014 und 2016 Demonstrationen eines Schwarms bewaffneter Motorboote durch, bei denen autonome
Bewegung und gegenseitige Koordination zentrale Ziele waren (Smalley 2016), hat aber laut eigenen Angaben keine
USVs stationiert (daf&#252;r aber unbemannte Luft- sowie Unterwasserfahrzeuge; Altmann/Gubrud 2017, S. 86).
Unterwasserfahrzeuge
Obwohl Torpedos im engeren Sinne nicht zu den Unterwasserfahrzeugen gez&#228;hlt werden, sind sie wichtige
Vorl&#228;ufer und weisen etliche technologische Parallelen zu UUVs auf. Bereits seit dem Zweiten Weltkrieg
verf&#252;gen sie teilweise &#252;ber eine akustische Zielsuche und damit automatisierte Steuerungsfunktionen (wie im Falle
des deutschen Torpedos T5 &#187;Zaunk&#246;nig&#171; oder der US-amerikanischen &#187;Mark 24 Mine&#171;; Altmann/Gubrud
2017, S. 101). Die F&#228;higkeiten moderner Torpedos wie beispielsweise des US-amerikanischen &#187;MK 54&#171;
(Raytheon Technologies o. J.) sind im Prinzip vergleichbar zu denen fortschrittlicher Lenkflugk&#246;rper, wie sie in
31 Inwiefern die bewaffnete Version &#252;ber autonome Feuerausl&#246;sung verf&#252;gt, ist nicht bekannt &#8211; es ist jedoch davon auszugehen, dass
auch in diesem Fall wie bei allen anderen beweglichen UGVs f&#252;r die Waffenbedienung ein menschlicher Operator vorgesehen ist.
Kapitel 4.1.1 beschrieben wurden: Navigation, die Abwehr von Gegenma&#223;nahmen sowie die Suche,
Identifizierung und Verfolgung von Zielen (mittels Signaturerkennung) erfolgen nach dem Abschuss weitgehend ohne
menschliches Zutun.
Die sowohl in Form als auch in Funktion an Torpedos angelehnten unbemannten Unterwasserfahrzeuge,
auch als Unterwasserdrohnen bezeichnet, nehmen weltweit an Verbreitung zu.32 Ihre milit&#228;rischen Aufgaben
konzentrieren sich allerdings bisher weitgehend auf Aufkl&#228;rung, &#220;berwachung und Seeminenr&#228;umung
(Alwardt et al. 2017, S. 17; Dean 2017).33 Beispiele sind u. a. der Seeotter &#187;MKII&#171; (AUVAC o. J.) von der
deutschen Atlas Elektronik GmbH, die russische Unterwasserdrohne &#187;Klavesin-2&#171; (Navy Recognition 2018) oder
der von Boeing entwickelte &#187;Echo Ranger&#171;, der dank eines &#187;hybriden wiederaufladbaren Energiesystems&#171;
(McCaney 2016) mehrere Monate autark operieren k&#246;nnen soll. Anders als Torpedos sind diese Systeme mit 
der Herausforderung konfrontiert, dass f&#252;r milit&#228;risch w&#252;nschenswerte Einsatzradien (idealerweise &#252;ber
Hunderte oder gar Tausende Seemeilen) eine langfristige Energieversorgung, m&#246;glichst &#252;ber mehrere Tage oder gar
Wochen, und eine weitreichende Kommunikationsverbindung sicherzustellen sind. Besonders Letzteres stellt 
ein Problem dar: W&#228;hrend die Energiefrage &#252;ber unterseeische Dockingstationen oder atomare Antriebe
zumindest prinzipiell l&#246;sbar erscheint (Dombe 2016; Eckstein 2016), sind die M&#246;glichkeiten der Kommunikation und
Daten&#252;bertragung &#8211; und damit auch der Fernsteuerung &#8211; aufgrund der Absorption von Funkwellen und anderen
elektromagnetischen Signalen in Meerwasser grunds&#228;tzlich begrenzt (Altmann/Gubrud 2017, S. 24). Dies hat
letztlich zur Konsequenz, dass bei diesen UUV-Systemen (vor allem bei bewaffneten Eins&#228;tzen, die besonders
hohe Anspr&#252;che an die Verhaltensplanung mit sich bringen) sehr weitreichende Autonomie praktisch
unabdingbar ist (Dean 2017), damit sie anspruchsvolle milit&#228;rische Aufgaben in den Weiten der Ozeane sinnvoll erf&#252;llen
k&#246;nnen. Um die Kommunikationsm&#246;glichkeiten unter Wasser zu verbessern, arbeitet die U.S. Navy mithilfe
der DARPA au&#223;erdem auch an der Entwicklung einer Art Unterwasser-Internet (McCaney 2015).
Die Entwicklung bewaffneter UUVs steckt aufgrund der beschriebenen H&#252;rden erst in den Anf&#228;ngen.
Dennoch ist klar, dass weitr&#228;umig operierenden Unterwassersystemen perspektivisch eine hohe strategische
Bedeutung zukommen k&#246;nnte (Dickow 2015, S. 15). Laut dem Chief of Naval Operations (2016, S. 14) sind autonome
UUVs eine wichtige Schl&#252;sselkomponente, um die US-amerikanische Unterwasserdominanz zu verbessern und
auszudehnen &#8211; entsprechend zeigen insbesondere die USA, Gro&#223;britannien, Russland und China ein gro&#223;es
Interesse an deren Entwicklung und Beschaffung (Alwardt et al. 2017, S. 17; New America o. J.a). So hat die
U.S. Navy im September 2017 ein erstes UUV-Geschwader aufgestellt (Altmann/Gubrud 2017, S. 86 f.) (NA-
VALTODAY 2017). Gr&#246;&#223;eres mediales Aufsehen erregten zudem Meldungen des Pentagon im Jahr 2018, dass 
Russland einen &#187;neuen interkontinentalen, nuklear bewaffneten und angetriebenen autonomen
Unterseetorpedo&#171; testen soll, das als &#187;Ocean Multipurpose System Status-6&#171; bezeichnet wird und &#252;ber eine Reichweite von
6.200 Meilen (9.978 km) verf&#252;gen soll (Ballesteros 2018) &#8211; &#252;ber technische Einzelheiten des Systems und
dessen aktuellen Entwicklungsstand gibt es allerdings derzeit keine verl&#228;sslichen Informationen.
4.2 Forschungs- und Entwicklungstrends
W&#228;hrend einige L&#228;nder enorme Ressourcen in die FuE zunehmend automatisierter Waffensysteme investieren
(z. B. die USA und China), haben andere L&#228;nder Bedenken und z&#246;gern bisher (z. B. Deutschland). In diesem
Kapitel werden &#8211; basierend auf dem Gutachten von Alwardt et al. (2017, S. 24 ff.) &#8211; die FuE-Aktivit&#228;ten
wichtiger staatlicher Akteure hinsichtlich der Autonomisierung von unbemannten (bewaffneten) Systemen
&#252;berblicksartig dargestellt. Wegen der F&#252;lle an Aktivit&#228;ten wird kein Anspruch auf Vollst&#228;ndigkeit erhoben, das
Ziel ist vielmehr, eine Vorstellung von zuk&#252;nftigen Entwicklungspfaden zunehmend automatisierter
Waffensysteme zu vermitteln. Zugleich ergeben sich auf diese Weise auch Anhaltspunkte &#252;ber die potenziellen
F&#228;higkeiten von zuk&#252;nftigen AWS.
Im Bereich der heutigen FuE und auch in der Beschaffung unbemannter Waffensysteme sind die USA der
weltweite Vorreiter, weshalb die US-amerikanischen FuE-Trends im Folgenden als wichtiger Ma&#223;stab dienen
und den gr&#246;&#223;ten Raum einnehmen. Insbesondere Gro&#223;britannien, Israel und China orientieren sich stark an den
32 Die Geschichte von UUVs reicht ebenfalls relativ weit zur&#252;ck (Altmann/Gubrud 2017, S.23): In den USA wurde in den sp&#228;ten
1950er Jahren das &#187;cable-controlled underwater vehicle&#171; (CURV) entwickelt und viel genutzt, u. a. zur Bergung der in spanischen 
Gew&#228;ssern verlorengegangenen Wasserstoffbombe. Ab 1984 setzte die U.S. Marine die kabelgesteuerte &#187;Deep Drone&#171; ein, sie
konnte bei 3 Knoten (5,6 km/h) Geschwindigkeit bis 1.800 m tief tauchen und war mit Videokameras, Sonar und Manipulatoren 
ausgestattet.
33 F&#252;r einen &#220;berblick &#252;ber verf&#252;gbare Systeme http://auvac.org (1.9.2020).
Entwicklungen im Bereich der US-R&#252;stungsindustrie und des US-Milit&#228;rs. Neben den Genannten werden
weitere Schl&#252;sselakteure betrachtet, n&#228;mlich Deutschland, Frankreich und Russland.
&#220;berwiegend beziehen sich die Informationen auf milit&#228;rische unbemannte Systeme im Allgemeinen und
nicht auf unbemannte Waffensysteme im Speziellen. Die resultierenden technologischen Trends im Hinblick
auf unbemannte Systeme gelten aber erst einmal unabh&#228;ngig von einer m&#246;glichen Bewaffnung, welche ggf.
auch parallel oder nachtr&#228;glich erfolgen kann. Neben den breiteren FuE-Trends wird exemplarisch auch auf
einige konkrete Entwicklungs- und Beschaffungsprojekte von UWS eingegangen.
4.2.1 USA
Die USA sind als Vorreiter und Treiber der Entwicklungen zunehmend automatisierter Waffensysteme und als
wesentlicher Wegbereiter f&#252;r zuk&#252;nftige autonome Waffensysteme anzusehen (dazu und zum Folgenden
Alwardt et al. 2017, S. 28). Zum einen gibt es in den USA auf den Gebieten der Robotik und k&#252;nstlichen
Intelligenz eine Vielzahl hochkar&#228;tiger Forschungseinrichtungen und Universit&#228;ten (Boulanin 2016a). Zum anderen
haben die USA mit der Policy Directive 3000.09 (DOD 2012) als erstes und bisher einziges Land damit
begonnen, ein formales Regelwerk f&#252;r die Entwicklung und den Gebrauch semiautonomer und autonomer
Waffensysteme zu entwickeln; darin werden u. a. Bedingungen festgelegt, unter denen autonome und halbautonome
Waffensysteme stationiert und eingesetzt werden d&#252;rfen (Altmann/Gubrud 2017, S. 154).
Passend dazu betonte der damalige US-Verteidigungsminister Chuck Hagel (2014) in der &#187;Third Offset
Strategy&#171;34 die im November 2014 von ihm angesto&#223;en wurde, die gro&#223;e milit&#228;r(strateg)ische Bedeutung von
k&#252;nstlicher Intelligenz und Autonomie (dazu und zum Folgenden Alwardt et al. 2017, S. 28; Altmann/Gubrud 
2017, S. 153 ff.). Der stellvertretende US-Verteidigungsminister Robert Work wurde mit der Leitung der
Initiative beauftragt (Work 2016). Er setzt dabei laut Ellman et al. (2017, S. 3) auf folgende technologischen
Schl&#252;sselbereiche: autonome Lernsysteme, kollaborative Entscheidungsfindung zwischen Mensch und Maschine,
assistierte menschliche Operationen, fortgeschrittene Eins&#228;tze bemannter und unbemannter Systeme (&#187;advanced
manned-unmanned systems operations&#171;), netzwerkf&#228;hige autonome Waffen sowie
Hochgeschwindigkeitsprojektile. Ohne direkt auf die dritte Offsetstrategie Bezug zu nehmen, werden deren Schwerpunkte von der Trump-
Administration offenbar fortgef&#252;hrt, vor allem mit Fokus auf die Entwicklung autonomer Systeme (DOD
2018b, S. 7; Thomas 2017). So wurde vom US-Verteidigungsministerium (DOD 2019) ein neues Joint Artificial
Intelligence Center (JAIC) (Pomerleau 2018) eingerichtet, das die milit&#228;rische Forschung in diesem Bereich
vorantreiben soll. Passend dazu unterzeichnete Pr&#228;sident Donald Trump Anfang Februar 2019 eine Anordnung,
die von den Regierungsbeh&#246;rden mehr Engagement bei Forschung, F&#246;rderung und Ausbildung im Bereich der
k&#252;nstlichen Intelligenz verlangt (Pamuk/Shepardson 2019; U.S. President 2019).
Der langj&#228;hrige Trend zeigt in den USA also deutlich in Richtung AWS, was sich auch in den vom DOD
regelm&#228;&#223;ig publizierten milit&#228;rischen Entwicklungsstrategien zu unbemannten Systemen (&#187;Unmanned Systems
Integrated Roadmap&#171;) widerspiegelt. Das DOD (2013, S. 71) gibt in seiner Roadmap von 2013 an, dass die
USA f&#252;r den Zeitraum 2017 bis 2020 planten, &#187;[to] move the capability further along the scale from automatic
to autonomous behavior&#171;. Auch in der aktuellen Roadmap von 2017 wird diese Linie best&#228;tigt: Autonomie und
Robotik werden dort als Schl&#252;sselfaktoren f&#252;r die weitere Entwicklung unbemannter Systeme benannt, die das
Potenzial h&#228;tten, die zuk&#252;nftige Kriegsf&#252;hrung zu revolutionieren (DOD 2017c, S. v). Entsprechendes ist
ebenfalls in den Strategiepapieren der einzelnen Teilstreitkr&#228;fte zu lesen (DON 2018; U.S. Air Force 2016; U.S.
Army 2017b), in denen autonomen Systemen eine entscheidende Rolle in zuk&#252;nftigen Konflikten
zugeschrieben wird. So stellt beispielsweise die U.S. Navy (2013, S. 4) in ihrer &#187;Information Dominance Roadmap 2013&#8211; 
2028&#171; fest, Streitkr&#228;fte k&#246;nnten zuk&#252;nftig unterst&#252;tzt werden durch &#187;improved robotics and remotely-guided
autonomous and miniaturized weapons&#171;. Anhand von aktuellen Strategiepapieren, Wei&#223;b&#252;chern und
Informationen staatlicher Institutionen ergibt sich so ein recht gutes Bild der mittel- und langfristigen FuE-Ziele auf
dem Feld zunehmend automatisierter Systeme des US-Milit&#228;rs.
34 Im Deutschen sinngem&#228;&#223; &#187;Dritte Kompensationsstrategie&#171;. Darunter versteht man die asymmetrische Kompensation im milit&#228;rischen
Wettbewerb durch eine Kombination von milit&#228;rischer Strategie, Technologie und Streitkr&#228;fteorganisation (dazu und zum Folgenden
Alwardt et al. 2017, S.25). Die erste Offsetstrategie war die Einf&#252;hrung von Nuklearwaffen, Bombern und Raketen unter Pr&#228;sident
Dwight Eisenhower angesichts der konventionellen &#220;berlegenheit der sowjetischen Streitkr&#228;fte in den 1950er Jahren, w&#228;hrend die
zweite Offsetstrategie (1970er/1980er Jahre) auf die zunehmende Bedeutung von digitalen Mikroprozessoren,
Informationstechnologien, Sensoren und Tarnkappenf&#228;higkeit setzte.
Bereits heute investieren die USA stattliche Summen in entsprechende FuE (dazu und zum Folgenden 
Alwardt et al. 2017, S. 26). Geforscht wird sowohl in &#252;bergeordneten Forschungsinitiativen wie dem Programm
&#187;Science of Autonomy&#171;35 des ONR oder der &#187;Defense Innovation Unit Experimental&#171; (DIUx)36 als auch in 
kleineren spezifischen Programmen, von denen im Folgenden einige beispielhaft genannt werden.37 F&#252;r die
dritte Offsetstrategie, die sich stark auf semiautonome und autonome Systeme konzentriert, wurden 2016 knapp
18 Mrd. US-Dollar f&#252;r einen Zeitraum von 5 Jahren veranschlagt (Gady 2016). Den Angaben des DOD (2013,
S. 3) zufolge sahen die USA f&#252;r den Zeitraum von 2016 bis 2018 f&#252;r FuE, Beschaffung und Betrieb
unbemannter Systeme 14,49 Mrd. US-Dollar vor. Hinsichtlich des Plattformtyps liegt der Schwerpunkt dabei eindeutig
auf UAVs, deren finanzieller Anteil in diesem Zeitraum 13,1 Mrd. US-Dollar betragen sollte. Deutlich
abgeschlagen an zweiter Stelle folgten maritime Systeme, also USVs und UUVs, die mit etwa 400 Mio. US-Dollar
gef&#246;rdert werden sollten. F&#252;r UGVs hingegen waren im Zeitraum von 2016 bis 2018 nur etwa 164 Mio. US-
Dollar vorgesehen (DOD 2013, S. 3). Trotz dieses weitreichenden Engagements kam das DSB (2016, S. 1) zu
dem Schluss, dass das Verteidigungsministerium die Erschlie&#223;ung (&#187;exploitation&#171;) der Autonomie weiter
beschleunigen muss.
Diesbez&#252;glich hat die Automatisierung von Datenprozessen mithilfe von machinellem Lernen und KI
einen hohen Stellenwert (dazu und zum Folgenden Alwardt et al. 2017, S. 27 f.). Automatisierte Datenprozesse 
sollen Kommandeure und Soldaten zuk&#252;nftig dabei unterst&#252;tzen, Entscheidungen zu treffen und mittelfristig
auch bei der Wahl der &#187;richtigen&#171; Waffen f&#252;r das &#187;richtige&#171; Ziel behilflich zu sein (&#187;automated battle
management aids&#171;) (U.S. Navy 2013, S. 29). In aktuellen FuE-Programmen werden u. a. neue Algorithmen f&#252;r die
Organisation und Zusammenf&#252;hrung von komplexen Daten oder Bild- und Datenverarbeitung hinsichtlich
genauer Zielauswahl und autonomer Navigation entwickelt (siehe z. B. apan o. J.). In anderen Projekten wird an
der selbstst&#228;ndigen Erfassung und Erkennung von Objekten, von U-Booten, Kleinwaffen und Luftangriffen
gearbeitet.38 Milit&#228;rische Grundlagenforschung im Hinblick auf KI und maschinelles Lernen findet
beispielsweise im Rahmen des Pentagonprojekts &#187;Maven&#171;,39 in den Forschungsprogrammen &#187;Computational Methods
for Decision Making&#171; (ONR o. J.b) sowie &#187;Computational Cognition and Machine Intelligence&#171; der U.S. Air 
Force Office of Scientific Research Laboratory (apan o. J.b) statt.
Trends hinsichtlich der Abmessungen
Hinsichtlich der Abmessungen von unbemannten Systemen sind zwei gegens&#228;tzliche Trends in der
milit&#228;rischen Forschung in den USA festzustellen. Auf der einen Seite stehen kleine kosteng&#252;nstige UAVs und UGVs
im Fokus, welche Soldaten bei ihren Eins&#228;tzen mitf&#252;hren k&#246;nnen oder die in gro&#223;er Anzahl als Schwarm
operieren sollen (dazu und zum Folgenden Alwardt et al. 2017, S. 26 f.). In dem Programm &#187;Low-Cost UAV
Swarming Technology&#171; (LOCUST) wird gezielt darauf hingearbeitet, die Produktionskosten unbemannter Systeme
zu reduzieren (Boulanin 2016b, S. 37). Der Trend zur Miniaturisierung wird anhand der Ver&#246;ffentlichung einer
eigenen Roadmap deutlich, dem &#187;Small Unmanned Aircraft Systems (SUAS) Flight Plan: 2016&#8211;2036&#171; der U.S.
Air Force (2016); und im Programm &#187;Fast Lightweight Autonomy&#171; (FLA) arbeitet z. B. auch die DARPA
(o. J.b) an kleinen und schnellen autonomen Luftfahrzeugen. Auf der anderen Seite geht der FuE-Trend zu
gr&#246;&#223;eren und leistungsf&#228;higeren unbemannten Systemen, die u. a. mehr Ausdauer, Robustheit, Nutzlast oder
Geschwindigkeit aufweisen sollen (Boulanin 2016b, S. 31 ff.). Im Bereich der UUVs wird an gro&#223;en
multifunktionalen Systemen geforscht, die von Schiffen oder milit&#228;rischen Basen starten k&#246;nnen. Entsprechende
Forschung findet z. B. im Programm &#187;Large Displacement Unmanned Undersea Vehicles&#171; statt, welches sich
aktuell in der Testphase befindet (Pomerleau 2016). Das ONR und die DARPA untersuchen M&#246;glichkeiten zur
35 Im Programm &#187;Science of Autonomy&#171; wird Grundlagenforschung in verschiedenen Themenfeldern f&#252;r unbemannte Schiffe und
U-Boote, insbesondere hinsichtlich neuer Konzepte f&#252;r Schw&#228;rme, maschinelles Lernen und Teamf&#228;higkeit zwischen Mensch und
Maschine betrieben (ONR o. J.a).
36 Das DIU wurde 2014 vom DOD ins Leben gerufen und soll im zivilen Sektor aufkommende Technologien schneller in milit&#228;rische 
Nutzungen &#252;berf&#252;hren. Der Fokus liegt auf Themen wie Roboter, autonome Systeme, Miniaturisierung, Big Data sowie moderne
Fertigungstechnologien einschlie&#223;lich 3-D-Drucker (Tucker 2015).
37 F&#252;r einen detaillierten &#220;berblick &#252;ber die milit&#228;rischen FuE-Programme der USA mit AWS-Bezug vgl. auch Altmann/Gubrud 
2017, S.114 ff.
38 Eine strukturierte &#220;bersicht zu US-amerikanischen FuE-Aktivit&#228;ten in Bezug auf AWS findet sich in Boulanin 2016b, S.31 ff.
39 Ziel des 2017 gestarteten Projekts &#187;Maven&#171; ist die automatisierte Analyse von Bildern und Videos, die von Drohnen aufgezeichnet 
wurden (Pellerin 2017). Als bekannt wurde, dass Google an diesem Projekt beteiligt ist, kam es zu Protesten unter Google-
Mitarbeitern, sodass die Firma die Zusammenarbeit mit dem Pentagon schlie&#223;lich nicht verl&#228;ngerte (Peitz 2018).
Steigerung der Ausdauer bzw. Stehzeiten milit&#228;rischer Systeme im Feld, insbesondere auch f&#252;r U-Boote und 
Schiffe, da diese sich &#252;ber weite Distanzen und lange Zeitr&#228;ume im offenen Meer bewegen k&#246;nnen sollen (DOD
2013, S. 88 u. 138). Einer der Schl&#252;sselfaktoren ist hier die Energiefrage. Es wird daran geforscht, wie sich
Energie aus nat&#252;rlichen Quellen wie Solarkraft oder Wellenbewegungen gewinnen l&#228;sst.40 
Anwendungskonzepte: Schw&#228;rme sowie Mensch-Maschine-Teams
Bez&#252;glich zuk&#252;nftiger Anwendungskonzepte ist es ein wichtiges Anliegen der US-Streitkr&#228;fte, unbemannte 
Systeme in Gruppen kommunizieren und kooperativ zusammenarbeiten zu lassen (dazu und zum Folgenden
Alwardt et al. 2017, S. 28). Teams aus unbemannten Systemen werden auch Schw&#228;rme genannt (Kasten 4.1).
Das ONR forscht beispielsweise an einer Software, die es erm&#246;glichen soll, UUVs als Schwarm zu koordinieren
und autonom fahren zu lassen (Smalley 2016). Die DARPA (o. J.c) wiederum arbeitet an heterogenen Gruppen
autonomer Luftfahrzeuge und das U.S. Army Combat Capabilities Development command Army Research
Laboratory (CCDC ARL)41 an schw&#228;rmenden mobilen, multifunktionalen Mikrorobotern (Luft-Boden) f&#252;r
Eins&#228;tze in urbanem Gel&#228;nde. Im Oktober 2016 lie&#223; die U.S. Air Force bereits 103 unbewaffnete Mikro-UAVs vom
Typ &#187;Perdix&#171; im Schwarm fliegen (DOD 2017c). Neben der intensiveren Vernetzung ist hierbei die
Weiterentwicklung von Sense-and-avoid-Technologien f&#252;r komplexe Umgebungen notwendig, die dazu bef&#228;higen sollen,
Hindernisse wahrzunehmen, ihnen auszuweichen und Kollisionen zu vermeiden. Insbesondere die Air Force
und die Navy arbeiten an diesen Technologien (DOD 2013, S. 68).
Neben Schwarmtechnologien ist die Entwicklung von Teams aus Mensch und Maschine (MUM-T), ein 
weiteres zentrales Ziel von U.S. Air Force, U.S. Navy und U.S. Army (DOD 2013, S. 69; U.S. Air Force 2016,
S. 17) (dazu und zum Folgenden Alwardt et al. 2017, S. 28 f.). So kann z. B. dem Piloten eines Kampfjets ein
einzelnes UCAV als &#187;loyal wingman&#171; (U.S. Air Force 2016, S. 45) zur Seite gestellt werden &#8211; oder sogar ein 
kompletter Schwarm, der der Kontrolle des Piloten untersteht. Laut der &#187;Robotic and Autonomous Systems
Strategy&#171; der U.S. Army (2017b, S. 24) produziert MUM-T Synergien und f&#252;hrt zu asymmetrischen Vorteilen
&#8211; wobei jedoch nicht weiter ausgef&#252;hrt wird, worin genau diese bestehen. Auch in der Roadmap von 2013 
(DOD 2013, S. 139) hei&#223;t es, das MUM-T-Konzept sieht vor, &#187;die inh&#228;renten St&#228;rken bemannter Plattformen
mit den St&#228;rken der unbemannten zu kombinieren, mit Produktsynergien, die bei einzelnen Plattformen nicht 
zu finden sind&#171;.42 An MUM-T wird daher intensiv geforscht43 &#8211; in der FuE-Budgetanfrage f&#252;r das Jahr 2017
veranschlagte das US-Verteidigungsministerium daf&#252;r insgesamt 3 Mrd. US-Dollar (inklusive &#187;human-
machine collaboration&#171;; Gady 2016).
F&#252;r die fernere Zukunft setzen die US-Streitkr&#228;fte auf unbemannte Fahrzeuge und komplexe
Schwarmsysteme, die autonom agieren k&#246;nnen, um menschliche Streitkr&#228;fte &#8211; auch in zugangsverwehrten Gebieten, dem
Anti-Access-and-Area-denial-Umfeld &#8211; zu unterst&#252;tzen (U.S. Army 2017b, S. 9 ff.) (dazu und zum Folgenden
Alwardt et al. 2017, S. 29). Geplant ist, unbemannte taktische Fahrzeuge zuk&#252;nftig als &#187;force multiplier&#171;
einzusetzen, d. h. in unterst&#252;tzender Form als Teil interaktiver Mensch-Maschine-Operationen (DOD 2013, S. 32 
ff.). Laut dem Leiter der Robotikabteilung des Army Capabilities Integration Centre, ist dabei zwar autonome 
Navigation vorgesehen, nicht jedoch der autonome Waffeneinsatz (Magnuson 2017). Auch das U.S. Marine
Corps verspricht sich perspektivisch Unterst&#252;tzung durch autonome Waffensysteme, die in der Lage sind, sich 
in taktischer Hinsicht &#228;hnlich wie ein Mensch zu verhalten (DOD 2013, S. 72 f.). Ebenso die Air Force: Sie will 
das Loyal-Wingman-Konzept so weiterentwickeln, dass kleine UAVs zuk&#252;nftig in der Lage sind, bemannte
Luftfahrzeuge zu begleiten und zu unterst&#252;tzen (U.S. Air Force 2016, S. 12) &#8211; Fortschritte in der Autonomie
spielen dabei laut U.S. Air Force eine Schl&#252;sselrolle. Neben der Entwicklung von autonom agierenden UAV-
Schw&#228;rmen sollen zudem per &#187;multi-aircraft control&#171; (MAC) die M&#246;glichkeiten einzelner Piloten ausgeweitet
werden, in der Luft die Kontrolle &#252;ber eine gr&#246;&#223;ere Anzahl von Drohnen auszu&#252;ben.
40 Es gibt bereits unbewaffnete Unterwasserroboter wie den &#187;slocum thermal glider&#171;: http://auvac.org/configurations/view/51
(1.9.2020), die aus Wassertemperaturunterschieden Energie beziehen und so bis zu 5 Jahre ohne zu tanken Meereserkundungen 
durchf&#252;hren k&#246;nnen.
41 https://www.arl.army.mil/www/default.cfm?page=332 (30.12.2019)
42 Im Original: &#187;to combine the inherent strengths of manned platforms with the strengths of UAS, with product synergy not seen in 
single platforms&#171;.
43 F&#252;r Projektbeispiele vgl. Boulanin 2016b, S.31 ff.
Kasten 4.1 Schw&#228;rme als neue Form der Kriegsf&#252;hrung?
Unbemannte Systeme k&#246;nnten sowohl zu Luft, zu Wasser als auch zu Land in gro&#223;en koordinierten Gruppen 
oder Schw&#228;rmen eingesetzt werden. Die U.S. Air Force (2016, S. 45) beschreibt einen Schwarm als &#187;eine
Gruppe von autonom vernetzten kleinen UAVs, die kollaborativ operieren, um gemeinsame Ziele mit einem
Betreiber auf oder in der Schleife zu erreichen&#171;.44 
Das Schwarmprinzip basiert auf der taktischen Idee, die Abwehr dadurch zu &#252;berfordern, dass man ein
bestimmtes Ziel gleichzeitig von vielen Seiten und mehrfach angreifen l&#228;sst (Scharre 2014b). Da der Verlust 
eines oder weniger Einzelsysteme kaum Auswirkungen auf die Gesamtwirkung eines Schwarms hat,
verspricht man sich davon nicht nur eine besonders wirkungsvolle, sondern auch eine besonders robuste Art der
Kampff&#252;hrung (Dickow 2015, S. 13).
Die erfolgversprechende Umsetzung dieses Prinzips setzt dreierlei voraus: erstens die kommunikative
Vernetzung der Einheiten, was entsprechender Kommunikationsverbindungen bedarf. Zweitens m&#252;ssen die 
Bestandteile eines Schwarms &#252;ber die F&#228;higkeit verf&#252;gen, weitgehend autonom zu agieren &#8211; zumindest was
die Navigation und Reaktion auf unvorhergesehene Ereignisse betrifft &#8211;, da die koordinierte Steuerung vieler
Einheiten in Echtzeit durch einen (oder mehrere) Befehlshaber dessen F&#228;higkeiten, das verf&#252;gbare Personal
und die Kommunikationsm&#246;glichkeiten &#252;bersteigt. Schwarmkonzepte sehen deshalb keine zentralisierte
Befehlsstruktur vor, wie im Milit&#228;r ansonsten &#252;blich. Eine weitere Bedingung ist schlie&#223;lich, dass die Einheiten
in m&#246;glichst gro&#223;er Zahl sowie zu m&#246;glichst geringen Kosten verf&#252;gbar sind, was jedoch im Gegenzug
impliziert, dass ihre F&#228;higkeiten eher begrenzt sind, jedenfalls im Vergleich zu teureren Systemen. Dies gilt 
besonders in Hinsicht auf Reichweite, Geschwindigkeit, Ausdauer und Nutzlast sowie in der Konsequenz
auch f&#252;r die Letalit&#228;t und andere Wirkf&#228;higkeiten.
Obwohl das Schwarmprinzip bereits im Zweiten Weltkrieg angewendet wurde &#8211; beispielsweise im U-
Boot-Krieg oder in der Schlacht um Britannien (Arquilla/Ronfeldt 2000) &#8211;, bieten die Fortschritte in Robotik
und KI ganz neue M&#246;glichkeiten, unz&#228;hlige Einzelplattformen in koordinierter Form miteinander zu
vernetzen (Tucker 2017). Damit verbunden ist ein milit&#228;rischer Paradigmenwechsel: &#187;weg vom teuren
Einzelsystem, hin zum preisg&#252;nstigen Wegwerfsystem&#171; und weg von der zentralen zur dezentralen Steuerung (Dickow
2015, S. 13). Trotz der inzwischen vielen Demonstrationen von fliegenden Drohnenschw&#228;rmen &#8211; mit
teilweise bis zu Hunderten Einheiten (DOD 2017) &#8211; und auch einiger Experimente zu Land und zu Wasser steht
die Entwicklung bewaffneter Schw&#228;rme aus unbemannten Systemen noch relativ am Anfang. Offene
Forschungsfragen betreffen neben der Miniaturisierung von Systemkomponenten u. a. die &#187;effiziente
Schwarmkommunikation, [die] Parallelisierung von Aufgabenbew&#228;ltigung und [die] &#252;bergeordnete Steuerung durch
angemessene Kompetenzverteilung&#171; (Dickow 2015, S. 13; f&#252;r Details zu einzelnen Forschungsprogrammen 
Kap. 4.2).
Quelle: Altmann/Gubrud 2017, S. 61 ff. u. S. 174 ff.
Entwicklungs- und Beschaffungsprojekte
Die USA engagieren sich in zahlreichen Entwicklungs- und Beschaffungsprojekten im Bereich unbemannter
Systeme.45 Dazu geh&#246;rt etwa der Prototyp eines autonomen Wasserfahrzeugs mit gro&#223;er Reichweite zur
Aufsp&#252;rung und Bek&#228;mpfung von dieselelektrischen U-Booten (&#187;anti-submarine warfare continuous trail
unmanned vessel&#171; &#8211; ACTUV), der j&#252;ngst von der DARPA (o. J.d) zur Weiterentwicklung an die U.S. Navy &#252;bergeben
wurde (Szondy 2018). Der klare Schwerpunkt der Entwicklungsaktivit&#228;ten liegt aber auf autonomen
Kampfflugzeugen (dazu und zum Folgenden Altmann/Gubrud 2017, S. 107; Alwardt et al. 2017, S. 29):
44 Im Original: &#187;a group of autonomous networked SUAS operating collaboratively to achieve common objectives with an operator
on or in the loop&#171;.
45 Aufstellungen hierzu finden sich u. a. in den &#187;Unmanned Systems Integrated Roadmaps&#171; oder in spezifischen Dokumenten der US-
Teilstreitkr&#228;fte; f&#252;r einen detaillierten &#220;berblick &#252;ber US-amerikanische Forschungs- und Entwicklungsprogramme mit AWS-
Bezug vgl. Altmann/Gubrud 2017, S.114 ff.
&#8250; Die U.S. Air Force arbeitet bereits seit mindestens 1999 an der Entwicklung unbemannter
Kampfflugzeuge, beginnend mit der &#187;Boeing X-45&#171;, die ihren Erstflug 2002 absolvierte und Teil des J-UCAS-
Projekts (&#187;Joint Unmanned Combat Air System&#171;) von DARPA war (Airforce Technology o. J.).46 Bedeutende 
aktuelle Projekte im Bereich unbemannter Flugsysteme sind z. B. die Hochgeschwindigkeitsplattformen
&#187;XQ-58 Valkyrie&#171; und &#187;UTAP-22 Mako&#171;, beide entwickelt von Kratos (o. J.). Das etwa 9 m lange
UCAV-Modell &#187;XQ-58 Valkyrie&#171; soll eine Traglast von ca. 227 kg und eine ungef&#228;hre Reichweite von 
3.500 km haben; es k&#246;nnte mit leistungsf&#228;higen Zielsensoren und schweren Waffen ausgestattet werden,
z. B. 113-kg-Small-Diameter-Bomben (Drew 2017). Ein erster Flugtest fand im Fr&#252;hjahr 2019 statt
(Wright-Patterson Air Force Base 2019). Die &#187;UTAP-22 Mako&#171; UCAVs sollen mit bis zu 250 kg
Munition ausgestattet werden k&#246;nnen, Testfl&#252;ge fanden im Sommer 2017 statt. Sowohl bei &#187;Valkyrie&#171; als auch
bei &#187;Mako&#171; soll es sich um relativ kosteng&#252;nstige, schwarmf&#228;hige Kampfdrohnen handeln, die bemannten
Plattformen zuk&#252;nftig als loyaler Fl&#252;gelmann (&#187;loyal wingmen&#171;) in umk&#228;mpften Luftr&#228;umen (&#187;contested 
airspace&#171;) dienen k&#246;nnen (Airforce Technology 2017).
&#8250; Die U.S. Navy verfolgt seit 2010 das Vorhaben, eine tr&#228;gergest&#252;tzte Kampfdrohne mit Stealth-
Eigenschaften und Angriffsf&#228;higkeiten zu entwickeln (Programm &#187;Unmanned Carrier Launched Airborne
Surveillance and Strike&#171; &#8211; UCLASS) (Naval Drones 2016). Ergebnis war u. a. der Prototyp &#187;X-47B&#171; von
Northrop Grumman, ein schwanzloses Unterschall-UCAV mit D&#252;senantrieb und verringerter
Radarsignatur, das von 2011 bis 2015 erfolgreich erprobt wurde.47 Nachdem Zweifel an der Sinnhaftigkeit einer
tr&#228;gergest&#252;tzten Angriffsdrohne aufgekommen waren, entschied die U.S. Navy 2016, das
Beschaffungsprogramm neu auszurichten und den Fokus weg von &#220;berwachung und Angriff und stattdessen auf
Aufkl&#228;rung und Luftbetankung zu legen &#8211; Aufgaben, die durch das von Boeing derzeit entwickelte
Nachfolgesystem &#187;MQ-25 Stingray&#171; erf&#252;llt werden sollen (Einsatzf&#228;higkeit wird f&#252;r die Mitte der 2020er Jahre
erwartet; Osborn 2016).
4.2.2 Europa
Deutschland
Von deutscher Seite gab es bislang kein starkes Engagement f&#252;r umfassende Investitionen in die FuE oder
Beschaffung zunehmend automatisierter Waffensysteme (dazu und zum Folgenden Alwardt et al. 2017, S. 30).
Ein Wandel diesbez&#252;glich scheint sich aber abzuzeichnen. So bewilligte der Bundestag im Juni 2018 das
Leasing israelischer &#187;Heron-TP&#171;, wenn auch vorerst nur zu Aufkl&#228;rungszwecken. Verteidigungsministerin Ursula
von der Leyen wiederum begr&#252;ndete 2014 den Bedarf einer europ&#228;ischen UCAV-Entwicklung damit, dass
Europa unabh&#228;ngiger von anderen Staaten werden solle, insbesondere von f&#252;hrenden Drohnenproduktionsl&#228;ndern
wie den USA und Israel. Es sei zudem n&#246;tig, das technologische Know-how in Europa zu kultivieren, und zwar
&#187;nicht nur unter milit&#228;rischen Gesichtspunkten, sondern vor allem f&#252;r die zivilen M&#246;glichkeiten, die dahinter
stecken&#171; (S&#252;ddeutsche Zeitung 2014). Im Wei&#223;buch zur Zukunft der Bundeswehr von 2016 werden FuE-
Aktivit&#228;ten im R&#252;stungsbereich als &#187;zentraler Treiber der Innovationskraft von Streitkr&#228;ften und wehrtechnischer
Industrie&#171; hervorgehoben. Weiter hei&#223;t es, dass &#187;die heutigen Herausforderungen rund um die Bereiche Cyber-
und Informationsraum und Digitalisierung, autonome Systeme und Hybridisierung die Fortentwicklung und
Erweiterung des klassischen FuT-Ansatzes mit Eigenmitteln&#171; verlangen (Bundesregierung 2016, S. 131). Eine
ausformulierte langfristige deutsche Strategie f&#252;r teilautomatisierte oder autonome Waffensysteme gibt es
jedoch nicht.
Hinsichtlich der FuE zu Automatisierungsfragen mit milit&#228;rischem Bezug ist Deutschland derzeit auf EU-
Ebene an einem Joint-Investment-Programm der European Defence Agency zu &#187;Remotely Piloted Aircraft
Systems&#171; beteiligt (Alwardt et al. 2017, S. 30). Gemeinsam wollen die zehn beteiligten Staaten neue Technologien
entwickeln und EU-Richtlinien schaffen, damit unbemannte Luftfahrzeuge, insbesondere solche milit&#228;rischer
Art, den nichtbeschr&#228;nkten Luftraum nutzen k&#246;nnen (EDA 2015; vgl. Bundesregierung 2016, S. 74). Aktuell
wirkt Deutschland in diesem Rahmen an zwei Forschungs- und Technologieprojekten mit (zum Folgenden EDA
2019):
46 F&#252;r eine detaillierte Rekonstruktion der Entwicklungsgeschichte US-amerikanischer Kampfdrohnen vgl. Rogoway 2016.
47 &#187;X-47B&#171; f&#252;hrte Flugzeugtr&#228;gerlandungen und -starts erfolgreich aus und demonstrierte 2015 die erste autonome Luftbetankung
(Spiegel Online 2013).
&#8250; Das Projekt &#187;Enhanced RPAS Autonomy&#171; (ERA) wurde 2015 lanciert und wird von Deutschland (als
federf&#252;hrende Nation) zusammen mit Frankreich, Polen, Schweden und Italien finanziell gef&#246;rdert
(Laufzeit bis 2019, Verl&#228;ngerung bis 2020 in Vorbereitung; Deutscher Bundestag 2018a, S. 6320). Ziel ist es,
die Automatisierung als Schl&#252;sselfaktor f&#252;r die Integration unbemannter Luftfahrzeuge in den
nichtbeschr&#228;nkten Luftraum technologisch voranzutreiben, vor allem in Hinsicht auf die Sicherheit und die
Notfallprozeduren bei Start, Landung sowie Rollen am Boden.
&#8250; Im Projekt &#187;MIDair Collision Avoidance System&#171; (MIDCAS) wurde untersucht, wie unbemannte
Flugfahrzeuge Hindernisse entdecken und umgehen k&#246;nnen. Dar&#252;ber hinaus wurde die Entwicklung von
technischen Standards auf EU-Ebene unterst&#252;tzt. Neben Schweden (als federf&#252;hrender Nation) und
Deutschland waren auch Frankreich, Italien und Spanien beteiligt. Das Programm lief Ende 2018 aus (Deutscher 
Bundestag 2018a, S. 6320).
Seit 2016 engagiert sich Deutschland au&#223;erdem in f&#252;hrender Rolle in der Entwicklung eines europ&#228;ischen
UCAV der neusten Generation, das unter der Bezeichnung European MALE RPAS (&#187;medium altitude long 
endurance remotely piloted aircraft system&#171;) firmiert (Hoffmann 2015; dazu und zum Folgenden Alwardt et al.
2017, S. 30). Hauptaufgaben der &#187;Eurodrohne&#171; sollen &#220;berwachungs- und Aufkl&#228;rungsmissionen sein; sie soll
aber auch bewaffnet werden k&#246;nnen. An dem Entwicklungsprojekt sind neben Deutschland auch Frankreich,
Italien, Spanien beteiligt. In einer zweij&#228;hrigen Konzeptstudie, die im Herbst 2016 startete, wurden die
jeweiligen nationalen Anforderungen an die Kampfdrohne untersucht und ein Systemdesign entwickelt.48 Nach 10-
monatiger Untersuchung einigten sich die beteiligten L&#228;nder 2017 auf eine Drohnenkonfiguration mit zwei
Turboproptriebwerken, die erstmals 2018 als Eins-zu-eins-Modell auf der Internationale Luft- und
Raumfahrtausstellung in Berlin vorgestellt wurde (UAS Vision 2017). Das mit der Durchf&#252;hrung der Studie beauftragte
internationale Industriekonsortium bestand aus Airbus Defense and Space (in Deutschland und Spanien),
Dassault Aviation (Frankreich) und Leonardo (Italien). Von der federf&#252;hrenden europ&#228;ischen Organisation for
Joint Armament Cooperation (OCCAR) wurde Airbus als Hauptkontraktor f&#252;r die folgende Entwicklungsphase
ausgew&#228;hlt mit dem Ziel eines Erstflugs 2023 und der Auslieferung des ersten Systems etwa 2025. Airbus
rechnet mit Entwicklungskosten von ca. 1 Mrd. Euro (Hoffmann 2015; OCCAR o. J.).
Zusammen mit Frankreich treibt Deutschland zudem zwei R&#252;stungsvorhaben f&#252;r die Luft- und
Landstreitkr&#228;fte voran, die im milit&#228;rischen Hochtechnologiebereich angesiedelt sind (BMVg 2018a). Zum einen geh&#246;rt
dazu die Entwicklung eines &#187;Luftkampfsystems der Zukunft&#171; (&#187;future combat air system&#171; &#8211; FCAS) bis 2040,
das von Dassault Aviation und Airbus produziert werden soll. Laut dem (BMVg 2018a) handelt es sich beim
FCAS um ein &#187;Waffensystem der n&#228;chsten Generation&#171;, das &#187;sowohl bereits existierende als auch zuk&#252;nftige
bemannte und unbemannte Komponenten in einem interoperablen Verbund vereinen&#171; wird &#8211; wobei &#187;die
unbemannten Systeme [...] die F&#228;higkeiten des gesamten Projekts entscheidend pr&#228;gen und dessen &#220;berlebens- und 
Durchsetzungsf&#228;higkeit gew&#228;hrleisten&#171; werden. Eine erste gemeinsame Konzeptstudie wird ab 2019 erstellt
(K.S. 2019). Zum anderen soll unter industrieseitiger F&#252;hrung Deutschlands bis Mitte 2030 das &#187;main ground 
combat system&#171; (MGCS) entwickelt werden, das den Kampfpanzer &#187;Leopard 2&#171; abl&#246;sen soll. Verfolgt wird 
ein &#187;Systemansatz, in dem auch unbemannte mit bemannten Systemen zusammenwirken sollen&#171; (BMVg
2018a).
&#220;ber die geplanten Autonomief&#228;higkeiten sowohl des FCAS als auch des MGCS ist derzeit noch nichts
bekannt &#8211; in ihrer Antwort auf eine entsprechende parlamentarische Anfrage hat die Bundesregierung jedoch
erneut best&#228;tigt, dass sie autonome Waffensysteme ablehnt und dass &#187;bei Entscheidungen &#252;ber den letalen
Waffeneinsatz [...] die menschliche Kontrolle erhalten bleiben&#171; muss (Bundesregierung 2018b).
Frankreich
Vom franz&#246;sischen Verteidigungswei&#223;buch aus dem Jahr 2013 ergibt sich kein Aufschluss &#252;ber unbemannte
oder autonome Waffensysteme, sondern es finden nur &#220;berwachungs-UAVs im Zusammenhang mit
automatisierter Informationsverarbeitung Erw&#228;hnung (Minist&#232;re de la D&#233;fense 2013; dazu und zum Folgenden Alwardt
et al. 2017, S. 32 ff.). In einem Senatsbericht von 2016 wurde festgestellt, dass wegen der Komplexit&#228;t moderner
Schlachtfelder und neuer Waffentechnologien eine st&#228;rkere Automatisierung n&#252;tzlich ist, vor allem damit
franz&#246;sischen Soldaten ein besseres Lagebild und damit auch eine bessere Entscheidungsgrundlage zur Verf&#252;gung 
Deutschland trug einen Kostenanteil von 18,6 Mio. Euro (31%) an der Konzeptstudie, die anderen drei L&#228;nder jeweils 13,8 Mio.
Euro (Hoffmann 2015).
48
stehen (Gautier et al. 2016, S. 90). Gleichzeitig ist es Frankreich traditionell &#228;u&#223;erst wichtig, dass es in der
nationalen Verteidigung eine gewisse Unabh&#228;ngigkeit bewahrt. Es hat den Anspruch, selbst &#252;ber die
notwendigen Technologien zu verf&#252;gen und eine wettbewerbsf&#228;hige Industrie zu haben, damit die franz&#246;sischen
Streitkr&#228;fte ihre Aufgaben erf&#252;llen k&#246;nnen (Minist&#232;re de la D&#233;fense/DGA 2013, S. 3).
Die franz&#246;sische milit&#228;rische FuE-Ausrichtung im Zeitraum 2014 bis 2019 baut auf Autonomie als eines
der Mittel auf, um franz&#246;sische Kampfsysteme zu modernisieren, insbesondere an der Schnittstelle zwischen
Mensch und Maschine, wie z. B. im Bereich der Entscheidungsfindung (dazu und zum Folgenden Alwardt et
al. 2017, S. 32 f.). Die franz&#246;sische KI-Strategie macht diesbez&#252;glich deutlich, dass Frankreich zwar m&#246;chte,
dass die Anwendung t&#246;dlicher Gewalt dem Menschen vorbehalten bleibt &#8211; was sich auch in dem entsprechenden
Engagement in den CCW-Verhandlungen widerspiegelt (Kap. 9.2) &#8211;, gleichzeitig aber auch bei der Forschung
zu &#187;diesem wichtigen strategischen Bereich&#171; nicht hinter andere Staaten zur&#252;ckfallen m&#246;chte (Villani 2018,
S. 125).
Im Themenfeld Robotik und Informatik treibt Frankreich FuE u. a. in den drei folgenden Bereichen voran:
erstens Kommunikationssicherheit, zweitens Roboter und komplexe kognitive Systeme sowie drittens digitale
Verarbeitung und Analyse von Big Data. Im Rahmen des letzten Bereichs wird etwa im Programm &#187;Syst&#232;me
d&#8217;aide &#224; l&#8217;interpr&#233;tation multicapteurs&#171; (SAIM) geforscht. Damit verbunden ist die Weiterentwicklung von
Multifunktionssensoren, von Techniken der Datenfusion sowie der Daten- und Bildverarbeitung. Ein
besonderes Augenmerk soll hierbei auf der Luftfahrt liegen. Im maritimen Bereich werden als Priorit&#228;ten autonome
Hochleistungsnavigationssysteme, der Bereich Entscheidungsfindung und die Seeminenr&#228;umung aufgelistet
(Minist&#232;re de la D&#233;fense/DGA 2013, S. 21 u. S. 26 ff.). Das milit&#228;rische Auftragswesen von Waffensystemen
sowie die milit&#228;rische FuE wird in Frankreich von der Direction g&#233;n&#233;rale de l&#8217;armement (DGA) koordiniert.
Bei der Entwicklung von unbemannten Waffensystemen setzt Frankreich haupts&#228;chlich auf UCAVs, und
das in dreifacher Hinsicht (dazu und zum Folgenden Alwardt et al. 2017, S. 32 f.):
&#8250; Erstens ist es an dem von Deutschland angef&#252;hrten MALE-RPAS-UCAV-Entwicklungsprogramm
beteiligt, das bereits zuvor beschrieben wurde.
&#8250; Zweitens f&#252;hrt es mit Dassault Aviation das internationale Entwicklungskonsortium des nEUROn-
Technologie-Demonstrators f&#252;r eine europ&#228;ische Kampfdrohne der MALE-Klasse an, die Tarnkappentechnik
und autonome Funktionen aufweisen soll (Altmann/Gubrud 2017, S. 108 ff.).49 Das Projekt wurde 2003 
von der franz&#246;sischen Regierung begonnen. Seit dem Erstflug 2012 hat der &#187;nEUROn&#171; bereits zahlreiche
Testfl&#252;ge erfolgreich absolviert (Dassault Aviation 2016; Tran 2016).
&#8250; Das nEUROn-Programm sollte drittens, neben dem in Entwicklung befindlichen britischen Taranis-
Demonstrator von BAE Systems, eine Grundlage des Programms &#187;Future-Combat-Air-System&#171; sein, das seit
2014 von Gro&#223;britannien und Frankreich angestrebt wurde und ein bewaffnungsf&#228;higes UCAV mit
Tarnkappeneigenschaften zum Ziel hatte (Altmann/Gubrud 2017, S. 109 ff.; Stevenson 2016). Nach der
Erstellung einer Machbarkeitsstudie scheint die diesbez&#252;gliche franz&#246;sisch-britische Zusammenarbeit ab 2016
jedoch zum Erliegen gekommen zu sein (Pocock 2018). Stattdessen verfolgt Frankreich die FCAS-
Entwicklung wie bereits beschrieben nun in enger Kooperation mit Deutschland.
Daneben entwickelt Frankreich gemeinsam mit Gro&#223;britannien USV- und UUV-Systeme, insbesondere zur
R&#228;umung von Seeminen (L3Harris ASV 2018). Au&#223;erdem plant die franz&#246;sische Regierung bis 2025 die
Beschaffung eines leichten Panzerfahrzeugs, wobei auch eine unbemannte, fernsteuerbare Ausf&#252;hrung in Betracht
gezogen wird (Tran 2018).
Gro&#223;britannien
Obwohl Gro&#223;britannien im Bereich der FuE bei Weitem nicht die Gr&#246;&#223;enordnung der USA erreicht hat, ist ein
gro&#223;es Interesse an den Forschungsfeldern automatisierter Datenprozesse, Logistik und Navigation zu erkennen
(Boulanin 2016b, S. 38 ff.; dazu und zum Folgenden Alwardt et al. 2017, S. 31 ff.). Die Kooperationen zwischen
Regierung und Industrie zeigen, dass intensiv an automatisierten Milit&#228;rsystemen gearbeitet wird. In
Gro&#223;britannien gibt es daf&#252;r eine solide industrielle Grundlage, z. B. forscht BAE Systems im Bereich Luftfahrt und
Dem Konsortium geh&#246;ren au&#223;erdem Griechenland (HAI), Italien (Alenia Aermacchi), Schweden (Saab), die Schweiz (Ruag) und 
Spanien (Airbus Defence and Space) an.
49
QinetiQ an autonomen maritimen Fahrzeugen (POST 2015). Neben einem UCAV-Testgel&#228;nde in Wales
(Brooke-Holland 2015, S. 28 ff.) er&#246;ffnete das britische Verteidigungsministerium 2013 das Maritime
Autonomy Centre in Portsmouth, wo (unbewaffnete) autonome Schiffe, U-Boote und andere Wasserplattformen
erforscht, entwickelt und getestet werden sollen (Rees 2013).
Bisher gibt es in Gro&#223;britannien jedoch weder eine dezidierte Milit&#228;rstrategie in Bezug auf autonome
Waffensysteme noch offizielle Angaben dar&#252;ber, welcher Anteil der nationalen Verteidigungsausgaben (ca. 45 Mrd. 
Euro im Jahre 2018; SIPRI 2019) f&#252;r die FuE automatisierter Systeme ausgegeben wird oder in welchem
Umfang Gro&#223;britannien an autonomen Waffensystemen forscht (dazu und zum Folgenden Alwardt et al. 2017,
S. 31 ff.). Das britische Ministry of Defence mahnt jedoch an, dass es &#187;bis 2035 wahrscheinlich scheint, dass
automatisierte Systeme fortschrittlich und hochgradig anpassungsf&#228;hig sein werden. [...] Technologische
Fortschritte werden mit ziemlicher Sicherheit Schwarmangriffe erm&#246;glichen, sodass zahlreiche Ger&#228;te gemeinsam
agieren k&#246;nnen&#171;.50 
&#214;ffentlichen Quellen zufolge gibt es in Gro&#223;britannien nur ein KI-Forschungsprogramm im
Verteidigungssektor zum Thema &#187;Information Processing and Sensemaking&#171; (Informationsprozessierung und
Sinnstiftung), das sich u. a. mit Datenverarbeitung, automatischem Lernen und Modellgenerierung befasst (dazu und
zum Folgenden Alwardt et al. 2017, S. 31 ff.; GOV.UK 2015). Die britische Regierung hat aber bereits einige 
bilaterale Innovationsprojekte auf dem Gebiet der Autonomie initiiert, wie z. B. die angloamerikanische &#187;
Innovation Autonomy Challenge&#171; (MOD 2017a) oder das britisch-franz&#246;sische &#187;Future Combat Air System&#171;
(POST 2015). Zusammen mit britischen R&#252;stungsunternehmen f&#252;hrt die Royal Navy das Programm &#187;
Unmanned-Warrior&#171; durch, welches an der Datenintegration von unbemannten Flugzeugen, Schiffen und U-Booten 
forscht. 2016 gelang eine entsprechende Demonstration mit 50 unbemannten Fahrzeugen in den Bereichen
&#220;berwachung, Informationssammlung und Minenabwehrma&#223;nahmen51 (Royal Navy 2016).
Au&#223;erdem investiert Gro&#223;britannien in Wettbewerbe und Entwicklungsprogramme zu (teil)
automatisierten Systemen, die das Defence Science and Technology Laboratory (DSTL) koordiniert (Alwardt et al. 2017,
S. 32 ff.). Der Schwerpunkt liegt dabei auf Plattformleistungsf&#228;higkeit und maschinellen Datenprozessen
(Boulanin 2016b, S. 38 ff.).
R&#252;stungspolitisch liegt der Fokus Gro&#223;britanniens, wie der der meisten anderen L&#228;nder auch, auf der
Drohnenentwicklung. Bis 2021 sollen zehn zus&#228;tzliche UCAVs in die britischen Streitkr&#228;fte integriert werden,
wodurch die Flotte auf 20 Plattformen anwachsen w&#252;rde (Ackerman 2016; Brooke-Holland 2015, S. 17 f.; dazu
und zum Folgenden Alwardt et al. 2017, S. 32 ff.). Obwohl es sich hierbei um &#187;General Atomics MQ-9 Reaper&#171;
handelt (fr&#252;her &#187;Predator B&#171; genannt), gab die Royal Air Force bekannt, diese &#187;Protector&#171; nennen zu wollen.
Anstatt mit US-amerikanischen Hellfire-Raketen werden sie mit britischen Brimstone-2-Raketen bewaffnet.
Parallel dazu arbeitet Gro&#223;britannien &#8211; inzwischen offenbar auf eigene Faust, nachdem die Kooperation
mit Frankreich am Future Combat Air System beendet scheint &#8211; unter dem Projektnamen &#187;Tempest&#171; an der
Entwicklung eines Kampfflugzeugs der n&#228;chsten Generation (Wiegold 2018). Durch BAE Systems hat
Gro&#223;britannien bereits einen UCAV-Demonstrator mit dem Namen &#187;Taranis&#171; entwickeln lassen; seinen
Jungfernflug absolvierte der Prototyp 2013 (Farmer 2015). 2010 wurde das Verteidigungsministerium mit den Worten
zitiert, bei &#187;Taranis&#171; handele es sich um ein &#187;vollst&#228;ndig autonomes&#171; Flugzeug, das &#187;sich gegen bemannte und
andere unbemannte Flugzeuge verteidigen kann&#171; (Cartwright 2010; vgl. auch Altmann/Gubrud 2017, S. 108
f.). Allerdings wird die Serienproduktion nicht angestrebt: Bei UCAV-Prototypen wie &#187;Taranis&#171; (oder auch 
&#187;nEUROn&#171;) geht es vielmehr darum, eine Grundlage f&#252;r zuk&#252;nftige Einsatzf&#228;higkeiten &#252;ber 2030 hinaus zu
schaffen und neue Konzepte zu erforschen, z. B. MUM-T mit bemannten Flugzeugen wie der &#187;Typhoon&#171; oder
&#187;Lightning II&#171; (Brooke-Holland 2015, S. 21 ff.).
50 Im Original: &#187;[b]y 2035, it seems likely that automated systems will be advanced and highly adaptable. [...] Advances in technology 
will almost certainly enable swarm attacks, allowing numerous devices to act in concert.&#171; (MOD 2015, S.16 ff.)
51 Im Original: &#187;surveillance, intelligence-gathering and mine countermeasures&#171;.
4.2.3 Weitere Schl&#252;sselakteure
Russland
Russlands Engagement ist im Bereich von unbemannten Systemen, insbesondere UCAVs, noch vergleichsweise 
neu und liegt bei FuE, Produktion und Beschaffung weit hinter f&#252;hrenden Herstellerl&#228;ndern wie den USA und
Israel zur&#252;ck (Bendett 2017; dazu und zum Folgenden Altmann/Gubrud 2017, S. 110 ff.; Alwardt et al. 2017,
S. 33). In den letzten Jahren soll Russland jedoch die Bandbreite seines teilautomatisierten Milit&#228;requipments
erweitert haben. 2017 pr&#228;sentierte Russland den d&#252;sengetriebenen Nurfl&#252;gel-Unterschall-Prototypen &#187;Skat&#171;
(mit verringerter Radarsignatur) und 2013 wurde berichtet, MiG habe einen Vertrag bekommen, basierend auf 
dessen Auslegung ein neues UCAV zu entwickeln (Rosenberg 2013). Allerdings legen andere Berichte nahe,
das Programm &#187;Skat&#171; sei beendet und an die Sukhoi Holding Company &#252;bertragen worden und werde dort
unter der Bezeichnung &#187;Okhotnik&#171; weitergef&#252;hrt. Laut Meldungen k&#246;nnte die tarnkappenf&#228;hige Kampfdrohne
&#187;Okhotnik-B&#171; (Deagel.com o. J.) bald flugfertig sein (Global Security o. J.c). Allerdings unterliegen die
Quellen f&#252;r diese Berichte der russischen Staatskontrolle, und sie geben wenig Spezifisches preis.
Ein Schwerpunkt der russischen Entwicklungsaktivit&#228;ten scheint &#8211; hier unterscheidet sich Russland von 
anderen wichtigen L&#228;ndern &#8211; auf unbemannten bewaffneten Bodensystemen zu liegen, die zum Teil bereits in
Syrien eingesetzt wurden (Kap. 4.1.2). Dies befindet sich im Einklang mit einer aktuellen
Modernisierungsstrategie f&#252;r das russische Milit&#228;r, bei der die Landstreitkr&#228;fte hohe Priorit&#228;t genie&#223;en. Diese sieht vor, 70 % des 
russischen Milit&#228;requipments bis 2020 zu erneuern (Bodner 2015). Die wichtigste staatliche FuE-Institution f&#252;r
neue Milit&#228;rtechnologien ist die Russia Foundation for Advanced Studies. Deren Hauptinitiative ist das
Programm &#187;Robotics-2025&#171;, das 2014 beschlossen wurde. Ziel ist es, erheblich in die Entwicklung unbemannter
Land-, See- und Luftfahrzeuge zu investieren &#8211; zuk&#252;nftig soll offenbar ein Drittel aller milit&#228;rischen Fahrzeuge
unbemannt sein (R&#246;tzer 2016). Jedoch ist es schwierig, die zuk&#252;nftigen russischen FuE-Pl&#228;ne oder konkrete
Programme pr&#228;ziser abzusch&#228;tzen, da hier&#252;ber nur sehr wenige Informationen &#246;ffentlich zug&#228;nglich sind.
China
Unbemannte Systeme spielen eine wichtige Rolle f&#252;r die Zukunft des chinesischen Milit&#228;rs (dazu und zum
Folgenden Altmann/Gubrud 2017, S. 109; Alwardt et al. 2017, S. 34 ff.). Bereits heute werden betr&#228;chtliche
staatliche F&#246;rdermittel bereitgestellt und die Chinesen forschen intensiv zu UWS an zahlreichen nationalen
Instituten (DOD 2017a). In den letzten Jahren hat die chinesische Regierung insbesondere massiv in UAVs
investiert. Peking orientiert sich dabei stark an den Aktivit&#228;ten der USA, was u. a. am &#228;u&#223;eren Erscheinungsbild
chinesischer UCAV-Modelle zu erkennen ist. Die chinesischen Institutionen Shenyang Aircraft Design Institute
der Aviation Industry Corporation of China, Shenyang Aerospace University und Hongdu Aviation Industry
Group entwickeln derzeit das &#187;Lijian&#171; (scharfes Schwert), ein d&#252;sengetriebenes UCAV mit verringerter
Radarsignatur, &#228;u&#223;erlich &#228;hnlich zu &#187;X-47B&#171;, &#187;nEUROn&#171; und &#187;Taranis&#171;; der Erstflug erfolgte 2013 (Lin/Singer
2017).
Zuk&#252;nftige chinesische Marschflugk&#246;rper sollen ein hohes Niveau an KI und Automatisierung aufweisen
(Lei 2016). F&#252;r die Zukunft sind zudem gro&#223;e Investitionen in unbemannte maritime Systeme &#8211; sowohl Schiffe
als auch U-Boote &#8211; zu erwarten (Chen 2013). Das US-Verteidigungsministerium geht davon aus, dass China
plant, zwischen 2014 und 2023 bis zu 41.800 unbemannte Luft- und Seesysteme zu produzieren (die jedoch
nicht alle bewaffnet sein werden) (DOD 2015a, S. 36 ff.). Im j&#228;hrlichen Bericht an den Kongress zu den
milit&#228;rischen und sicherheitspolitischen Entwicklungen in China von 2017 stellte das US-Verteidigungsministerium
fest, dass Chinas diesbez&#252;gliche Aktivit&#228;ten im Luft- und Seebereich den &#187;signifikanten technischen Vorteil
der USA&#171; mindern werden (DOD 2017a, S. 28 ff.).
Israel
Sowohl die Erforschung als auch die Produktion von unbemannten Systemen hat einen hohen Stellenwert in
Israel, weshalb es einer der weltweit f&#252;hrenden Staaten auf diesem Gebiet ist (dazu und zum Folgenden Alwardt
et al. 2017, S. 34). Die Entwicklung vor allem von unbemannten Luftsystemen ist eine der h&#246;chsten
Verteidigungspriorit&#228;ten, um den milit&#228;rischen Vorsprung in der Region zu halten (Sadot 2016). Diese Systeme werden
heute schon h&#228;ufig als autonom angepriesen (siehe z. B. IAI o. J.). Zwar sind Israels genaue FuE-Bestrebungen
&#252;berwiegend intransparent, aber die israelische Luftwaffe arbeitet derzeit an einer UAV-Roadmap. Von dieser
ist bereits bekannt, dass MUM-T eine wichtige Rolle spielen wird (Egozi 2017). Nicht nur hinsichtlich der 
Roadmap, sondern auch bei den Technologien orientiert sich Israel an den Entwicklungen in den USA, z. B. in
Hinblick auf eine neue Generation von USVs f&#252;r die israelische Marine (Opall-Rome 2016).
        
 
 
  
   
   
  
   
  
  
  
       
    
  
    
  
   
   
 
   
  
     
 
 
 
 
 
    
   
   
 
    
     
    
   
    
  
    
   
    
  
                                              
    
         
         
5
Einsatzszenarien
Aus den beschriebenen Forschungs- und Entwicklungstrends bei AWS k&#246;nnen m&#246;gliche technische
Eigenschaften und andere damit verbundene Charakteristika zuk&#252;nftiger AWS eingesch&#228;tzt und daraus erwartbare
operative F&#228;higkeiten abgeleitet werden. Auf dieser Grundlage lassen sich denkbare milit&#228;rische Missionen
sowie Einsatzszenarien entwickeln, bei denen die erwarteten Vorz&#252;ge von AWS besonders gut zur Geltung
kommen w&#252;rden. Dies dient dazu, einen plastischeren Eindruck zu gewinnen, wie bzw. auf welchen Feldern
AWS zu einer neuen Aufgabenverteilung zwischen Mensch und Maschine f&#252;hren und so zu einer Ver&#228;nderung 
der Kriegsf&#252;hrung insgesamt beitragen k&#246;nnten (Kap. 6).
Da allerdings sowohl die technischen Eigenschaften als auch die Verf&#252;gbarkeit zuk&#252;nftiger AWS noch
eher spekulativ sind, k&#246;nnen hier lediglich Umrisse eines m&#246;glichen Einsatzportfolios skizziert werden.
Dar&#252;ber hinaus sollte bei vorausschauenden Analysen wie dieser immer im Blick behalten werden, dass die damit
vielfach einhergehende &#220;bertragung heutiger Denkweisen auf die Technologien von morgen auch fehlleiten
kann, da Struktur- und Traditionsbr&#252;che nur schwer vorherzusehen sind.
5.1 Argumente f&#252;r AWS
Die drei am h&#228;ufigsten genannten Argumente, warum unbemannte und in Zukunft st&#228;rker autonome Systeme
eine wesentliche Rolle bei milit&#228;rischen Operationen spielen k&#246;nnten, werden im Folgenden dargelegt und
kritisch eingeordnet:
&#8250; AWS k&#246;nnen gef&#228;hrliche, langweilige und schmutzige52 Aufgaben &#252;bernehmen bzw. f&#252;r Menschen
unm&#246;gliche Missionen durchf&#252;hren.
&#8250; AWS k&#246;nnen zu einer schnelleren und besseren Entscheidungsfindung in zeitkritischen Operationen
beitragen.
&#8250; AWS k&#246;nnen zu einer Reduzierung von Kosten f&#252;hren, insbesondere durch Verringerung des
Personaleinsatzes.
Entlastung des Menschen von gef&#228;hrlichen, langweiligen und schmutzigen Aufgaben
Das am h&#228;ufigsten genannte Argument f&#252;r einen verst&#228;rkten Einsatz von autonomen Systemen in den
Streitkr&#228;ften ist, dass Menschen von gef&#228;hrlichen, langweiligen sowie schmutzigen Aufgaben entlastet werden
k&#246;nnten (U.S. Army 2017b, S. 3 ff.). Sowohl anstrengende k&#246;rperliche Aufgaben (Tragen von Lasten) als auch
langweilige oder solche mit kognitiver Dauerbelastung (Datenauswahl und -analyse, Errechnen und Vorschlagen
von Handlungsoptionen etc.) k&#246;nnten Soldaten durch autonome Systeme abgenommen werden. Mit den
freigewordenen Kapazit&#228;ten k&#246;nnten neue Aufgaben &#252;bernommen werden oder es k&#246;nnte Personal eingespart
werden. Ganz generell soll dadurch die Mobilit&#228;t, Effektivit&#228;t und das Durchhalteverm&#246;gen von Truppen erh&#246;ht
werden (U.S. Army 2017b, S. 1).
Andererseits bringt gerade die &#220;bernahme kognitiver Aufgaben durch autonome Systeme eine neue
Qualit&#228;t der Interaktion zwischen Mensch und Maschine ins Spiel, die neue potenzielle Fehlerquellen zur Folge
haben kann. Wenn beispielsweise Soldaten nur noch mit Systemaufsicht und -management besch&#228;ftigt sind und
darauf warten, dass das System eine Auff&#228;lligkeit berichtet, schwindet die kontinuierliche Wachsamkeit und
vermehrte Fehler k&#246;nnen die Folge sein.53 Wenn ein System dagegen vollst&#228;ndig eigenst&#228;ndig und ohne
menschliche Aufsicht operiert, k&#246;nnten schwerwiegende Fehlfunktionen oder Systemfehler unentdeckt bleiben
(Heyns 2013, S. 18). Au&#223;erdem ist unklar, wer die Verantwortung f&#252;r m&#246;gliche (Fehl-)Entscheidungen von 
AWS tragen soll (Kap. 8.3).
52 englisch: &#187;dull, dirty and dangerous&#171;
53 Haider (2014, S.58 f.) berichtet, dass bei mehr als der H&#228;lfte der Unf&#228;lle aller Art bei Predator-Missionen der U.S. Air Force, bei
denen Crewmitglieder involviert waren, Fehler dieser Art der Ausl&#246;ser waren.
Schnellere und bessere Entscheidungsfindung
Autonome Systeme k&#246;nnen Unmengen von Daten aus verschiedensten Quellen sammeln und KI-gest&#252;tzt
auswerten, organisieren und priorisieren. Bei der Verarbeitung gro&#223;er Datenmengen &#252;bertreffen sie menschliche
F&#228;higkeiten bei Weitem. Vom milit&#228;rischen Einsatz solcher Systeme verspricht man sich deshalb einen
wesentlich umfassenderen Gesamt&#252;berblick &#252;ber die jeweilige Situation und somit eine bessere Grundlage f&#252;r schnelle
Entscheidungen und Operationen (U.S. Army 2017b, S. 16). Die Leistungsf&#228;higkeit KI-basierter Systeme wird
dar&#252;ber hinaus auch nicht durch Stress oder M&#252;digkeit beeintr&#228;chtigt, und kognitive Verzerrungen &#8211; z. B. durch
zu optimistische Annahmen (Sharot 2014) bzw. Selbst&#252;bersch&#228;tzung (Johnson 2004) &#8211;, f&#252;r die Menschen
anf&#228;llig sind, spielen keine Rolle. Besonders in zeitkritischen Operationen sollen autonome Systeme zu
schnelleren und konsistenteren Entscheidungen als Menschen f&#228;hig sein (DSB 2016, S. 1 u. 6). Allerdings sind KI-
basierte Systeme oft anf&#228;llig f&#252;r Bias und andere unerw&#252;nschte Eigenschaften (Kap. 3.3.3).
Aber auch auf der Ebene der Strategieentwicklung werden KI-basierten Systemen Vorteile gegen&#252;ber
Menschen zugeschrieben: Sie k&#246;nnen Muster in Daten entdecken, die ansonsten unentdeckt bleiben w&#252;rden,
und neuartige Strategien entwickeln, die menschlichen Gewohnheiten und (Vor)Urteilen zuwiderlaufen. Die
aktuellen Erfolge von KI &#252;ber die besten menschlichen Spieler in etlichen strategischen Spielen sprechen eine
deutliche Sprache (Kap. 3.2).
Es ist allerdings gegenw&#228;rtig fraglich, wie robust durch KI entwickelte Strategien in realen offenen,
dynamischen und feindlich dominierten Umgebungen sind. Skepsis ist angebracht, da teilweise schon minimale
Ver&#228;nderungen der Szenarien, an denen die KI trainiert wurde, ausreichen, um sie v&#246;llig hilflos zu machen (Marcus
2018, S. 7 ff.). Hinzu kommen das Fehlen eines echten Verst&#228;ndnisses f&#252;r Kontexte sowie die
Blackboxeigenschaft der meisten KI-Systeme, die es schwer bis unm&#246;glich macht nachzuvollziehen, aus welchen Gr&#252;nden
bestimmte Entscheidungen zustande gekommen sind.54 Dies erschwert es, Vertrauen in die Sinnhaftigkeit dieser
Entscheidungen aufzubauen (DSB 2016, S. 14 ff.). Dass die Aktionen autonomer Systeme im Einzelnen oft
schwer vorherzusehen sind, stellt gleichzeitig eine H&#252;rde daf&#252;r dar, sie in milit&#228;rische Kommandostrukturen zu
integrieren, die darauf ausgelegt sind, dass Befehle akkurat und zuverl&#228;ssig ausgef&#252;hrt werden.
Reduzierung von Kosten
Ein h&#228;ufig angef&#252;hrtes Argument f&#252;r den verst&#228;rkten Einsatz unbemannter und zuk&#252;nftig autonomer Systeme
sind verringerte Kosten gegen&#252;ber bemannten Systemen. Einige sind sogar der Auffassung, dass diese
Entwicklung unausweichlich ist, da die Kosten f&#252;r Entwicklung, Beschaffung und Unterhalt bemannter Systeme sowie 
die zugeh&#246;rigen Personalkosten so stark ansteigen, dass sie kaum noch bew&#228;ltigt werden k&#246;nnen (Work/
Brimley 2014, S. 5 f.). Bei anhaltendem Trend w&#252;rden beispielsweise in den USA die Personalkosten bis 2021 ca.
46 % des gesamten Verteidigungsbudgets aufzehren (Work/Brimley 2014, S. 21). Vor diesem Hintergrund
sollen AWS ein Mittel darstellen, um mit weniger Personal und Equipment dieselben (oder sogar bessere)
Ergebnisse zu erreichen.
Wie begr&#252;ndet diese Hoffnung ist, l&#228;sst sich aus heutiger Sicht nicht abschlie&#223;end beurteilen. Existierende 
Vergleiche von bemannten mit unbemannten Flugzeugen ergeben kein eindeutiges Bild. So kommt das britische
Verteidigungsministerium zu dem Schluss, dass die Betriebskosten von unbemannten Flugzeugen auf
demselben Niveau oder sogar h&#246;her als die &#228;quivalenter bemannter Flugzeuge liegen k&#246;nnen, aufgrund der
notwendigen Bodeninfrastruktur, Vernetzung sowie logistischer Unterst&#252;tzung (MOD 2013, S. 10). Dies l&#228;sst sich an
einem konkreten Zahlenbeispiel aufzeigen: Die gesamten Betriebskosten pro Flugstunde betragen bei einem
unbemannten &#187;RQ-4B Global Hawk&#171; etwa 49.000 US-Dollar, wohingegen eine &#187;U-2 Dragon Lady&#171;
(bemanntes Spionageflugzeug) mit ca. 31.000 US-Dollar zu Buche schl&#228;gt (Thompson 2013).
Allerdings ist einer der gr&#246;&#223;ten Kostenfaktoren beim Betrieb bemannter Flugzeuge der Bedarf an
Trainingsfl&#252;gen zur Ausbildung von Piloten und zur Aufrechterhaltung ihrer F&#228;higkeiten.55 Bediener von
unbemannten Flugzeugen k&#246;nnen dagegen weitgehend in Simulatoren ausgebildet werden. Daher w&#252;rde eine
Substitution bemannter durch unbemannte Flugzeuge die Kosten f&#252;r Trainingsfl&#252;ge erheblich reduzieren (Boula-
54 Laut dem ehemaligen Schachweltmeisters Garri Kasparov (2018): &#187;Explainability is still an issue&#171;.
55 Gem&#228;&#223; Boulanin/Verbruggen (2017, S.63) muss ein Kampfpilot 10 bis 20 Flugstunden im Monat unter realen Bedingungen
absolvieren.
nin/Verbruggen 2017, S. 63). Ein weiterer Ansatz zur Reduktion der Personalkosten ist, Systeme so zu
designen, dass ein menschlicher Bediener gleichzeitig mehrere Plattformen bedienen kann (DOD 2013, S. 68), was
f&#252;r zunehmend autonomere Systeme eine naheliegende Option ist.
Insgesamt ist es somit schwer zu sagen, ob beim Vergleich der Gesamtkosten bemannte oder unbemannte 
Systeme g&#252;nstiger abschneiden. Hinzu kommt, dass ein direkter Vergleich oft gar nicht m&#246;glich ist, da
unbemannte Systeme ein anderes F&#228;higkeits- und Einsatzspektrum abdecken als bemannte Systeme.56 
5.2 Erwartete milit&#228;rische F&#228;higkeiten
Die milit&#228;rischen F&#228;higkeiten, die f&#252;r AWS erwartet werden, umfassen auf der einen Seite die Vorteile aktueller
bzw. in Entwicklung befindlicher UWS gegen&#252;ber bemannten Systemen. Diese beruhen in erster Linie darauf,
dass kein menschlicher Bediener an Bord des Systems ist und daher die daraus resultierenden Beschr&#228;nkungen
hinsichtlich Gr&#246;&#223;e, Gewicht und Ausstattung des Systems entfallen. Auf der anderen Seite unterscheiden sich
die technischen Systemf&#228;higkeiten von AWS von UWS dadurch, dass Datenprozesse und
Kommunikationserfordernisse in autonomen Systemen v&#246;llig anders gestaltet sind als in unbemannten, aber mindestens partiell 
ferngesteuerten Systemen.
Im Folgenden werden zu erwartende Charakteristika zuk&#252;nftiger autonomer Systeme dargestellt und
hinsichtlich ihrer m&#246;glichen Vorteile, aber auch der gegen sie bestehenden Vorbehalte beleuchtet. Die Darstellung
konzentriert sich stark auf fliegende Systeme, da die Unterschiede zu bemannten Systemen hier am
prominentesten hervortreten. Au&#223;erdem befinden sich fliegende AWS weitaus n&#228;her am Stadium der Einsatzreife als
boden- bzw. seegest&#252;tzte Systeme. Bei den folgenden Ausf&#252;hrungen handelt es sich um eine &#252;berarbeitete
Fassung des Gutachtens von Alwardt et al. (2017, Kap. 3).
Gewicht
UWS &#8211; ob autonom agierend oder ferngesteuert &#8211; sparen das Gewicht des menschlichen Operators und der
Innenausstattung ein (Sitz, gepanzerte Kabine, Sauerstoffversorgung etc.). Daher haben sie h&#228;ufig ein
geringeres Gewicht als bemannte Systeme, was u. a. zu einem geringeren Treibstoffverbrauch f&#252;hren kann (Scharre
2014a, S. 12 ff.). Dadurch k&#246;nnten Plattformen l&#228;nger ununterbrochen eingesetzt werden. Allerdings schl&#228;gt
dies umso weniger zu Buche, je gr&#246;&#223;er die Plattform ist (Scharre 2014a, S. 10). Zudem m&#252;ssen
Gewichtseinsparungen nicht zwingend zu leichteren Systemen f&#252;hren; sie k&#246;nnen sich auch in einer h&#246;heren Nutzlast
niederschlagen (Sensoren, Wirkmittel, h&#246;herer Treibstoffvorrat f&#252;r gr&#246;&#223;ere Reichweite etc.).
Der Verzicht auf einen menschlichen Bediener an Bord erlaubt auch sehr kleine und leichte Plattformen.
Hier stellt sich allerdings die Frage, ob diese bei schwierigen Operationsbedingungen (beispielsweise starker
Wind, Regen etc.) robust genug sind.
Einsatzdauer
Bei unbemannten Systemen muss nicht auf menschliche Einschr&#228;nkungen, z. B. hinsichtlich Beschleunigung
(G-Kraft), Bed&#252;rfnisse und insbesondere Ruhephasen R&#252;cksicht genommen werden (Heyns 2013, S. 10).
Letzteres erm&#246;glicht eine l&#228;ngere Einsatzdauer, was gleichzeitig auch eine gr&#246;&#223;ere Reichweite bedeutet, da l&#228;ngere
Distanzen zur&#252;ckgelegt werden k&#246;nnen. Damit werden sowohl zeitlich als auch geografisch ausgedehntere
&#220;berwachungs- und Aufkl&#228;rungsmissionen m&#246;glich, auch an Orten, an die bemannte Plattformen nicht
gelangen k&#246;nnen (U.S. Army 2017b, S. 1 f.). Um etwa eine Rund-um-die-Uhr-Aufkl&#228;rungsmission mit durchgehend
mindestens einem Flugzeug vor Ort in Libyen von einer Basis in Sizilien aus durchzuf&#252;hren, w&#252;rden f&#252;nf
bemannte, aber nur zwei unbemannte Flugzeuge ben&#246;tigt.57 Eine Mission in Mali w&#228;re von Sizilien aus mit vier
56 Beispielsweise findet man im englischen Wikipediaartikel &#252;ber das f&#252;r Anti-U-Boot-Eins&#228;tze in der Entwicklung befindliche
autonome Schiff &#187;Sea Hunter&#171; die Angabe, es sei mit Betriebskosten von 15.000 bis 20.000 US-Dollar pro Tag erheblich g&#252;nstiger als
der Betrieb eines Destroyer (deutsche Bezeichnung: Fregatte) mit 700.000 US-Dollar. Allerdings ist die &#187;Sea Hunter&#171; lediglich 
40 m lang und hat eine Verdr&#228;ngung von 135 t, ein Destroyer der Arleigh-Burke-Klasse ist hingegen etwa 150 m lang und verdr&#228;ngt
ca. 9.000 t (Wikipedia 2003b u. 2016b).
57 Angenommen wurden: unbemanntes Flugzeug: Ausdauer 24 Stunden, bemanntes Flugzeug: Ausdauer 8 Stunden; Transitzeit
Sizilien &#8211; Libyen: 2 Stunden, Sizilien &#8211; Mali: 8 Stunden; d.h., der Transit nach Mali w&#252;rde die gesamte maximal m&#246;gliche Flugzeit
des bemannten Aufkl&#228;rungsflugzeugs aufzehren.
7
unbemannten Flugzeugen durchf&#252;hrbar und mit bemannten Flugzeugen &#252;berhaupt nicht darstellbar (Scharre 
2014a, S. 14 f.) (Abb. 5.1). 
Einsatzf&#228;higkeit 
AWS werden voraussichtlich aufgrund ihrer l&#228;ngeren Ausdauer mobiler sein und k&#246;nnten somit vom Ort ihrer 
Stationierung aus flexibler eingesetzt werden als bemannte Plattformen. Dies kann potenziell erhebliche
Auswirkungen darauf haben, wie sich Streitkr&#228;fte in Bezug auf m&#246;gliche Einsatzgebiete rund um den Globus
aufstellen (Work/Brimley 2014, S. 31 f.). Somit bef&#246;rdern autonome Systeme gemeinsam mit anderen neuen
Technologien die in der aktuellen Verteidigungsstrategie der USA geplante Modernisierung der noch aus der an den 
Kalten Krieg anschlie&#223;enden &#196;ra stammenden Leitbilder f&#252;r die globale Handlungsf&#228;higkeit der US-Kr&#228;fte 
(DOD 2018b, S. 7).58 
Abb. 5.1 Anzahl ben&#246;tigter Luftfahrzeuge f&#252;r 24/7-Abdeckung 
Atlantik
Schwarzes
unbemanntes Meer
MALE Italien 
Spanien 
Milit&#228;rflugplatz 
Sigonella 
T&#252;rkei 
Ausdauer: 460 Seemeilen 
24 Flug- 2 Stunden Transitzeit 
stunden 
Algerien Mittelmeer Irak 
Tripolis 
Saudi 
bemanntes Arabien 
Flugzeug &#196;gypten 
Lybien 
1.840 Seemeilen 
8 Stunden Transitzeit 
Ausdauer: 
8 Flug- Mali Niger 
stunden 
unzureichende 
Entfernung  
Quelle: nach Scharre 2014a, S. 14 f. 
Die US-Armee geht von einer rascheren Einsatzf&#228;higkeit bzw. Abrufbarkeit von AWS aus. Langfristig soll dies 
eine verbesserte Integration und Synchronisierung von Teilstreitkr&#228;ften und eine Flexibilisierung und
Dezentralisierung von Operationen erm&#246;glichen bis hin zu einem schnelleren &#220;bergang zu offensiven Operationen 
nach Eintritt in den Gefechtsraum (U.S. Army 2017b, S. 10 ff.). Dies mag zutreffen, allerdings stellen sich auch 
neue Fragen bez&#252;glich der zunehmenden Abh&#228;ngigkeit von Technologie bei der Entscheidungs- und
Zielfindung sowie der Operationsf&#252;hrung (Unfallh&#228;ufigkeit, St&#246;ranf&#228;lligkeit mit und ohne feindliche Einwirkung etc.). 
Abstandsf&#228;higkeit 
W&#252;rden AWS k&#252;nftig gef&#228;hrliche Missionen &#252;bernehmen, k&#246;nnten die eigenen Truppen einen gr&#246;&#223;eren
Sicherheitsabstand zu gegnerischen Verb&#228;nden oder gef&#228;hrlichen Arealen einhalten. Dadurch verringert sich das Ri-
  
Im Original: &#187;A modernized Global Operating Model of combat-credible, flexible theater postures will enhance our ability to
compete and provide freedom of maneuver during conflict, providing national decision-makers with better military options.&#171; 
58
siko f&#252;r die eigenen Soldaten. Dies ist sowohl in Aufkl&#228;rungs- und Angriffsoperationen als auch bei
Versorgungsmissionen relevant, da Konvois h&#228;ufig attackiert oder mit improvisierten Sprengk&#246;rpern konfrontiert
werden. Demzufolge k&#246;nnten autonom operierende Konvois hier gro&#223;e Vorteile bieten (U.S. Army 2017b, S. 2). 
Dar&#252;ber hinaus wird oft betont, dass Roboter Arbeiten in kontaminierten Gebieten &#252;bernehmen k&#246;nnen, z. B.
nach chemischen, biologischen, radiologischen oder nuklearen Attacken (DSB 2016, S. 45).
Verl&#228;sslichkeit
Bef&#252;rworter einer st&#228;rkeren Nutzung von autonomen Funktionen argumentieren, dass das Verhalten autonomer
Systeme oft verl&#228;sslicher, konsistenter und vorhersehbarer als das von Menschen ist, da es auf programmierten
Regeln beruht (DSB 2016, S. 1). Gleichzeitig sollen autonome Systeme die Beschr&#228;nkungen von
automatisierten Systemen &#252;berwinden und auch in unvorhergesehenen Situationen zielgerichtet agieren k&#246;nnen. Dies wird
wiederum damit erkauft, dass das System sich unvorhersagbar verhalten kann bzw. nicht mehr nachvollziehbar
ist, warum ein System eine bestimmte Aktion durchgef&#252;hrt hat (U.S. Air Force/Office of the Chief Scientist
2015, S. 5 f.). Verl&#228;sslichkeit und &#220;berpr&#252;fbarkeit der Aktionen eines AWS w&#228;ren damit infrage gestellt (UN-
IDIR 2018b, S. 8).
Datenprozesse
Autonome Systeme k&#246;nnen ein gro&#223;es Datenvolumen und diverse Datentypen (Bilder in allen verf&#252;gbaren
Spektralbereichen, akustische, Radar- und andere elektromagnetische Signale, nachrichtendienstliche
Datenanalysen etc.) selbstst&#228;ndig erfassen, bearbeiten, auswerten und nutzen (DSB 2016, S. 45). Der Prozess, aus
Rohdaten verwertbare Informationen zu generieren, k&#246;nnte somit erheblich beschleunigt werden. Aufgrund der
effizienteren Datenprozesse und rascheren Kommunikation k&#246;nnen AWS Aufgaben erledigen, die die
menschliche Reaktionszeit &#252;berfordern w&#252;rden. Dies k&#246;nnte die Geschwindigkeit von Operationen ma&#223;geblich
erh&#246;hen (DOD 2017b, S. 20).
Einschr&#228;nkend muss jedoch festgehalten werden, dass neben einer schnellen Reaktionszeit auch die
Qualit&#228;t der Reaktion relevant ist. Im Gegensatz zum Menschen sind autonome Systeme weder zur Interpretation
eines Kontextes noch zu reflektiertem oder moralischem Handeln bzw. zur situationsgerechten Auslegung
rechtlicher Prinzipien in der Lage. Nur weil ein Ziel identifiziert und lokalisiert wurde und die
Wahrscheinlichkeit hoch ist, dass es effizient und schnell bek&#228;mpft werden kann, hei&#223;t das noch nicht, dass seine Bek&#228;mpfung
im strategischen, sozialen, ethischen und rechtlichen Kontext sinnvoll oder erlaubt ist (Heyns 2013, S. 10 f.)
(siehe dazu Kap. 7 u. 8).
Kommunikationsverbindung
Kommunikationsverbindungen sind ein zentrales Schl&#252;sselelement des gegenw&#228;rtigen Paradigmas der
netzwerkzentrierten Kriegsf&#252;hrung (in den USA &#187;Network Centric Warfare&#171; &#8211; NCW; Wilson 2005; in Deutschland 
&#187;Vernetzte Operationsf&#252;hrung&#171; &#8211; NetOpF&#252;; BMVg 2004, S. 15). Bei unbemannten Systemen sind Daten- und 
Kommunikationsverbindungen zum Austausch von Sensordaten, Missionsparametern und Steuerbefehlen
zwischen dem mobilen System und einer Basisstation unverzichtbar. Dies bringt mehrere Probleme mit sich, f&#252;r
die ein h&#246;heres Ausma&#223; an Autonomie oft als Beitrag zu deren L&#246;sung dargestellt wird:
Die Schnelligkeit von heutigen Operationen mit unbemannten ferngesteuerten Systemen leidet h&#228;ufig
unter Verz&#246;gerungen durch die (in der Regel satellitengest&#252;tzte) Daten&#252;bertragung &#252;ber globale Distanzen. Dies
w&#252;rde entfallen, da AWS nicht auf externe Steuerbefehle angewiesen w&#228;ren (zumindest nicht kontinuierlich),
d. h., AWS k&#246;nnten wesentlich schneller operieren (Heyns 2013, S. 10).
Die verf&#252;gbare Bandbreite der Kommunikationsverbindung begrenzt oft die M&#246;glichkeiten der
Datennutzung (z. B. keine &#220;bertragung von hochaufl&#246;senden Echtzeitvideos m&#246;glich). Durch eigene
Datenverarbeitungskapazit&#228;ten eines AWS k&#246;nnten aus gro&#223;en Mengen an Sensorrohdaten direkt an Bord
entscheidungsrelevante Informationen gewonnen werden, sodass eine breitbandige &#220;bertragung ggf. entbehrlich w&#228;re.
Daten- und Kommunikationsverbindungen hinterlassen elektronische und digitale Spuren, die vom Gegner
aufgesp&#252;rt werden k&#246;nnen, was dazu f&#252;hren kann, dass Plattformen entdeckt und bek&#228;mpft werden. Auch die
Kommunikationsverbindungen selbst k&#246;nnen zum Angriffsobjekt werden. Beispielsweise k&#246;nnen GPS-Signale 
von Satelliten durch Widersacher blockiert (&#187;jamming&#171;) oder gef&#228;lscht (&#187;spoofing&#171;) werden, damit der Pilot
die Kontrolle &#252;ber das ferngesteuerte Ger&#228;t verliert.59 Denkbar ist ebenfalls, dass Systeme &#252;ber die
Kommunikationsverbindungen gehackt und ihre Software oder das Aufgabenprofil ver&#228;ndert werden. AWS k&#246;nnten
dagegen &#252;ber einen l&#228;ngeren Zeitraum eigenst&#228;ndig operieren, womit die Kommunikation mit einer Basisstation
minimiert wird. Dies w&#252;rde sowohl das Entdeckungsrisiko als auch die Angriffsfl&#228;che f&#252;r elektronische
Attacken (&#187;jamming&#171;, &#187;spoofing&#171;, &#187;hacking&#171;) reduzieren. Direkte Kommunikation zwischen AWS (&#187;peer to peer&#171;)
ist deutlich schwerer zu entdecken und zu manipulieren als satellitengest&#252;tzter Funk und k&#246;nnte die
selbstst&#228;ndige Koordination mehrerer AWS (als Schwarm) untereinander zulassen (DSB 2016, S. 84). Dem geringeren
Risiko elektronischer Attacken stehen allerdings potenziell gravierende Konsequenzen gegen&#252;ber, besonders
wenn aufgrund fehlender Kommunikationsm&#246;glichkeiten kein menschlicher Bediener in der Lage w&#228;re
einzugreifen, wenn AWS-Operationen torpediert oder manipuliert w&#252;rden. V&#246;llig ohne Kommunikationsverbindung 
werden die meisten AWS voraussichtlich nicht auskommen. Ein gewisser Datenaustausch wird zur Betreuung
der Operation und zur Verteilung von Daten an andere an der Operation beteiligte Kr&#228;fte sowie nicht zuletzt
zur Aufrechterhaltung eines gewissen Ma&#223;es an menschlicher Kontrolle ben&#246;tigt. Um die Systeme und
Operationen zu sichern, w&#228;ren insofern hochentwickelte Verschl&#252;sselungs- und Schutztechnologien notwendig (U.S.
Air Force 2016, S. 26 ff.).
Schwarmf&#228;higkeit
Im Kontrast zu den bisher behandelten konventionellen Eigenschaften er&#246;ffnet die F&#228;higkeit von AWS, aus
einer Vielzahl von Einzelsystemen Schw&#228;rme zu bilden, v&#246;llig neuartige Einsatzm&#246;glichkeiten f&#252;r AWS.
Schw&#228;rme k&#246;nnen in ihrer Gesamtheit Wirkungen erzielen, die quantitativ und qualitativ weit &#252;ber die eines
einzelnen Systems hinausreichen. Dass eine Gruppe von Individuen sich mit Blick auf ein gemeinsames Ziel 
selbst koordiniert und ihre Aktionen untereinander abstimmt, ist ein in der Tierwelt verbreitetes Ph&#228;nomen.
Selbst mit minimaler Kommunikation untereinander und auf einfachen Regeln beruhenden Aktionen von
Individuen kann ein Schwarm als Ganzes intelligent erscheinendes Verhalten ausbilden. Dieses auf KI-gest&#252;tzte
technische Systeme zu &#252;bertragen, ist Gegenstand intensiver Forschungsbem&#252;hungen. Bei hinreichend gro&#223;er
Autonomie auf Missionsebene w&#228;ren keine oder nur eine kleine Anzahl menschlicher Bediener f&#252;r eine gro&#223;e
Menge an Plattformen erforderlich.
Dieser Ansatz weist wesentliche Vorteile auf. Entsprechende technologische Fortschritte vorausgesetzt,
k&#246;nnte dies eine Abkehr vom Paradigma einleiten, immer teurere und spezialisiertere Systeme zu entwickeln.
Stattdessen k&#246;nnte eine gro&#223;e Menge kleiner und kosteng&#252;nstiger Ger&#228;te im Kollektiv &#228;hnliche Funktionen 
erf&#252;llen oder v&#246;llig neue M&#246;glichkeiten er&#246;ffnen (Scharre 2014b, S. 13 ff.):
&#8250; Die Kampfkraft wird st&#228;rker verteilt, sodass der Gegner mit einer gr&#246;&#223;eren Anzahl an Zielen konfrontiert
wird.
&#8250; Durch die gro&#223;e Anzahl k&#246;nnen Verteidigungssysteme &#252;berlastet werden.
&#8250; Die &#220;berlebensf&#228;higkeit einzelner Plattformen wird weniger wichtig. An ihre Stelle tritt die
Schwarmresilienz.
&#8250; Bei Verlusten sinkt die Kampfkraft nur sukzessive, statt zusammenzubrechen, wenn eine gro&#223;e Einheit 
verlorengeht.
Durch Schw&#228;rme k&#246;nnten sowohl die offensive Schlagkraft erh&#246;ht als auch die Defensive gest&#228;rkt werden. Sie
k&#246;nnten beispielsweise als Ablenkungsman&#246;ver, zum &#187;jamming&#171; der gegnerischen Kommunikation, zum
Aufbau von robusten Sensor- und Kommunikationsnetzen bzw. als Tr&#228;ger variabler, der Situation angepasster
letaler oder sonstiger Nutzlasten eingesetzt werden (DSB 2016, S. 61 ff.). So k&#246;nnte beispielsweise eine gro&#223;e
Anzahl billiger Wegwerfsysteme einen gegnerischen Flugplatz lahmlegen, indem sie den Luftraum wie
Heuschrecken quasi verminen, wenn sie vom Ger&#228;usch herannahender Flugzeuge angelockt werden.
Wegwerfsysteme m&#252;ssen nicht per se klein und billig sein. Ein Beispiel sind die Antischiffsmarschflugk&#246;rper &#187;LRASM&#171;,
bei denen es sich um technisch hochentwickelte und kostspielige Plattformen handelt, die &#252;ber die F&#228;higkeit
verf&#252;gen, sich untereinander zu koordinieren (Kap. 4.1.1).
Dickow (2015, S.11) schildert ein Fallbeispiel, wie 2011 eine US-Drohne vom Typ &#187;RQ-170 Sentinel&#171; offenbar vom iranischen 
Milit&#228;r durch elektronische Manipulation entf&#252;hrt wurde.
59 
        
 
 
  
    
 
         
  
   
     
  
     
    
     
   
 
    
      
       
  
  
 
  
 
 
 
 
 
 
  
 
  
 
   
 
 
   
 
  
  
  
   
   
 
  
  
  
  
  
 
        
5.3
Als Gegenma&#223;nahme zu Schw&#228;rmen kommt der Einsatz von Gegenschw&#228;rmen in Betracht. Taktiken f&#252;r
solche Schwarm-gegen-Schwarm-K&#228;mpfe sind derzeit noch Neuland. Es ist nicht unplausibel, dass die
Kampfkraft von Schw&#228;rmen st&#228;rker von den Taktiken und Algorithmen f&#252;r bessere Koordination bestimmt wird als
von der Qualit&#228;t der einzelnen Plattformen (Scharre 2014b, S. 42). Allerdings best&#252;nde ein erhebliches Risiko 
zur ungewollten Eskalation, wenn in Krisensituationen gegnerische Schw&#228;rme in gro&#223;er N&#228;he zueinander
stationiert w&#228;ren und auch bei fehlinterpretierten Signalen automatische Gegenreaktionen getriggert w&#252;rden, ohne 
dass Menschen die M&#246;glichkeit h&#228;tten, korrigierend einzugreifen (Altmann/Sauer 2017, S. 128).
Missionen/Einsatzszenarien
Vor dem Hintergrund der dargestellten potenziellen milit&#228;rischen F&#228;higkeiten von AWS werden nun denkbare 
Einsatzszenarien diskutiert, in denen diese Systemf&#228;higkeiten besonders zum Tragen kommen k&#246;nnten. In der
&#220;bersicht in Tabelle 5.1 werden Typen von Missionen aufgelistet, f&#252;r die laut Strategiepapieren der US-
Streitkr&#228;fte potenzielle operative Vorteile von AWS in den Kategorien Geschwindigkeit, Agilit&#228;t, Ausdauer,
Reichweite sowie Koordination Anwendung finden.
Im Folgenden werden basierend auf dem Gutachten von Alwardt et al. (2017, S. 46 ff.) einige der
erwarteten Missionstypen aus Tabelle 5.1 aufgegriffen, in denen AWS alleine, als Schwarm mit anderen AWS oder
zusammen mit Menschen agieren k&#246;nnten (Kap. 5.3.1). Im Anschluss werden auf dieser Grundlage f&#252;nf
denkbare Einsatzszenarien f&#252;r zuk&#252;nftige AWS entwickelt (Kap. 5.3.2).
Tab. 5.1 Missionen, in denen operative Vorteile von AWS besonders zum Tragen
kommen
Hauptvorteile der Autonomie Art der Mission
Geschwindigkeit: Lichtgeschwindigkeit bei
der Umsetzung der Schleife
&#187;beobachten, orientieren, entscheiden,
handeln&#171;
Agilit&#228;t: weniger Abh&#228;ngigkeit von
Kommando und Kontrolle
Beharrlichkeit: konstante Leistung
unbemannter Systeme f&#252;r &#187;langweilige,
schmutzige und gef&#228;hrliche&#171; Eins&#228;tze
Reichweite: Zugang zu GPS und
Umgebungen ohne Kommunikation
Koordinierung: F&#228;higkeit, gro&#223;e Gruppen
von Waffensystemen auf strukturierte und
strategische Weise zu
koordinieren
Luftverteidigung, Cyberverteidigung;
elektronische Kriegsf&#252;hrung
ISR-Mission, Cyberkriegsf&#252;hrung,
elektronische Kriegsf&#252;hrung, U-Boot- und Minenjagd, 
logistische Operationen
Luftverteidigung, lange ISR-Missionen,
Antimineneinsatz, Evakuierung von Verletzten, 
logistische Operationen auf feindlichem
Gebiet
ISR-Missionen in A2/AD-Umgebungen, U-
Boot- und Minenjagd, Evakuierung von
Verletzten, Logistikoperationen in A2/AD-
Umgebungen, Schl&#228;ge in A2/AD-Umgebungen
Schutz der Streitkr&#228;fte, Kampfeins&#228;tze in
A2/AD-Umgebungen, ISR-Missionen in
komplexen und un&#252;bersichtlichen Umgebungen
A2/AD (&#187;anti access and area denial&#171;) = Zugangsverweigerung und Absperrung von
Gebieten; GPS = globales Positionsystem; ISR = Nachrichtengewinnung, &#220;berwachung und
Aufkl&#228;rung
Quellen: Boulanin/Verbruggen 2017, S. 62; DOD 2016; &#220;bersetzung durch das TAB
5.3.1 Erwartete Missionen
Milit&#228;rische Planer und Analysten gehen davon aus, dass AWS aufgrund ihrer angenommenen Vielseitigkeit 
bei zahlreichen milit&#228;rischen Einsatzszenarien eine wichtige Rolle spielen werden &#8211; mit oder ohne letale
Waffeneins&#228;tze. Sie k&#246;nnten alleine, als Schwarm oder gemeinsam mit Soldaten im Sinne eines Mensch-Maschine-
Teams (MUM-T) eingesetzt werden (DOD 2013, S. 139).
Ein wichtiger Einsatzfaktor ist die Beschaffenheit des Terrains. Gebiete, die relativ homogen sind und
wenige nat&#252;rliche Hindernisse aufweisen, sind daf&#252;r pr&#228;destiniert, dass weitgehend autonome Systeme dort als
Erstes verwendet werden. Dies ist der Grund, warum fliegenden Systemen in der Diskussion um AWS eine 
herausgehobene Rolle zukommt, da der Luftraum &#252;ber genau diese Eigenschaften verf&#252;gt. Dar&#252;ber hinaus sind
aber auch der Weltraum, Meere (&#252;ber, aber vor allem auch unter der Wasseroberfl&#228;che) sowie strukturarme
Landschaften wie W&#252;sten f&#252;r einen AWS-Einsatz besonders geeignet. Einsatzr&#228;ume wie St&#228;dte oder auch
solche innerhalb von Geb&#228;uden sind hingegen wesentlich komplexer strukturiert, sodass es f&#252;r Roboter dort
erheblich schwieriger ist, sich zielgerichtet und effizient fortzubewegen und Missionen durchzuf&#252;hren. Dennoch
sind auch urbane Einsatzszenarien Gegenstand von Forschung und strategischen &#220;berlegungen (U.S. Army
2017b, S. 6). Dass die verschiedenen Terrains nicht voneinander getrennt zu betrachten sind, zeigt das Multi-
Domain-Battle-Konzept, das von den US-Streitkr&#228;ften entworfen wurde, um milit&#228;rische Schlagkraft in alle
Dom&#228;nen projizieren zu k&#246;nnen (U.S. Army 2017a u. 2017b, S. i. ff.).
Im Folgenden werden potenzielle milit&#228;rische Einsatzformen zuk&#252;nftiger AWS &#8211; ohne Anspruch auf
Vollst&#228;ndigkeit &#8211; anhand von Einsatzbeispielen erl&#228;utert.
Kampfmissionen
Im Luft-zu-Luft-Kampf werden Loyal-Wingman-Konzepte diskutiert. Dabei handelt es sich um autonome
unbemannte Systeme, die im Verbund mit einem bemannten Flugzeug operieren und dessen Kampfkraft bzw.
&#220;berlebensf&#228;higkeit steigern. AWS k&#246;nnten als loyaler Fl&#252;gelmann beispielsweise als Container f&#252;r zus&#228;tzliche
Bewaffnung bzw. Munition oder als Sensortr&#228;ger f&#252;r Aufkl&#228;rungsmissionen dienen. Au&#223;erdem k&#246;nnten sie
feindliches Feuer auf sich ziehen, um das bemannte Flugzeug zu sch&#252;tzen (Scharre 2014b, S. 15; U.S. Air Force
2016, S. 45).
Im Luft-zu-Boden-Kampf k&#246;nnten AWS die feindliche Luftabwehr angreifen (DOD 2013, S. 24).
Au&#223;erdem w&#228;ren Eins&#228;tze m&#246;glich, in denen autonome Luft- oder Bodensysteme unbemerkt und ausdauernd, alleine
oder in Teams tief im feindlichen Gebiet hochwertige Ziele, z. B. wichtige Personen oder Geb&#228;ude, angreifen
(DOD 2013, S. 24). Autonome Bodensysteme k&#246;nnten Infanterietruppen dabei unterst&#252;tzen, die feindliche
Verteidigung zu durchbrechen, oder sie k&#246;nnten bei potenziellen Nahk&#228;mpfen vorausgeschickt werden (U.S. Army
2017b, S. 10).
Elektronische Angriffe
AWS k&#246;nnten elektronische Angriffe unterst&#252;tzen, die ein Ziel immobil oder kampfunf&#228;hig machen, anstatt es
mit physischer Gewalt auszuschalten (siehe z. B. Northrop Grumman o. J.). M&#246;gliche elektronische
Angriffsformen sind das Blockieren von Signalen (&#187;jamming&#171;) oder die &#220;berforderung der feindlichen
Kommunikationsnetze durch elektronische Angriffe in gro&#223;er Zahl (U.S. Air Force 2016, S. 8). Die UAVs &#187;Reaper&#171; haben
bereits ihre F&#228;higkeit zum elektronischen Angriff unter Beweis gestellt (Scharre 2014a, S. 28). Auch f&#252;r
autonome UUVs wird erwartet, dass elektronische Angriffsman&#246;ver in naher Zukunft zu deren F&#228;higkeitsspektrum
geh&#246;ren wird (Chief of Naval Operations 2016, S. 5, 9).
Verdeckte Operationen
F&#252;r verdeckte oder geheime Operationen w&#252;rden sich (kleine) autonome Roboter besonders eignen, da sie
weniger Spuren hinterlassen und unauff&#228;lliger als gro&#223;e bemannte Plattformen sind. Dadurch k&#246;nnten sie leichter
unentdeckt in feindliches Territorium eindringen. Wenn sie zudem aus kommerziell erh&#228;ltlichen Komponenten
bestehen und keine identifizierbaren Markierungen aufweisen, k&#246;nnte eine Regierung ihre Verantwortlichkeit 
abstreiten, wenn die Plattform in die H&#228;nde des Gegners f&#228;llt (&#187;plausible deniability&#171;) (Scharre 2014a, S. 11). 
Im Umkehrschluss ist dies f&#252;r die angegriffene betroffene Seite problematisch, denn wenn ein Angriff keinem
Verantwortlichen nachweisbar zugeordnet werden kann, sind Gegenma&#223;nahmen schwer zu begr&#252;nden und
durchzuf&#252;hren.
Nachrichtengewinnung, &#220;berwachung, Aufkl&#228;rung
Nachrichtengewinnung, &#220;berwachung und Aufkl&#228;rung (&#187;intelligence&#171;, &#187;surveillance&#171;, &#187;reconnaissance&#171; &#8211;
ISR) ist bereits heute ein routinem&#228;&#223;iges Einsatzfeld f&#252;r unbemannte Plattformen. Indem sie &#252;ber lange
Zeitr&#228;ume und weite Strecken die Umgebung erkunden, tragen sie zu einem besseren Situationsbewusstsein von 
Truppen bei (U.S. Army 2017b, S. 5 ff.). Es ist wahrscheinlich, dass Aufkl&#228;rungsdrohnen mit AWS synchron
operieren k&#246;nnen werden. Als Sp&#228;her w&#252;rden sich auch hier kleine Systeme besonders eignen, u. a. weil sie gut 
portabel sind und Soldaten sie zu Eins&#228;tzen mitnehmen k&#246;nnen (Scharre 2014a, S. 11). Schw&#228;rme unbemannter
Plattformen w&#252;rden qualitativ hochwertige Aufkl&#228;rungsresultate liefern, da sie Ziele von verschiedenen
Blickwinkeln beobachten k&#246;nnen (U.S. Air Force 2016, S. 43 f.). Au&#223;erdem k&#246;nnten sehr hoch fliegende und
ausdauernde unbemannte Plattformen &#228;hnlich wie Satelliten Kommunikations- und Navigationsfunktionen
&#252;bernehmen (&#187;pseudolites&#171;) (Scharre 2014a, S. 19).
T&#228;uschungsman&#246;ver
T&#228;uschungsman&#246;ver sind ein weiterer Missionstyp, f&#252;r die AWS pr&#228;destiniert sein k&#246;nnten. Diese sind oft auf
einen kalkulierten Verlust oder die Besch&#228;digung der beteiligten Plattformen ausgelegt. Darum sind kleine und
g&#252;nstige Plattformen, wie sie z. B. im LOCUST-Programm (Atherton 2018) entwickelt werden, hierf&#252;r
attraktiver als gro&#223;e, teure Systeme. Als eine Art Lockvogel k&#246;nnten Kleinplattformen einen Feind ablenken und 
seine Truppen in eine falsche Richtung f&#252;hren oder ihn zum Feuern provozieren und so seine Ressourcen (z. B. 
Munition) ersch&#246;pfen (Scharre 2014b, S. 14).
Sicherung milit&#228;rischer St&#252;tzpunkte
Zur Sicherung von milit&#228;rischen St&#252;tzpunkten sind autonome Systeme unterschiedlicher Art denkbar.
Hochautomatisierte (von autonomen oft nur schwer abgrenzbare; Kap. 2.2) Nahbereichsverteidigungssysteme sichern
bereits heute kritische Standorte, indem sie ohne menschliche Intervention reaktiv auf sich n&#228;hernde Ziele
schie&#223;en k&#246;nnen (beispielsweise &#187;C-RAM&#171;; UNIDIR 2014a, S. 5). Durch verbesserte Sensoren und zunehmende
Autonomie k&#246;nnte die Effizienz dieser Systeme erheblich gesteigert werden. Staaten wie Israel setzen bereits
jetzt ferngesteuerte Bodenfahrzeuge zur Sicherung von Gebietsabschnitten ein (Kap. 4.1.2). Autonome
maritime Waffensysteme k&#246;nnen au&#223;erdem zur Sicherung von H&#228;fen und kritischen Wasserwegen eingesetzt
werden (DOD 2013, S. 88).
Weitere Einsatzbeispiele
Zus&#228;tzliche Operationsformen f&#252;r unbemannte Boden- oder Luftsysteme, die auch f&#252;r autonome Plattformen
denkbar w&#228;ren, sind die nichtletale Kontrolle und Lenkung von Menschenmengen (DOD 2013, S. 24) und
diverse logistische Aufgaben (Bornstein 2015, S. Folie 6). Da unbemannte Luft- und Bodensysteme bereits heute
erfolgreich im Umgang mit improvisierten Sprengk&#246;rpern (IEDs) verwendet werden, sollen zuk&#252;nftig
autonome maritime Systeme, sowohl Schiffe als auch U-Boote, nach Unterwasserminen suchen, sie neutralisieren
und ggf. auch selbst legen (Chief of Naval Operations 2016, S. 4).60 Ferner k&#246;nnten unbemannte U-Boote der
strategischen nuklearen Abschreckung dienen (Ballesteros 2018; Bitzinger/Leah 2016).
5.3.2 Denkbare milit&#228;rische Einsatzszenarien f&#252;r AWS
Auf Basis der bisherigen Betrachtungen zu m&#246;glichen Einsatzumgebungen und Operationsformen zuk&#252;nftiger
AWS und unter Ber&#252;cksichtigung der &#8211; zum jetzigen Zeitpunkt noch sp&#228;rlichen &#8211; &#246;ffentlich verf&#252;gbaren
Informationen zu zuk&#252;nftigen Einsatzszenarien (z. B. Chief of Naval Operations 2016, S. 4 f.; DOD 2013, S. 12 ff.;
60 Das unbemannte Antiminenmodul (&#187;mine countermeasures module&#171; &#8211; MCM) soll ab 2021 einsatzbereit sein (The Maritime
Executive 2019).
iPRAW 2017a, S. 14 f.; U.S. Air Force 2016, S. 9 f.) werden im Folgenden einige exemplarische Szenarien
entwickelt und andiskutiert, in denen der Einsatz von AWS zuk&#252;nftig denkbar w&#228;re.
Maritime Eins&#228;tze
Maritime AWS wie UUVs und USVs k&#246;nnten sich &#252;ber lange Zeitr&#228;ume hinweg in oder auf dem offenen Meer
bewegen. Da sie keine Mannschaft an Bord haben, w&#228;re ihre Ausdauer erheblich gr&#246;&#223;er als die bemannter
Systeme und da sie keine kontinuierliche Steuerungs- oder Kommunikationsverbindung brauchen, k&#246;nnten sie
eigenst&#228;ndig in abgelegenen Gegenden operieren. Insbesondere UUVs k&#246;nnten vermehrt verwendet werden,
denn sie sind tief unter der Wasseroberfl&#228;che nur sehr schwer zu detektieren und nicht den oft unwirtlichen
Wetter- und Umweltbedingungen &#252;ber der Wasseroberfl&#228;che ausgesetzt. Somit sind sie im Einsatz robuster als
USVs. In Anbetracht weiter Strecken, die global operierende UUVs im Meer zur&#252;cklegen m&#252;ssten, k&#246;nnten
auch extra gro&#223;e unbemannte Unterwasserfahrzeuge (&#187;extra-large unmanned underwater vehicles&#171; &#8211; XLUUVs)
(siehe z. B. Baker 2019) eingesetzt werden, die eine &#228;u&#223;erst lange Ausdauer haben &#8211; potenziell sogar mehrere 
Monate. Durch die Ozeane streifende UUVs k&#246;nnten bei Bedarf zur Unterst&#252;tzung von Fregatten und
Tr&#228;gerverb&#228;nden hinzugerufen werden. Sie k&#246;nnten ebenfalls andere maritime Plattformen beschatten, die ihnen
aufgrund ihrer Operationsparameter verd&#228;chtig erscheinen, und deren Aktivit&#228;ten ausspionieren. Weiterhin ist
denkbar, dass sie z. B. f&#252;r Hunter-Killer-Missionen eingesetzt werden und selbstst&#228;ndig feindliche U-Boote
oder Kriegsschiffe aufsp&#252;ren, identifizieren und durch abgefeuerte Torpedos zerst&#246;ren. Da die Kommunikation
zwischen bzw. mit Objekten unter Wasser sehr schwierig ist, w&#228;re die kontinuierliche Aufsicht und Kontrolle
ebenso wie die Koordination von Schw&#228;rmen von AWS erheblich erschwert. Es ist absehbar, dass Gegner
ihrerseits mit verst&#228;rkten Anstrengungen zur Detektion von autonomen USVs und UUVs reagieren w&#252;rden,
beispielsweise durch das Ausbringen von Sensornetzwerken.
Einsatz in A2/AD-Gebieten
Bisher operieren unbemannte Systeme vor allem in offenen, also durch eigene oder befreundete Kr&#228;fte
hinreichend gut kontrollierten, Gebieten (z. B. durch Luft&#252;berlegenheit). Hier werden unbemannte Systeme f&#252;r
Aufkl&#228;rungs-, &#220;berwachungs- und Erkundungsmissionen sowie zur Logistikunterst&#252;tzung breit genutzt. Auch f&#252;r
offensive Operationen sind bewaffnete UCAVs im Einsatz, um hochwertige Ziele (z. B. Infrastruktur,
milit&#228;rische Objekte oder Personen) direkt anzugreifen.
Seit einigen Jahren spielen auch geschlossene Gebiete, bei denen es eine starke Luftverteidigung bzw.
anderweitige Abwehrma&#223;nahmen gibt (&#187;anti access and area denial&#171;), in Bezug auf den Einsatz von
unbemannten Systemen eine immer gr&#246;&#223;ere Rolle (DSB 2016, S. 61; U.S. Air Force 2014, S. 3). Es handelt sich hierbei
h&#228;ufig um Territorien, deren Gebietshoheit umstritten ist bzw. bei denen starke Verteidigungskr&#228;fte den Zugang
verwehren oder hemmen. Da dort Kommunikationsverbindungen vom Gegner gest&#246;rt oder aber anhand der
elektromagnetischen Signale die eigene Position offenbart werden k&#246;nnte, w&#252;rde sich hier der Einsatz von AWS
anbieten. Entsprechend genie&#223;t die Entwicklung dieser F&#228;higkeit eine hohe Priorit&#228;t in gegenw&#228;rtigen
strategischen &#220;berlegungen (DOD 2013, S. 67). Das DSB (2016, S. 61) beschreibt A2/AD-Gegenma&#223;nahmen sogar
als &#187;ein Paradebeispiel einer Mission, die durch autonome Systeme verbessert werden k&#246;nnte&#171;.
Ein m&#246;gliches Szenario, um den milit&#228;rischen Nutzen des Einsatzes von AWS in A2/AD-Umgebungen zu
illustrieren, diskutiert Scharre (2014a, S. 19 ff.): Kampfdrohnen, die von Flugzeugtr&#228;gern aus gestartet werden,
k&#246;nnten mit ihrer Reichweite von bis zu 1.500 Seemeilen (2.778 km) Landziele angreifen, ohne dass der
Flugzeugtr&#228;ger in die Reichweite gegnerischer Antischiffsraketen mit ihrer aktuellen Reichweite von bis zu 810
Seemeilen (1.500 km) geraten w&#252;rde (Abb. 5.2). Als Gegenreaktion der verteidigenden Seite w&#228;re hier mit der
intensiven Entwicklung und Verbreitung von Antischiffsraketen mit gr&#246;&#223;erer Reichweite zu rechnen, die es
erlauben, potenzielle Angriffe bereits im Vorfeld und in gr&#246;&#223;erer Entfernung zu vereiteln.
Ein A2/AD-Einsatz ist aber weitaus anspruchsvoller als Eins&#228;tze in einer einfachen, offenen Umgebung.
Die Verteidigungskapazit&#228;ten von aktuellen UCAVs in umk&#228;mpften Luftr&#228;umen sind unzureichend. In diesen
Umgebungen sind sie nicht sehr widerstandsf&#228;hig, zudem sind sie aufgrund ihrer Datenverbindung aufsp&#252;rbar
und manipulierbar. Um potenziellen Angriffen zu entgehen, fliegen sie bei Nacht oder sehr hoch. Weder haben
existierende Systeme nennenswerte Tarnkappenf&#228;higkeiten (bei neuen Modellen wie z. B. der &#187;Taranis&#171; soll
sich dies &#228;ndern; Kap. 4.2.2) noch k&#246;nnen sie feindliche Angriffe ausman&#246;vrieren oder das Feuer auf schnelle
mobile Fahrzeuge er&#246;ffnen. Daher w&#252;rde es z. B. ausreichen, in einem Hubschrauber neben einer UCAV
herzufliegen und von dort auf sie zu schie&#223;en (Williams 2017).
Falls AWS eigenst&#228;ndig agieren k&#246;nnten, ohne eine Kommunikationsverbindung zu unterhalten, w&#228;ren
sie sehr viel schwerer detektierbar. Flugzeugtr&#228;ger k&#246;nnten autonome Luftfahrzeuge nahe an das Ziel
heranbringen, von wo sie dann ins feindliche Gebiet eindringen k&#246;nnten. Welche Aufgaben sie &#252;bernehmen k&#246;nnen,
h&#228;ngt wesentlich von der Plattformgr&#246;&#223;e ab, da diese mit der erreichbaren Ausdauer, Nutzlast (z. B. Waffenmix)
und Geschwindigkeit korreliert. Denkbar w&#228;re, dass AWS einzeln oder als Schwarm ISR-Aufgaben erledigen 
oder Ziele wie Geb&#228;ude, Infrastruktur oder Menschengruppen angreifen, als unabh&#228;ngige Mission oder auch
als Vorhut f&#252;r eine Intervention weiterer Einsatzkr&#228;fte. Weiterhin k&#246;nnten Schw&#228;rme die feindliche Luftabwehr
ablenken oder &#252;berlasten. Falls sie jedoch entdeckt w&#252;rden, m&#252;ssten sie in der Lage sein, dem gegnerischen
Feuer schnell auszuweichen oder es zu erwidern. Sollten sie in die H&#228;nde des Gegners fallen, k&#246;nnte der
betreffende Staat die Verantwortung abstreiten.
Abb. 5.2 Kampfradien bemannter und unbemannter Flugzeuge
810 nm DF-21D Missile 
1.500 nm N-UCAS 
550 nm 
F/A-18E/F 
650 nm F-35C 
500 nm F35B 
nm: nautische Meile; &#187;DF-21D Missile&#171;: Antischiffsrakete; N-UCAS (&#187;naval uninhabited
combat aircraft&#171;): unbemanntes Kampfflugzeug der Marine; &#187;F/A-18E/F&#171;, &#187;F-35B/C&#171;: bemannte
Kampfflugzeuge
Quelle: Scharre 2014a, S. 20
Spezialeins&#228;tze
F&#252;r besondere oder verdeckte Operationen w&#252;rden sich eher kleinere autonome Boden-, See- oder Luftsysteme
eignen. Zudem k&#246;nnten AWS auch in Zusammenarbeit mit menschlichen Soldaten ins Feld geschickt werden
(MUM-T). Da sich Spezialeins&#228;tze h&#228;ufig auf hochrangige Ziele richten (ob einzelne Personen oder Gruppen,
z. B. die F&#252;hrungselite terroristischer Organisationen), ihnen eine intensive Vorbereitungsphase vorgeschaltet
ist und die Missionserf&#252;llung eine sehr hohe Priorit&#228;t hat, w&#228;re vermutlich das Risiko zu hoch, sich
ausschlie&#223;lich auf AWS zu verlassen. Stattdessen k&#246;nnten Soldaten kleine, m&#246;glicherweise nur insektengro&#223;e autonome
UGVs oder UAVs mit auf ihre Mission nehmen, die mit Miniaturkameras und Mikrosensoren ausgestattet sind
(iPRAW 2017a, S. 14). Da sie nicht nur unauff&#228;llig, sondern auch agil sind, k&#246;nnten die AWS unter der
Anleitung eines Soldaten eigenst&#228;ndig die Umgebung aussp&#228;hen oder sichern und dabei auch als Schwarm operieren,
wenn Schnelligkeit oder mehrere Blickwinkel gew&#252;nscht werden (U.S. Air Force 2016, S. 43). Auf diese Weise 
k&#246;nnten einzelne Gegner auch gezielt ausgeschaltet werden. Derartige Eins&#228;tze k&#246;nnten auch in urbanem
Gel&#228;nde stattfinden, wo AWS z. B. &#187;kritische Informationen zur Position feindlicher K&#228;mpfer und den Status von 
R&#228;umen, Geb&#228;uden, und Nachbarschaften f&#252;r den Task Force Kommandeur&#171; aussp&#228;hen, der darauf aufbauend
taktische Entscheidungen trifft (iPRAW 2017a, S. 14). Es ist davon auszugehen, dass AWS hier sowohl zu
Aufkl&#228;rungs- als auch zu Kampfzwecken Verwendung finden k&#246;nnten.
AWS-AWS-Kampf
Wenn ein Staat von einem Gegner den Kampfeinsatz von AWS erwartet, w&#228;re eine plausible Gegenma&#223;nahme,
ebenfalls AWS ins Feld zu schicken, wenn diese zur Verf&#252;gung stehen. In einem symmetrischen Szenario mit 
Antagonisten mit vergleichbaren technologischen F&#228;higkeiten w&#228;ren zuk&#252;nftige Eins&#228;tze, in denen sich AWS
in einer Kampfsituation gegen&#252;berstehen, nicht nur denkbar, sondern sogar naheliegend. F&#252;r zeitkritische
duellartige Situationen w&#228;ren AWS aufgrund der m&#246;glichen schnellen Man&#246;vrierf&#228;higkeit (die bei bemannten
Flugzeugen durch die maximale Beschleunigung, die Menschen aushalten k&#246;nnen, begrenzt ist),61 der schnellen 
Reaktionszeit sowie Entscheidungsfindung pr&#228;destiniert. Im Falle von Luft-zu-Luft-K&#228;mpfen haben KI-
Systeme zumindest in Simulationen bereits unter Beweis gestellt, dass sie menschlichen Piloten &#252;berlegen sein
k&#246;nnen (Ernest et al. 2016; Reilly 2016). Boden-zu-Boden-K&#228;mpfe stehen aufgrund der Schwierigkeiten des
Terrains derzeit noch nicht im Fokus strategischer &#220;berlegungen.
Damit w&#228;ren Menschen in das direkte Kampfgeschehen nicht mehr involviert. Dem Argument, dass somit 
auch keine Menschen zu Schaden kommen k&#246;nnen, stehen allerdings einige Bedenken gegen&#252;ber. Zum einen
w&#252;rde sich das Gefechtstempo ggf. so sehr erh&#246;hen und quasiautomatisch ablaufen, dass eine Kontrolle oder
ein Eingreifen durch menschliche Bediener nicht mehr m&#246;glich w&#228;ren. Wenn der Kampf nicht in einem
isolierten Teil des Schlachtfeldes stattfindet, w&#252;rde diese rasante Gefechtsgeschwindigkeit und m&#246;glicherweise hohe
Letalit&#228;t m&#246;glicherweise anwesende Menschen gef&#228;hrden (Neuneck 2014). Dar&#252;ber hinaus best&#252;nde das
Risiko, dass automatisiert durch Aktion und schnelle Reaktion sich Konflikte aufschaukeln oder gar erst ausgel&#246;st
werden (Kap. 6). Wie KI- bzw. softwaregest&#252;tzte Systeme unterschiedlicher Herkunft interagieren w&#252;rden, ist 
kaum vorhersehbar und im Vorfeld auch nicht zu testen.
Nuklearszenario
Grunds&#228;tzlich ist nicht auszuschlie&#223;en, dass AWS zuk&#252;nftig mit Nuklearwaffen best&#252;ckt werden k&#246;nnten.
Szenarien, in denen unbemannte Langstreckensysteme mit nuklearer Bewaffnung eingesetzt werden, werden in
Strategiezirkeln bereits durchgespielt (Bitzinger/Leah 2016). So sind beispielsweise Raketen, Marschflugk&#246;rper
oder Hyperschalltr&#228;gersysteme mit nuklearen Sprengk&#246;pfen denkbar, die im Endanflug autonom agieren.
Autonome, durch Kernreaktoren angetriebene und mit Nuklearwaffen best&#252;ckte UUVs k&#246;nnten mehrere Monate
in den Tiefen des Meeres &#252;ber den Globus verteilt versteckt ausharren, bis ein Funkbefehl sie aktiviert. Ohne
eine Besatzung, die nach gewisser Zeit an Land gehen m&#252;sste, z. B. um Proviant aufzunehmen, und ohne
kontinuierliche Datenverbindung zu einer Milit&#228;rbasis w&#228;ren sie noch schwerer auffindbar als U-Boote mit
menschlicher Besatzung. Bei einem potenziellen nuklearen Erstschlag auf das Heimatland k&#246;nnten sie dann aus sicherer
Entfernung einen Zweitschlag durchf&#252;hren. Insofern k&#246;nnten autonome Nuklear-UUVs zur strategischen
Abschreckung eingesetzt werden. Nach Auffassung der USA ist Russland im Begriff, ein solches autonomes
nuklear bewaffnetes Unterwasserfahrzeug zu entwickeln und zu stationieren (Kap. 4.1.3).62 
Autonome UCAVs k&#246;nnten ebenfalls mit nuklearen Waffen best&#252;ckt und bei Bedarf eingesetzt werden.
Wenn sie eigenst&#228;ndig Objekten oder auch bekannten Luft&#252;berwachungsst&#252;tzpunkten ausweichen k&#246;nnten und 
durch Tarnkappenf&#228;higkeiten schwerer durch Verteidigungsradare erkennbar w&#228;ren, w&#228;re dies ein
nennenswerter Vorteil gegen&#252;ber bemannten oder ferngesteuerten Flugzeugen. Gleichzeitig k&#246;nnten autonome nukleare
Waffensysteme die Reaktionszeiten f&#252;r Vergeltungsschl&#228;ge nach einem nuklearen Erstschlag durch den Feind
verk&#252;rzen, was einerseits der Abschreckung dienen, andererseits die M&#246;glichkeiten zur &#220;berpr&#252;fung der
Situationslage durch den Menschen einschr&#228;nken k&#246;nnte (Altmann/Sauer 2017, S. 128 ff.). Aber auch das nicht 
geringe Unfallrisiko, m&#246;gliches Fehlverhalten oder Gegenma&#223;nahmen und die ohnehin hohe Anzahl von
Nuklearwaffen sprechen gegen die Entwicklung nuklearwaffenf&#228;higer AWS.
61 Diese betr&#228;gt etwa die 9-fache Erdbeschleunigung (Wikipedia 2003c).
62 Im Original: &#187;Russia is also developing at least two new intercontinental range systems, a hypersonic glide vehicle, and a new
intercontinental, nuclear-armed, nuclear-powered, undersea autonomous torpedo.&#171; (DOD 2018a, S.9)
5.3.3 Zwischenfazit
Die besonderen Systemf&#228;higkeiten zuk&#252;nftiger AWS gr&#252;nden sich zum einen auf die physische Abwesenheit 
eines Bedieners, die u. a. kleinere und agilere Plattformen erm&#246;glicht, die in Gebieten operieren k&#246;nnen, die f&#252;r
Menschen nicht oder nur schwer zug&#228;nglich sind. Zum anderen werden die F&#228;higkeiten durch die M&#246;glichkeit
bestimmt, autonom zu operieren, was schnellere und bessere Entscheidungen in zeitkritischen Situationen
erm&#246;glichen soll sowie neue Optionen zur Koordination in einer Gruppe (bzw. einem Schwarm) von AWS oder
aber zur Kooperation zwischen einem oder mehreren AWS und Menschen (&#187;manned-unmanned teaming&#171;)
er&#246;ffnet.
Aufgrund ihrer potenziellen F&#228;higkeiten und daraus resultierender milit&#228;rischer Vorteile werden sich AWS
insbesondere f&#252;r Einsatzszenarien eignen, die f&#252;r menschliche Bediener wegen der Umgebungsbedingungen 
problematisch oder zu gef&#228;hrlich sind (z. B. Unterwasserkriegsf&#252;hrung, durch starke Kr&#228;fte verteidigtes Terrain
oder A2/AD-R&#228;ume), die sehr schnelle Reaktionszeiten oder eine hohe Man&#246;vrierf&#228;higkeit erfordern (z. B. 
Luftkampf) oder aber in einer dynamischen Umgebung ohne eine permanente Kommunikationsverbindung 
stattfinden (verdeckte Operationen hinter den feindlichen Linien oder bei St&#246;rung der
Kommunikationsverbindung). Da AWS perspektivisch vermutlich f&#252;r zunehmend unterschiedliche Operationsarten eingesetzt werden
k&#246;nnen, wird durch sie auch die Aufgabenverteilung zwischen Mensch und Maschine neu definiert werden.
Zum gegenw&#228;rtigen Zeitpunkt sind jedoch sowohl die technischen Eigenschaften als auch die
Verf&#252;gbarkeit zuk&#252;nftiger AWS noch spekulativ, sodass die hier entwickelten m&#246;glichen Einsatzszenarien lediglich die
Umrisse eines m&#246;glichen Portfolios skizzieren. Dessen konkrete Ausgestaltung h&#228;ngt von einem komplexen
Zusammenspiel technologischer Entwicklungen und der Wandlung der Streitkr&#228;ftestrukturen ab, die sich im
Zuge der sukzessiven Integration von Robotik und Autonomie in Doktrinen, Strategien sowie Taktiken,
Techniken und Prozeduren abzeichnet.
Ein Feld, das bisher noch wenig beleuchtet ist, das aber absehbar mit einer zunehmenden Verbreitung von
AWS erheblich an Bedeutung gewinnen wird, betrifft m&#246;gliche technologische oder operative
Gegenma&#223;nahmen, die von Antagonisten gegen AWS getroffen werden k&#246;nnten. Die Sensoren und computergest&#252;tzten
kognitiven F&#228;higkeiten von AWS werden ganz eigene Schwachstellen aufweisen, die von entsprechend innovativen 
Antagonisten ausgenutzt werden k&#246;nnten. Hierzu geh&#246;ren sowohl M&#246;glichkeiten der elektronischen
Manipulation (&#187;jamming&#171;, &#187;spoofing&#171;, &#187;hacking&#171;). Aber auch eher im Lowtechbereich angesiedelte Ma&#223;nahmen
k&#246;nnten ggf. sehr effektiv sein, beispielsweise Fangnetze gegen kleine fliegende AWS oder aber aufblasbare
Attrappen oder thermische Quellen, um hitzesuchende Sensoren zu t&#228;uschen (Neuneck 2014, S. 68 ff.).
Kasten 5.1 Die milit&#228;rische KI-Strategie der US-Regierung
Die USA messen der Anwendung k&#252;nstlicher Intelligenz eine herausragende Bedeutung bei der
Modernisierung ihrer Streitkr&#228;fte bei. In ihrer 2018 vom Verteidigungsministerium formulierten &#187;Nationalen
Verteidigungsstrategie&#171; hei&#223;t es: &#187;Das Ministerium wird umfangreich investieren in die milit&#228;rische Anwendung von
Autonomie, k&#252;nstlicher Intelligenz und Maschinenlernen, einschlie&#223;lich der schnellen Anwendung von im
kommerziellen Sektor erzielten (technologischen) Durchbr&#252;che, um milit&#228;rische Wettbewerbsvorteile zu
generieren&#171;63 (DOD 2018b, S. 7).
Das Verteidigungsministerium hat diese Ank&#252;ndigung untermauert und konkretisiert durch die
Ausarbeitung einer KI-Strategie (DOD 2019). Diese wurde am 12. Februar 2019 im Vorfeld der M&#252;nchner
Sicherheitskonferenz &#246;ffentlich vorgestellt. Unterstrichen wird die hohe Priorit&#228;t, die die US-Regierung der KI
beimisst, durch einen Exekutiverlass zur F&#246;rderung und Priorisierung von KI, den der Pr&#228;sident am Tag vor
der Pr&#228;sentation der milit&#228;rischen KI-Strategie unterzeichnete (U.S. President 2019).64 
Als Hauptziele der Einf&#252;hrung von KI im Verteidigungssektor werden in dem Strategiepapier
Verbesserungen bei der Unterst&#252;tzung und dem Schutz der eigenen Soldaten bzw. der Bev&#246;lkerung genannt sowie
Vorteile bei der Verteidigung von Verb&#252;ndeten und Partnern. Zudem k&#246;nnten milit&#228;rische Operationen
63 Im Original: &#187;The Department will invest broadly in military application of autonomy, artificial intelligence, and machine learning,
including rapid application of commercial breakthroughs, to gain competitive military advantages.&#171;
64 Dies erfolgte nach dem Redaktionsschluss des vorliegenden TAB-Berichts. Wegen der gro&#223;en Relevanz wurde dennoch dieser
Textteil aufgenommen. Es war allerdings nicht m&#246;glich, alle Kapitel eingehend auf m&#246;glichen &#196;nderungs- bzw. Erg&#228;nzungsbedarf
zu pr&#252;fen.
schneller durchgef&#252;hrt werden und w&#228;ren erschwinglicher.65 Als starkes Motiv wird der Wettbewerb
angef&#252;hrt, der weltweit um KI f&#252;r milit&#228;rische Zwecke entbrannt ist. Insbesondere China und Russland w&#252;rden
hier voranschreiten, auch bei Anwendungen, bei denen es fragw&#252;rdig ist, ob sie mit internationalen Normen
und den Menschenrechten vereinbar sind. Diese Entwicklungen drohen den technologischen und
operationellen Vorsprung der USA zu gef&#228;hrden und die freie und offene internationale Ordnung zu destabilisieren.66 
Die Kernelemente der Strategie sind unter f&#252;nf &#220;berschriften zusammengefasst (DOD 2019, S. 7 f.):
Bereitstellen von KI-getriebenen F&#228;higkeiten f&#252;r zentrale Missionen
Hierf&#252;r werden als Beispiele genannt die Verbesserung des Lagebewusstseins und der Entscheidungsfindung,
die Steigerung der Betriebssicherheit von Ausr&#252;stung, die Implementierung von vorausschauender
Instandhaltung und Beschaffung sowie die Straffung von Gesch&#228;ftsabl&#228;ufen. Es sollen KI-Systeme priorisiert
werden, die die F&#228;higkeiten des Personals erweitern, indem sie es von m&#252;hsamen kognitiven oder physischen
Aufgaben entlasten und neue Arbeitsweisen einf&#252;hren.
Ausweiten der Wirkungen von KI im gesamten Gesch&#228;ftsbereich des Verteidigungsministeriums durch
Schaffung einer gemeinsamen Grundlage, die zu dezentraler Entwicklung und experimenteller Erprobung bef&#228;higt
Es wird erwartet, dass insbesondere von der Praxis von Nutzern in vorderster Front Impulse f&#252;r die
Entwicklung transformativer KI-Anwendungen ausgehen und nicht (nur) aus zentralen Entwicklungsabteilungen. Es
soll eine gemeinsame Grundlage f&#252;r dezentrale Entwicklung geschaffen werden, die aus gemeinsam
genutzten Daten, Tools, Bezugssystemen sowie Standards in Cloud- und Edge-Diensten67 bestehen soll.
Gleichzeitig sollen bestehende Prozesse und Routinen durch Digitalisierung und Automatisierung f&#252;r die Einbindung
von KI vorbereitet werden, um deren Diffusion zu beschleunigen.
Kultivieren einer Belegschaft, die f&#252;hrend in KI ist
Um die KI-bezogenen Kenntnisse und F&#228;higkeiten des Personals auf das h&#246;chstm&#246;gliche Niveau zu heben,
soll umfassend in Ausbildung und Training investiert werden und gleichzeitig externes Know-how von
Weltklasseformat durch Rekrutierung und Bildung von Partnerschaften integriert werden.
Einbinden von kommerziellen, akademischen und internationalen Verb&#252;ndeten und Partnern
Partnerschaften entlang der gesamten Innovationskette sollen gepflegt und verst&#228;rkt werden, vor allem mit
dem kommerziellen Sektor, um dortige Fortschritte mit den Anforderungen des Verteidigungssektors zu
verkn&#252;pfen. Auch f&#252;r den Verteidigungsbereich eher un&#252;bliche Kooperationen sollen vorangetrieben werden,
insbesondere mit der globalen Open-Source-Community. Bei der Interaktion mit dem kommerziellen Sektor
soll die Defense Innovation Unit (DIU) eine zentrale, beschleunigende Rolle einnehmen (DOD 2019, S. 13).
Bei milit&#228;rischer Ethik und KI-Sicherheit die F&#252;hrung &#252;bernehmen
Die Formulierung eines Leitbildes von rechtlich zul&#228;ssiger und ethisch vertretbarer Entwicklung und
Nutzung von KI soll in f&#252;hrender Rolle vorangetrieben werden. Hierbei sollen die akademische Welt, die
Privatwirtschaft und die internationale Gemeinschaft einbezogen werden, um Ethik und Sicherheit der KI im
milit&#228;rischen Kontext zu verbessern. FuE von KI-Systemen sollen gest&#228;rkt werden, die robust, zuverl&#228;ssig
und sicher sind. Forschung zu Techniken, die eine besser erkl&#228;rbare KI (&#187;explainable AI&#171;; Kap. 3.3.3)
erzeugen, sollen gef&#246;rdert werden. Bei der Entwicklung von Methoden f&#252;r Tests, Bewertungen, Verifizierung und 
Validierung (&#187;test and evaluation, verification and validation&#171; &#8211; TEVV) soll Pionierarbeit geleistet werden.
65 Im Original: &#187;With the application of AI to defense, we have an opportunity to improve support for and protection of U.S. service
members, safeguard our citizens, defend our allies and partners, and improve the affordability and speed of our operations.&#171; (DOD 
2019, S.5)
66 Im Original: &#187;Other nations, particularly China and Russia, are making significant investments in AI for military purposes, including 
in applications that raise questions regarding international norms and human rights. These investments threaten to erode our
technological and operational advantages and destabilize the free and open international order.&#171; (DOD 2019, S.5)
67 Edge Computing bezeichnet im Gegensatz zum Cloud Computing die dezentrale Datenverarbeitung am Rand des Netzwerks
(Wikipedia 2017).
Es soll nach M&#246;glichkeiten Ausschau gehalten werden, KI einzusetzen, um unbeabsichtigtes Leid und
Kollateralsch&#228;den durch ein gesteigertes Situationsbewusstsein und eine verbesserte
Entscheidungsunterst&#252;tzung zu reduzieren. Um zu f&#246;rdern, dass die Entwicklung und der Einsatz von KI auch in anderen L&#228;ndern
in verantwortungsbewusster Weise geschehen, sollen Ziele, ethische Richtlinien und Sicherheitsverfahren
mit diesen geteilt werden (DOD 2019, S. 8, 15 f.).
Das Joint Artificial Intelligence Center
Die KI-Strategie wird institutionell unterf&#252;ttert durch die Gr&#252;ndung des JAIC, das als Brennpunkt f&#252;r ihre 
Umsetzung fungieren soll. Die Arbeit des JAIC soll die eher langfristig orientierte Grundlagenforschung und
Technologieentwicklung, die in bestehenden Organisationen wie der DARPA und weiteren vom
Verteidigungsministerium unterst&#252;tzten Forschungseinrichtungen beheimatet sind, durch kurzfristigere
umsetzungsorientierte FuE komplettieren. Als Schwerpunkte des JAIC werden genannt: Prototypen identifizieren und
zur Verf&#252;gung stellen, Wissenstransfer vorantreiben und Forschung mit dem praktischen Betrieb verzahnen,
Prototypen zur Praxisreife fortentwickeln, laufende Unterst&#252;tzung bieten (DOD 2019, S. 9 f.).
6 Sicherheitspolitische Implikationen von AWS
Aktuell ist ein Wettlauf im Gange, wer die Entwicklung und den Einsatz von KI in vielen Lebensbereichen
anf&#252;hrt. Wie der russische Pr&#228;sident Putin k&#252;rzlich ausf&#252;hrte: &#187;K&#252;nstliche Intelligenz ist die Zukunft, nicht nur
f&#252;r Russland, sondern f&#252;r die gesamte Menschheit. Sie ist mit gewaltigen M&#246;glichkeiten verbunden, aber auch
mit Gefahren, die schwer vorherzusehen sind. Wer auf diesem Feld f&#252;hrend ist, wird die Welt beherrschen.&#171;68 
Oder wie es in der k&#252;rzlich ver&#246;ffentlichten nationalen Verteidigungsstrategie der USA hei&#223;t: &#187;Neue
Technologien einschlie&#223;lich hochentwickelte Rechenverfahren, &#8250;Big Data&#8249;-Analysen, k&#252;nstliche Intelligenz,
Autonomie, Robotik, geb&#252;ndelte Energie, &#220;berschall- und Biotechnologie &#8211; genau diese Technologien stellen sicher,
dass wir in der Lage sein werden, die Kriege der Zukunft zu f&#252;hren und zu gewinnen&#171;.69 Einsch&#228;tzungen dieser
Art sowie die im Kapitel 5 skizzierten milit&#228;rischen Vorteile, die AWS versprechen, generieren einen hohen
Anreiz, die milit&#228;rische Nutzung von zunehmend autonomen Systemen voranzutreiben. In Staaten mit
verantwortlich handelnder F&#252;hrung wird dies zwar begrenzt durch die Schwelle, jenseits derer dies nicht mehr ethisch
zu rechtfertigen scheint. Diese Schwelle ist jedoch bis heute weder national noch im internationalen Kontext
klar definiert. Auch wenn dies durch r&#252;stungskontrollpolitische Vereinbarungen (Kap. 9) geleistet w&#252;rde,
best&#252;nde keine Gew&#228;hr, dass Regierungen mit weniger moralischen Skrupeln dieses als verbindliche rote Linie
akzeptieren w&#252;rden.
Um auf alle m&#246;glichen Entwicklungen vorbereitet zu sein, ist es daher essenziell, sich mit den
sicherheitspolitischen Implikationen zu befassen, die die Verbreitung und der m&#246;gliche Einsatz von AWS mit sich bringen
k&#246;nnen. Risiken f&#252;r die internationale Stabilit&#228;t und Sicherheit lassen sich grob in drei Dimensionen unterteilen
(Altmann/Sauer 2017; Scharre 2018):
Die erste ist verbunden mit der Gefahr von unkontrollierter Verbreitung und dem Triggern von
R&#252;stungswettl&#228;ufen. So w&#252;rde allein die begr&#252;ndete Vermutung eines Staates, dass ein anderer AWS mit neuartigen
F&#228;higkeiten anstrebt oder sogar bereits besitzt, intensive Entwicklungs- und Beschaffungsbem&#252;hungen
ausl&#246;sen, um nicht r&#252;stungstechnologisch ins Hintertreffen zu geraten. Diese Bem&#252;hungen w&#252;rden wohl wiederum
den ersten Staat veranlassen, seine Aktivit&#228;ten noch zu intensivieren, wodurch sich eine R&#252;stungsspirale
ausbilden und immer schneller drehen k&#246;nnte, die einerseits bei allen Beteiligten ein Bedrohungsgef&#252;hl erzeugen
und andererseits erhebliche &#246;konomische Ressourcen verschlingen w&#252;rde. Beide Effekte k&#246;nnten dazu
beitragen, Krisen zu erzeugen oder zu versch&#228;rfen. Auch wenn substaatliche Akteure (von Aufst&#228;ndischen bis
Terrorgruppen) sich neuartige AWS beschaffen oder glaubw&#252;rdig damit drohen k&#246;nnten, w&#228;ren schwerwiegende
sicherheitspolitische Konsequenzen zu bef&#252;rchten.
Die zweite Dimension bezieht sich darauf, dass die Eigenschaften von AWS dazu f&#252;hren k&#246;nnten, dass die
Hemmschwelle zum Einsatz von Waffengewalt sinken k&#246;nnte. So k&#246;nnte im Konfliktfall h&#228;ufiger
Waffengewalt eingesetzt werden und Krisen auf diese Weise schneller eskalieren. Dies gilt umso mehr, als der Raum f&#252;r
deeskalierend wirkende politische Initiativen durch die technologisch induzierte Beschleunigung des
Konfliktgeschehens tendenziell schrumpfen k&#246;nnte.
Die dritte Dimension bezeichnet die Gefahr, dass die neuartigen F&#228;higkeiten von AWS einen milit&#228;rischen
Erstschlag erm&#246;glichen k&#246;nnten, der einen der Kontrahenten seiner F&#228;higkeit zum Zur&#252;ckschlagen beraubt.
Selbst die begr&#252;ndete Vermutung, dass ein solches Erstschlagpotenzial aufgebaut werden k&#246;nnte, w&#252;rde in
h&#246;chstem Ma&#223;e destabilisierend wirken, insbesondere wenn es sich bei den Akteuren um Nuklearm&#228;chte
handelt.
Einige Aspekte der sicherheits- und stabilit&#228;tspolitischen Dimensionen werden im Folgenden eingehender
untersucht. Manche der Argumente sind in &#228;hnlicher Weise schon f&#252;r ferngesteuerte bzw. teilweise
automatisierte unbemannte Systeme zutreffend, andere beziehen sich expliziter auf die F&#228;higkeit zu autonomen
Operationen von AWS. Das Gutachten von (Alwardt et al. 2017) stellt eine wesentliche Grundlage f&#252;r das Folgende
dar.
68 Rede gehalten f&#252;r Sch&#252;ler anl&#228;sslich der Onlineveranstaltung &#187;Russland, Blick in die Zukunft&#171;. Im Original: &#187;&#1048;&#1089;&#1082;&#1091;&#1089;&#1089;&#1090;&#1074;&#1077;&#1085;&#1085;&#1099;&#1081; 
&#1080;&#1085;&#1090;&#1077;&#1083;&#1083;&#1077;&#1082;&#1090; &#8211; &#1073;&#1091;&#1076;&#1091;&#1097;&#1077;&#1077; &#1085;&#1077; &#1090;&#1086;&#1083;&#1100;&#1082;&#1086; &#1056;&#1086;&#1089;&#1089;&#1080;&#1080;, &#1101;&#1090;&#1086; &#1073;&#1091;&#1076;&#1091;&#1097;&#1077;&#1077; &#1074;&#1089;&#1077;&#1075;&#1086; &#1095;&#1077;&#1083;&#1086;&#1074;&#1077;&#1095;&#1077;&#1089;&#1090;&#1074;&#1072;. &#1047;&#1076;&#1077;&#1089;&#1100; &#1082;&#1086;&#1083;&#1086;&#1089;&#1089;&#1072;&#1083;&#1100;&#1085;&#1099;&#1077; &#1074;&#1086;&#1079;&#1084;&#1086;&#1078;&#1085;&#1086;&#1089;&#1090;&#1080; &#1080; &#1090;&#1088;&#1091;&#1076;&#1085;&#1086; 
&#1087;&#1088;&#1086;&#1075;&#1085;&#1086;&#1079;&#1080;&#1088;&#1091;&#1077;&#1084;&#1099;&#1077; &#1089;&#1077;&#1075;&#1086;&#1076;&#1085;&#1103; &#1091;&#1075;&#1088;&#1086;&#1079;&#1099;. &#1058;&#1086;&#1090;, &#1082;&#1090;&#1086; &#1089;&#1090;&#1072;&#1085;&#1077;&#1090; &#1083;&#1080;&#1076;&#1077;&#1088;&#1086;&#1084; &#1074; &#1101;&#1090;&#1086;&#1081; &#1089;&#1092;&#1077;&#1088;&#1077;, &#1073;&#1091;&#1076;&#1077;&#1090; &#1074;&#1083;&#1072;&#1089;&#1090;&#1077;&#1083;&#1080;&#1085;&#1086;&#1084; &#1084;&#1080;&#1088;&#1072;.&#171; (Jaroslawl 2017)
69 Im Original: &#187;New technologies include advanced computing, &#8250;big data&#8249; analytics, artificial intelligence, autonomy, robotics,
directed energy, hypersonics, and biotechnology &#8211; the very technologies that ensure we will be able to fight and win the wars of the
future.&#171; (DOD 2018b, S.3)
        
 
 
  
  
 
 
       
    
   
   
     
  
   
  
    
    
     
    
     
 
    
    
       
   
 
      
 
  
     
 
    
   
   
      
 
      
  
 
   
  
 
 
   
  
    
      
  
     
6.1
6.2
Mehr oder weniger Kriege?
Ob die Verf&#252;gbarkeit von AWS dazu f&#252;hrt, dass im Konfliktfall schneller zum Mittel der milit&#228;rischen Gewalt
gegriffen wird oder dass milit&#228;rische Auseinandersetzungen gewaltsamer gef&#252;hrt werden, ist derzeit eine
kontrovers diskutierte Frage.
Einige Argumente sprechen f&#252;r eine solche Entwicklung (Heyns 2013, S. 11 ff.): Der gr&#246;&#223;ere physische
und psychologische Abstand vom eigentlichen Kampfgeschehen, der durch AWS erm&#246;glicht wird, k&#246;nnte die 
Hemmschwelle, Gewalt einzusetzen, senken. Demokratisch gew&#228;hlte Regierungen stehen regelm&#228;&#223;ig unter
erheblichem Rechtfertigungsdruck, wenn eigene Soldaten gef&#228;hrdet oder get&#246;tet werden. Aufgrund des durch
AWS verringerten Risikos f&#252;r eigene Soldaten k&#246;nnte die &#214;ffentlichkeit einen Krieg nicht mehr als letzten
Ausweg bzw. als einschneidendes Ereignis von nationaler Bedeutung wahrnehmen, sondern diesen als einen
unter dem Primat diplomatischer und &#246;konomischer Abw&#228;gungen stehenden Normalfall akzeptieren. Auch
unterhalb der Schwelle ausgewachsener Kriege k&#246;nnen Milit&#228;reins&#228;tze zur Durchsetzung politischer Ziele
attraktiv sein und immer mehr zur Regel werden.
Andererseits k&#246;nnen einige Argumente gegen eine wesentliche Erh&#246;hung der Konfliktgefahr ins Feld
gef&#252;hrt werden: Jede Technologie und milit&#228;rische Praktik, die das Risiko f&#252;r eigene Soldaten bzw. die
Opferzahlen auf dem Schlachtfeld verringert, senkt nach der angef&#252;hrten Logik die Schwelle zum Einsatz milit&#228;rischer
Mittel. AWS w&#252;rden hier nichts grunds&#228;tzlich Neues bringen, sondern eine ohnehin vonstattengehende
Entwicklung lediglich graduell verst&#228;rken. Auch sind Situationen vorstellbar &#8211; beispielsweise, wenn Warlords die
Bev&#246;lkerung terrorisieren bzw. ethnische S&#228;uberungen durchf&#252;hren &#8211;, in denen ein schnellerer und
entschlossenerer Einsatz von Gewaltmitteln viel menschliches Leid verhindert. Generell verliert das Argument, dass
AWS zu einem h&#228;ufigeren Einsatz von Gewalt f&#252;hren, erheblich an Kraft, wenn man ein Szenario mit etwa 
gleich starken Kontrahenten betrachtet. Hier w&#252;rde f&#252;r die Seite, die AWS einsetzt, immer die Gefahr bestehen,
dass schmerzhafte Vergeltungsma&#223;nahmen des Gegners folgen bzw. im ung&#252;nstigsten Fall eine Eskalation zu
einem ausgewachsenen Krieg mit ungewissem Ausgang eintritt. Insbesondere f&#252;r die Nuklearwaffenstaaten
untereinander w&#252;rde dieses Kalk&#252;l stark gegen einen beabsichtigten und als lokal begrenzt intendierten Einsatz
von AWS sprechen.
Ver&#228;nderung der Kriegsf&#252;hrung
Neue Technologien haben historisch immer auch die Kriegsf&#252;hrung ver&#228;ndert oder gar revolutioniert, sowohl
in Form der zur Verf&#252;gung stehenden Wirkmittel als auch in der Art und Strategie ihres Einsatzes. Gegenw&#228;rtig
sind bereits einige Umw&#228;lzungen im Gange, die sich im Wechselspiel geopolitischer Entwicklungen,
strategischer &#220;berlegungen und von technologischem Fortschritt vollziehen.
So tritt die klassische Art von Krieg, in dem zwei souver&#228;ne Staaten direkt gegeneinander k&#228;mpfen, im
historischen Vergleich immer mehr hinter andere Formen der Austragung von Konflikten zur&#252;ck
(Pettersson/Eck 2018). AWS k&#246;nnen hier existierende Asymmetrien zwischen Staaten, die AWS besitzen, und
technologisch weniger fortgeschrittenen L&#228;ndern noch vergr&#246;&#223;ern (CCW 2016b, S. 3 Pkt. 19). Auch w&#252;rde der
bestehende Trend durch AWS verst&#228;rkt, dass das Risiko, im bewaffneten Konflikt get&#246;tet zu werden, f&#252;r das
Milit&#228;rpersonal technologisch fortgeschrittener L&#228;nder sinkt, f&#252;r andere Beteiligte, sowohl feindliche K&#228;mpfer als
auch Zivilisten, hingegen ansteigt (Shaw 2002).
Im Folgenden werden einige Aspekte, wie AWS zu Ver&#228;nderungen in der Kriegsf&#252;hrung beitragen
k&#246;nnen, n&#228;her betrachtet.
Digitalisierung der Kriegsf&#252;hrung
Die Digitalisierung der Kriegsf&#252;hrung, die mit einer zunehmenden Automatisierung und Vernetzung von
Waffensystemen einhergeht, ist erkl&#228;rtes Ziel der strategischen &#220;berlegungen in den Streitkr&#228;ften technologisch 
potenter Staaten. In den USA l&#228;uft dies unter dem Titel &#187;Network Centric Warfare&#171; (Wilson 2005), in
Deutschland unter &#187;Vernetzte Operationsf&#252;hrung&#171; (NetOpF&#252;) (BMVg 2004, S. 15). Dabei werden sukzessive Aufgaben
an Maschinen bzw. Programmsoftware abgegeben, die bisher von Menschen erledigt worden sind. So werden
z. B. aktuell Drohnen nicht mehr von einem Operateur direkt gesteuert, sondern dieser nimmt nur noch eine
&#252;bergeordnete Kontrollfunktion wahr. Im Rahmen netzwerkzentrierter Kriegsf&#252;hrung w&#228;ren AWS zur
Durchf&#252;hrung komplexer Aufgaben pr&#228;destiniert, die menschliche Operateure &#252;berfordern. Hierzu z&#228;hlen schnelle
Reaktionen auf sich ver&#228;ndernde Missionsparameter oder die Koordination bzw. Steuerung komplexer
Systemverb&#252;nde in Echtzeit. Au&#223;erdem k&#246;nnten AWS abseits von Gefechtsfeldern in einer dynamischen, vielleicht
auch zivilen Umgebung hinter den feindlichen Linien eingesetzt werden. Damit einhergehend droht dem
Menschen jedoch zusehends der Einblick in die zugrundeliegenden Entscheidungsprozesse und
Informationsgrundlagen verlorenzugehen, sodass er seiner Kontrollfunktion nicht mehr nachkommen kann.
Allerdings ist es keineswegs selbstverst&#228;ndlich, dass Streitkr&#228;fte, die auf der Grundlage von
hochorganisierten Kommando- und Kontrollstrukturen operieren, in der Lage sind, Systeme mit autonomen Funktionen
reibungslos in ihre Abl&#228;ufe zu integrieren. Beim Milit&#228;r handelt es sich kulturell zudem um eine eher
konservative Institution, die durchaus skeptisch gegen&#252;ber tiefgreifenden institutionellen Ver&#228;nderungen sein kann
(Rosen 1991).
Verdeckte Operationen
Wie der heutige Einsatz von UCAVs im Rahmen von asymmetrischer Kriegsf&#252;hrung und gezielten T&#246;tungen
(&#187;targeted killings&#171;) vor Augen f&#252;hrt, k&#246;nnen AWS sehr wirkungsvolle Instrumente f&#252;r verdeckte Operationen
oder geheimdienstliche Ma&#223;nahmen in Graubereichen und auch in zivilen Umgebungen sein. Damit w&#252;rde der
gegenw&#228;rtige Trend, dass die Grenzen zwischen zivilen und milit&#228;rischen R&#228;umen zunehmend verschwimmen,
noch verst&#228;rkt und einer weiteren Entgrenzung des Krieges Vorschub geleistet.
Gegenreaktionen
F&#252;r Staaten, denen der Zugriff auf AWS verwehrt bleibt, w&#228;re eine (radikale) Option die Anschaffung von
Massenvernichtungswaffen (MVW), z. B. von Nuklearwaffen, um gegen die milit&#228;rtechnische &#220;berlegenheit
von gro&#223;en Milit&#228;rm&#228;chten wie den USA oder China vorzugehen und etwaige Interventionen mit AWS
abzuschrecken (Neuneck 2014). Derartige Entwicklungen w&#252;rden sich auf die strategische Stabilit&#228;t auswirken. Es
ist jedoch fraglich, ob es wirklich realistischer ist, MVW statt AWS zu erwerben.
Eine weitere m&#246;gliche Reaktion ist, umfassend in die Entwicklung sowie Produktion von Anti-AWS-
Mitteln zu investieren. Eventuell w&#228;ren auch technologisch einfache Gegenma&#223;nahmen effektiv, beispielsweise 
St&#246;rsender, aufblasbare Panzerattrappen oder thermische Quellen, um hitzesuchende Sensoren zu t&#228;uschen.
Auch wenn die Soldaten durch entsprechendes Equipment gut vor AWS gesch&#252;tzt sind, trifft dies nicht
unbedingt auf die Zivilbev&#246;lkerung zu. Folglich k&#246;nnte es unter Umst&#228;nden zu h&#246;heren Kollateralsch&#228;den, d. h.
zivilen Opfern kommen (Neuneck 2014).
Antiterrorismuseins&#228;tze
In US-Strategiepapieren wird explizit darauf hingewiesen, dass Milit&#228;rsysteme mit &#187;zunehmend autonomen
Funktionen&#171; erforderlich w&#228;ren, um terroristische Gruppen weltweit zu verfolgen, zu &#252;berwachen und zu
bek&#228;mpfen (U.S. Air Force 2016, S. 10). UCAVs haben im Rahmen des &#187;Global War on Terror&#171; bereits immens
an taktischer Bedeutung gewonnen. Pr&#228;sident Obama bezeichnete sie im Mai 2013 als &#187;Allheilmittel&#171; gegen
Terrorismus (Zenko/Kreps 2014, S. 10). Derzeitige UCAVs eignen sich f&#252;r Antiterrorismuseins&#228;tze besonders
gut; sie operieren in R&#228;umen, in denen sie keiner nennenswerten Luftverteidigung ausgesetzt sind. Einige
Tendenzen, die heute bei der Nutzung von UCAVs in Antiterrorismusoperationen beobachtbar sind, k&#246;nnten sich 
bei AWS weiter verst&#228;rken, z. B. verleiten bereits heutige UCAV-Eins&#228;tze vielfach dazu, verd&#228;chtige Personen
in Kriegszonen und instabilen Gebieten zu t&#246;ten, anstatt sie gefangen zu nehmen (Horowitz et al. 2016, S. 20).
Innerstaatliche Eins&#228;tze
Prognosen zum innerstaatlichen Einsatz von AWS konzentrieren sich h&#228;ufig auf Autokratien. Autokratische
Herrscher, die bestimmten Bev&#246;lkerungsteilen schaden oder sie bestrafen wollten, h&#228;tten durch AWS eine neue 
Handlungsoption. In der Vergangenheit kam es vor, dass sich Milit&#228;rpersonal weigerte, gegen die eigene
Bev&#246;lkerung vorzugehen. Laut dem stellvertretenden US-Verteidigungsminister Robert Work (2015) k&#246;nnten
&#187;autorit&#228;re Regime zu g&#228;nzlich automatisierten L&#246;sungen tendieren&#171;, weil sie menschlichen Sicherheitskr&#228;ften
nicht vertrauen.70 
Auch in Demokratien ist jedoch ein innerstaatlicher Einsatz nicht auszuschlie&#223;en. Das Beispiel von
heutigen unbemannten Systemen zeigt, dass die Einsatzhemmschwelle sinkt, wenn sich neue Technologien
verbreiten und die Bev&#246;lkerung sich an sie gew&#246;hnt. W&#228;hrend die M&#246;glichkeit eines UCAV-Einsatzes gegen
amerikanische Staatsb&#252;rger auf US-Territorium 2013 noch eine heftige Kontroverse ausgel&#246;st hatte (Swaine 2013), 
verf&#252;gen heute zunehmend mehr Polizeieinheiten &#252;ber kleine unbewaffnete Luftfahrzeuge. Im September 2015
legalisierte North Dakota dann als erster Bundesstaat die polizeiliche Nutzung von UAVs, die mit Tr&#228;nengas,
Gummigeschossen und Pfefferspray bewaffnet werden k&#246;nnen (Austin 2015). In Connecticut wurde im
Fr&#252;hjahr 2017 ein Gesetzesvorschlag debattiert, der lokalen Polizeikr&#228;ften die Ausr&#252;stung von UAVs mit t&#246;dlichen
bzw. nichtt&#246;dlichen Waffen gestattet h&#228;tte (Reuters 2017). Letztendlich wurde der Vorschlag zwar abgelehnt
(Smith 2017), dennoch ist es denkbar, dass auch AWS in Zukunft nicht nur in ausl&#228;ndischen Konflikten, sondern
auch innerhalb eines Staates eingesetzt werden.
6.3 Destabilisierende Wirkung in Krisen
Zwischen Milit&#228;rm&#228;chten hat es schon immer Zeiten erh&#246;hter Spannungen gegeben, in denen Akteure bewusst
Risiken der Eskalation auf sich nahmen, um politische Ziele durchzusetzen.71 Dabei ist die F&#228;higkeit, diese
Eskalation gezielt zu managen, entscheidend, um Ziele zu m&#246;glichst geringen (politischen und &#246;konomischen)
Kosten zu erreichen und gleichzeitig das Ausbrechen eines Krieges zu vermeiden.
Es stellt sich die Frage, welche Auswirkungen AWS in einer solchen spannungsgeladenen Situation haben
k&#246;nnten. Auf der einen Seite k&#246;nnten sie die Stabilit&#228;t dadurch erh&#246;hen, dass mehr Informationen in k&#252;rzerer
Zeit beschafft und ausgewertet werden k&#246;nnen und somit eine bessere Grundlage und mehr Zeit f&#252;r menschliche
Entscheidungstr&#228;ger zur Verf&#252;gung steht, alle Konsequenzen einer Eskalation zu durchdenken und die richtige
Entscheidung zu treffen. Au&#223;erdem k&#246;nnten AWS auch als defensive Systeme ausgelegt sein und so einen
eskalationswilligen Akteur von einer Aggression abbringen. Eine weitere M&#246;glichkeit w&#228;re, die Abschreckung
zu erh&#246;hen, indem glaubw&#252;rdig im Falle einer m&#246;glichen Aggression des Gegners mit einem massiven
autonomen Zweitschlag gedroht wird, um auf diese Weise die Stabilit&#228;t zu erh&#246;hen.
Auf der anderen Seite k&#246;nnten autonome Waffensysteme destabilisierend wirken, wenn deren
unvorhergesehene Interaktionen mit der Umwelt, mit gegnerischen Kr&#228;ften oder mit anderen autonomen Systemen
unbeabsichtigt zur Eskalation f&#252;hren. AWS k&#246;nnten das operative Geschehen und die Entscheidungsprozesse
derart beschleunigen, dass das menschliche Reaktionsverm&#246;gen &#252;berfordert w&#252;rde. In einer Krise k&#246;nnte hierdurch
die zur Verf&#252;gung stehende Bedenkzeit, um z. B. m&#246;gliche Unf&#228;lle, Fehlkommunikation oder
Fehlinterpretationen auszuschlie&#223;en, stark verringert werden und eine Eskalationsspirale damit automatisiert und
m&#246;glicherweise ungewollt in Gang gesetzt werden. In Analogie zu Kursverlusten an Devisen- und Aktienm&#228;rkten, die
von automatisierten Handelsplattformen ausgel&#246;st und unter dem Schlagwort &#187;flash crash&#171; (Wikipedia 2010b)
bekannt geworden sind, hat Scharre (2018) f&#252;r dieses Szenario den Begriff &#187;flash war&#171; gepr&#228;gt.
Die potenzielle Beschleunigung des Geschehens k&#246;nnte auch dazu f&#252;hren, dass Akteure dazu veranlasst 
werden, pr&#228;emptiv zuzuschlagen. Au&#223;erdem k&#246;nnte der skizzierte Versuch, das Abschreckungspotenzial durch
autonome Antworten zu erh&#246;hen, auch die Stabilit&#228;t unterminieren, wenn z. B. bei technischen Fehlfunktionen 
keine M&#246;glichkeit der menschlichen Intervention mehr besteht und so ein potenziell katastrophaler Schlag
unbeabsichtigt gef&#252;hrt w&#252;rde.
AWS w&#228;ren auch dazu pr&#228;destiniert, proaktiveres und eventuell provokativeres milit&#228;risches Verhalten zu
unterst&#252;tzen, wie z. B. das Eindringen in gegnerisches Territorium, um dort Aufkl&#228;rung zu betreiben. Ein
gegnerischer Staat k&#246;nnte nun auch eine geringere Hemmung haben, eine solche unbemannte Plattform
abzuschie&#223;en, woraus wiederum ein unmittelbares neues Eskalationspotenzial erw&#228;chst. M&#246;gliche Situationen, in denen
das Eskalationspotenzial von AWS besonders hoch sein kann, sind unterschwellige Krisen, wie z. B. ein
eskalierender diplomatischer Schlagabtausch zwischen Staaten oder schwelende Grenzkonflikte (z. B. zwischen
Indien und China).
70 Rede von Robert O. Work, gehalten in Washington, D.C., 14. Dezember 2015, CNAS Defense Forum, zitiert nach Horowitz et al.
(2016, S.34).
71 Die folgende Darstellung orientiert sich an Scharre (2018, S. 199 ff.).
        
 
 
  
  
     
      
    
  
 
     
    
  
      
     
       
      
     
   
  
    
 
    
     
  
  
  
   
    
    
     
     
   
    
   
  
   
     
    
  
   
   
  
 
     
     
    
   
    
    
     
6.4
6.5
Auswirkungen auf regionale Stabilit&#228;t
Eine Destabilisierung einer latenten Krise zwischen regionalen Kontrahenten droht unmittelbar, falls AWS
aufgrund spezifischer F&#228;higkeiten oder Einsatzoptionen einen bestehenden milit&#228;rischen Status quo gef&#228;hrden, also
das Kr&#228;fteverh&#228;ltnis zu verschieben drohen. Dar&#252;ber hinaus kann die Stabilit&#228;t innerhalb bestimmter Regionen 
durch R&#252;stungsdynamiken gef&#228;hrdet werden, die durch AWS angesto&#223;en bzw. verst&#228;rkt werden. Dies kann sich
zu einer massiven gegenseitigen Aufr&#252;stung (Wettr&#252;sten) entwickeln, die in hohem Ma&#223;e destabilisierend
wirken w&#252;rde.
Als Beispiel hierf&#252;r kann die Situation im Westpazifik angef&#252;hrt werden. Hier stehen sich &#8211; neben einer
Vielzahl weiterer Akteure &#8211; mit den USA und China auch zwei Gro&#223;m&#228;chte gegen&#252;ber, die beide eine regionale
F&#252;hrungsrolle beanspruchen und diese auch mit politischen und milit&#228;rischen Mitteln zu untermauern oder
auszuweiten versuchen. Die USA operieren in diesem Gebiet bisher relativ unbehelligt, u. a. mit ihren
Flugzeugtr&#228;gerverb&#228;nden. In Form von verst&#228;rkten A2/AD-Kapazit&#228;ten arbeitet China bereits an robusten Ma&#223;nahmen,
mit denen es sich zuk&#252;nftig in die Lage versetzen m&#246;chte, gegnerischem Milit&#228;r den Zutritt zu bestimmten
Gebieten des Westpazifiks zu verwehren. Neben dem heutigen Ausbau seiner maritimen Streitkr&#228;fte, von
Offshore-Milit&#228;rbasen und einer Verst&#228;rkung der regionalen Luftverteidigung k&#246;nnten AWS zuk&#252;nftig insbesondere
in der See- und Luftkriegsf&#252;hrung ein probates Mittel f&#252;r das chinesische Milit&#228;r sein, um der maritimen
Dominanz und Luft&#252;berlegenheit der USA zu begegnen. Die USA werden sich ihrerseits den Zutritt zum
Westpazifik mit geeigneten milit&#228;rischen Mitteln sichern wollen, wozu dann sehr wahrscheinlich auch AWS z&#228;hlen
werden. Eine Schw&#228;chung regionaler Stabilit&#228;t kann hier also sowohl aus R&#252;stungsdynamiken erwachsen als
auch diese befeuern. Angesichts einer Vielzahl von Akteuren und Interessen im westpazifischen Raum und
damit einhergehender Krisenpotenziale k&#246;nnte durch AWS die Eskalationsgefahr weiter steigen und damit zur
Instabilit&#228;t der Region beitragen.
Auswirkungen auf das strategische Gleichgewicht
Auf der globalen Ebene spielt &#252;ber den Kalten Krieg hinaus auch immer noch die strategische Stabilit&#228;t
zwischen den Nuklearwaffenstaaten eine herausragende Rolle. Das strategische Gleichgewicht, das seinen
Ausdruck in der gesicherten F&#228;higkeit eines Zweitschlags findet, wird nach Ansicht einiger Staaten (u. a. Russland
u. China) bereits heute durch zusehends bessere Raketenabwehrsysteme und immer zielgenauere konventionelle
Pr&#228;zisionswaffen (&#187;prompt global strike&#171; [Wikipedia 2006a] und zielgenaue Marschflugk&#246;rper) tangiert.
K&#252;nftige AWS (in Form von UCAVs oder Marschflugk&#246;rpern mit KI-Suchk&#246;pfen) m&#252;ssen vom jeweiligen
Widersacher als eine potenzielle Bedrohung der strategischen Stabilit&#228;t angesehen werden. So ist es vorstellbar, dass 
sehr potente AWS zuk&#252;nftig z. B. als konventionelle Erstschlagwaffen zur Zerst&#246;rung gegnerischer
Nuklearwaffenarsenale eingesetzt werden, m&#246;gliche Ziele (Raketensilos oder mit Nuklearwaffen best&#252;ckte U-Boote)
selbstst&#228;ndig ausmachen, in deren N&#228;he unentdeckt verweilen und auf Befehl koordiniert diese Ziele angreifen
und zerst&#246;ren. AWS k&#246;nnten auch selber als Tr&#228;gerplattformen f&#252;r Nuklearwaffen verwendet werden, mit der
Intention, schneller, &#252;berraschender und koordinierter als bisherige Tr&#228;gersysteme zuzuschlagen und
vorhandene Verteidigungsma&#223;nahmen aushebeln zu k&#246;nnen. Die Gefahr und die Angst vor einem m&#246;glicherweise
vernichtenden Erstschlag w&#252;rden durch eine solche Nutzung von AWS erheblich zunehmen und die strategische
Stabilit&#228;t infrage stellen. Dieses k&#246;nnte weitere nukleare Abr&#252;stung unm&#246;glich machen und vielmehr eine &#196;ra
nuklearer Modernisierung oder sogar nuklearer Aufr&#252;stung einl&#228;uten.
R&#252;stungsdynamiken
R&#252;stung ist im Wesentlichen ein Wechselspiel aus der Modernisierung des eigenen Waffenpotenzials
(Effizienz- und F&#228;higkeitssteigerung) und der Reaktion auf potenzielle bzw. wahrgenommene &#228;u&#223;ere Bedrohungen
(Auf- oder Abr&#252;sten). F&#252;hlen sich L&#228;nder bedroht oder streben eine milit&#228;rische Hegemonie an, so kann es zu
einem Wettr&#252;sten oder einer R&#252;stungsspirale zwischen diesen L&#228;ndern kommen.
Die Entwicklung neuer Waffentechnologien verspricht eine Steigerung der milit&#228;rischen Effizienz und
Schlagkraft, und es werden h&#228;ufig auch erweiterte milit&#228;rische Einsatzszenarien m&#246;glich. Da die allermeisten
Staaten danach streben, milit&#228;risch ihren potenziellen Kontrahenten mindestens ebenb&#252;rtig zu sein, f&#252;hren neue
Waffentechnologien h&#228;ufig zu R&#252;stungsbestrebungen unter denjenigen Staaten, die &#252;ber die entsprechenden
6.6
        
 
 
      
   
  
 
 
    
  
   
  
   
    
  
 
  
      
  
     
    
    
    
  
 
   
  
 
      
 
     
   
   
  
   
      
   
 
  
   
  
 
 
   
   
 
   
  
   
   
6.7
6.8
&#246;konomischen und industriellen Voraussetzungen verf&#252;gen. Damit entsteht bei Kontrahenten ein Anreiz
nachzuziehen und ebenfalls in die R&#252;stung und Entwicklung neuer Waffentechnologien zu investieren. Somit wird
eine wechselseitige R&#252;stungsspirale angetrieben (technologisches Wettr&#252;sten).
Wie bereits ausgef&#252;hrt, schreiben die Gro&#223;m&#228;chte autonomen Technologien einen langfristig hohen
milit&#228;rischen Stellenwert zu. Technologische Durchbr&#252;che einer Partei k&#246;nnen das bestehende Kr&#228;ftegleichgewicht
fundamental ersch&#252;ttern. Ein solches Bedrohungsszenario dient verbreitet als Legitimation f&#252;r eigene
Entwicklungen und Anstrengungen auf diesem Feld. Zwischen den USA und China ist bereits heute eine beginnende
R&#252;stungsdynamik im Feld zunehmend automatisierter UWS zu beobachten (Kap. 4.1.1). Auch Russland
unternimmt konzentrierte Bestrebungen, um hinsichtlich heutiger UWS den bisherigen Vorsprung des Westens
aufzuholen, und scheint auch zuk&#252;nftig AWS entwickeln zu wollen. Indien und S&#252;dkorea wiederum sind ohnehin
schon in regionalen R&#252;stungsspiralen mit ihren jeweiligen Nachbarn engagiert (Pakistan u. China bzw.
Nordkorea), die Beschaffung von AWS k&#246;nnte hier zus&#228;tzliche R&#252;stungsdynamiken in Gang setzen.
Unkontrollierte Weiterverbreitung
Zuk&#252;nftige AWS werden auf technologischen Entwicklungen basieren, die ihren Ursprung gr&#246;&#223;tenteils im
zivilen Sektor haben. Die Forschung zur k&#252;nstlichen Intelligenz &#8211; einschlie&#223;lich maschinellem Lernen, Big-Data-
Analysen &#8211; wird ma&#223;geblich von gro&#223;en Konzernen mit Blick auf private Konsumenten und den zivilen
Wirtschaftssektor vorangetrieben. Bei einem gro&#223;en Teil der Hardware zuk&#252;nftiger AWS, vor allem aber bei der
Software, handelt es sich &#252;berwiegend um Dual-Use-Technologien. Von dezidiert milit&#228;rischen Bestandteilen
&#8211; wie moderne Bewaffnung, Hochleistungsantriebe oder spezielle milit&#228;rische Werkstoffe (zur Panzerung,
Tarnung etc.) &#8211; einmal abgesehen, unterliegen die wesentlichen technologischen Grundlagen und das Know-how
f&#252;r eine zunehmende Automatisierung bzw. Autonomie milit&#228;rischer Systeme keinen Beschr&#228;nkungen bei der
Weiterverbreitung und sind im Prinzip frei verf&#252;gbar.
Die Entwicklung und Produktion von leistungsstarken AWS wird trotz verf&#252;gbarer Dual-Use-
Technologien allerdings erhebliche finanzielle und zeitliche Ressourcen erfordern. Andererseits k&#246;nnten Staaten oder
andere Akteure auch versuchen, fortschrittliche zivile Technologien in bewaffnete AWS zu &#252;berf&#252;hren, ggf.
unter Abstrichen bei der Leistungsf&#228;higkeit (Nutzlast, Reichweite, Zuverl&#228;ssigkeit etc.), aber dennoch mit
weitgehend autonomen Funktionen. Auf diesem Weg k&#246;nnte es auch nichtstaatlichen Akteuren gelingen, in den
Besitz von kleineren AWS zu gelangen. Eine &#228;hnliche Entwicklung ist heute bereits im Hinblick auf Drohnen 
zu beobachten (Kap. 4). Ob z. B. f&#252;r Terroristen kleine bewaffnete kommerzielle AWS zuk&#252;nftig attraktiv sein 
werden, ist derzeit noch nicht abzusehen, zumal &#228;hnliche Wirkungen auch mit sehr viel einfacheren Mitteln und
geringerem Aufwand erreichbar sind, wie Anschl&#228;ge des IS in der j&#252;ngsten Zeit gezeigt haben.
Bereits heute existiert eine zunehmend sch&#228;rfer werdende internationale Konkurrenz um den Export von
UCAVs (besonders China und Israel sind hier sehr aktiv; Kap. 4). Auch die USA haben ihre bisher strengen
Exportvorschriften in dieser Hinsicht j&#252;ngst gelockert (U.S. Department of State 2018). Der Wettbewerb um
R&#252;stungsexporte k&#246;nnte zuk&#252;nftig auch die Weiterverbreitung von Systemen mit mehr und mehr autonomen
Funktionen beschleunigen.
Technologische Risiken
Moderne Waffensysteme werden immer komplexer und deren Hardware, aber insbesondere die Software
weisen vielfache Fehlerpotenziale bzw. Einfallstore f&#252;r Systemmanipulationen (&#187;hacking&#171;) und St&#246;rungen von
au&#223;en (&#187;jamming&#171;, &#187;spoofing&#171;) auf. Diese potenzielle Verwundbarkeit moderner Waffensysteme gilt generell f&#252;r
bemannte und unbemannte Systeme. Bemannte Systeme sind zumeist aber sehr viel ausfallsicherer konzipiert
und weisen gerade bei lebenswichtigen Systemen oft Redundanzen auf. Sie haben dar&#252;ber hinaus den Vorteil,
&#252;ber einen menschlichen Operateur zu verf&#252;gen, dem im Falle von Fehlfunktionen noch (manuelle)
Eingriffsm&#246;glichkeiten zur Verf&#252;gung stehen.
Die netzwerkzentrierte Kriegsf&#252;hrung setzt die Existenz und das reibungslose Funktionieren einer
hochkomplexen Infrastruktur voraus, die eine hinreichende geografische Abdeckung, verl&#228;ssliche
Kommunikationsverbindungen und hohe Datendurchsatzraten aufweist. Diese Datennetze bieten potenziellen Gegnern
Angriffsm&#246;glichkeiten, sei es das Zerst&#246;ren wichtiger Knotenpunkte (Relaisstationen, Kommandobasen, Satelliten etc.),
das St&#246;ren des Kommunikationsfrequenzspektrums als solches oder die Manipulation des Datenflusses.
Zunehmend automatisierte UWS oder zuk&#252;nftige AWS zeichnen sich durch ihre immer komplexere
Programmierung aus. Die Erfahrung zeigt, dass es unvermeidlich ist, dass diese Programme eine hohe Anzahl an
Fehlern enthalten, deren Auswirkungen im Detail nicht abzusehen sind, Programmabst&#252;rze und
unvorhersehbare Reaktionen eingeschlossen. Auch ist es unm&#246;glich, alle Eventualit&#228;ten einer operationellen Umgebung
vorherzusehen und entsprechend zu ber&#252;cksichtigen. Falls zuk&#252;nftige AWS die F&#228;higkeit zum eigenst&#228;ndigen
Lernen haben werden, stellt sich die Frage, wie sichergestellt werden kann, dass nicht unerw&#252;nschtes Verhalten
mit potenziell katastrophalen Konsequenzen erlernt wird.72 
Hinzu kommt, dass jede Programmierung auch manipuliert werden kann (z. B. &#252;ber Datenverbindungen
von au&#223;en oder direkt am System) und Schadcodes sehr lange unentdeckt in einem solchen System verweilen
k&#246;nnen. So k&#246;nnte im Ernstfall ohne Vorwarnung das Waffensystem z. B. gest&#246;rt, manipuliert oder sogar durch
den Gegner &#252;bernommen werden. Insgesamt bleibt festzuhalten, dass AWS zwar weitreichende F&#228;higkeiten
aufweisen, diese aber mit einer erheblichen systemischen Verwundbarkeit verbunden ist.
Aus diesem Grund pl&#228;diert die Ethik-Kommission (2017) f&#252;r automatisiertes und vernetztes Fahren daf&#252;r, selbstlernende Systeme 
nur f&#252;r nichtsicherheitsrelevante Funktionen zuzulassen.
72 

        
 
 
  
  
  
 
 
     
   
   
  
  
  
     
 
  
        
      
 
        
 
    
    
  
    
  
 
      
      
    
      
       
   
  
    
    
  
                                              
            
                
  
   
        
      
           
   
 
 
     
7
Humanit&#228;res V&#246;lkerrecht und autonome Waffensysteme
Wie jedes Waffensystem sind auch AWS im Kontext der geltenden Normen des humanit&#228;ren V&#246;lkerrechts zu
betrachten. Das HVR soll im Falle eines internationalen bewaffneten Konflikts (&#187;ius in bello&#171;) den
gr&#246;&#223;tm&#246;glichen Schutz von Zivilisten, nichtmilit&#228;rischen Geb&#228;uden und Infrastrukturen sowie der nat&#252;rlichen Umwelt 
gew&#228;hrleisten und intendiert somit, eine Balance zwischen humanit&#228;ren Erw&#228;gungen und milit&#228;rischen
Notwendigkeiten zu schaffen. Das HVR bezieht sich nicht prim&#228;r auf technologische Systeme als solche, sondern
zielt darauf ab, die Art und Umst&#228;nde ihres Einsatzes einzuschr&#228;nken.73 
Im Zentrum des HVR stehen die vier Genfer Konventionen aus dem Jahr 1949 sowie zwei
Zusatzprotokolle von 1977 (IKRK o. J.). Darin sind drei fundamentale Prinzipien des Waffeneinsatzes festgeschrieben:
&#8250; das Unterscheidungsgebot
&#8250; das Prinzip der Verh&#228;ltnism&#228;&#223;igkeit
&#8250; das Vorsorgeprinzip74 
Zus&#228;tzlich soll mittels der Martens&#8217;schen Klausel sichergestellt werden, dass Sachverhalte, die nicht explizit im
HVR geregelt sind, mit dem Prinzip der Menschlichkeit und dem &#246;ffentlichen Gewissen in Einklang gebracht 
werden. Die Klausel wurde bereits in der Pr&#228;ambel der Haager Landkriegsordnung (1899) eingef&#252;hrt und ist in 
abgewandelter Formulierung im ZP I (1977) der Genfer Konventionen (1949), Artikel 1 Absatz 2, zu finden: 
&#187;In F&#228;llen, die von diesem Protokoll oder anderen internationalen &#220;bereink&#252;nften nicht erfasst sind, verbleiben
Zivilpersonen und Kombattanten unter dem Schutz und der Herrschaft der Grunds&#228;tze des V&#246;lkerrechts, wie
sie sich aus feststehenden Gebr&#228;uchen, aus den Grunds&#228;tzen der Menschlichkeit und aus den Forderungen des
&#246;ffentlichen Gewissens ergeben.&#171;
Das Internationale Komitee vom Roten Kreuz (IKRK 2006, S. 17) legt diesen Artikel dahingehend aus,
dass neue Waffensysteme den Prinzipien der Menschlichkeit und den Forderungen des &#246;ffentlichen Gewissens
nicht grundlegend widersprechen d&#252;rfen75.
7.1 Pr&#252;fungspflicht (Artikel 36 ZP I)
Ob der Einsatz neu entwickelter Waffen oder Mittel und Methoden der Kriegsf&#252;hrung stets oder unter
bestimmten Umst&#228;nden durch das ZP I oder eine andere v&#246;lkerrechtliche Bestimmung verboten sein k&#246;nnte, muss
bereits im Vorfeld, und zwar &#187;bei Pr&#252;fung, Entwicklung, Beschaffung oder Einf&#252;hrung&#171; gepr&#252;ft und festgestellt
werden. Hierzu verpflichten sich die Vertragsparteien in Artikel 36 ZP I (1977) der Genfer Konventionen
(1949).76 Wie dieser Pr&#252;fprozess im Detail auszusehen hat, ist im HVR jedoch nicht n&#228;her geregelt.77 
Tats&#228;chlich setzt derzeit nur eine kleine Anzahl von L&#228;ndern diese grundlegende Verpflichtung zur
Waffenpr&#252;fung um. Das IKRK (2006) nennt Australien, Belgien, die Niederlande, Norwegen, Schweden, die
Vereinigten Staaten, Frankreich, das Vereinigte K&#246;nigreich und Deutschland. Dies sind alles Staaten mit einer
modernen Verteidigungsindustrie. Die Pr&#252;fung wird von den L&#228;ndern sowohl prozedural als auch hinsichtlich der
angewandten inhaltlichen Pr&#252;fkriterien auf sehr verschiedene Weise durchgef&#252;hrt (IKRK 2006; McClelland
2003). In Deutschland wird sie im Rahmen des Beschaffungsverfahrens vom Referat Recht I 3 des
Bundesverteidigungsministeriums durchgef&#252;hrt und muss vor Beginn der Nutzungsphase abgeschlossen sein (BMVg
73 Siehe hierzu insbesondere die Artikel 35, 48, 51, 52 bis 57, ZP I (1977) der Genfer Konventionen (1949).
74 Obwohl einige L&#228;nder, u. a. auch die USA, das ZP I nicht ratifiziert haben, besteht weitreichende &#220;bereinstimmung darin, dass die
hier formulierten Grundprinzipien Eingang in das V&#246;lkergewohnheitsrecht gefunden haben und somit als allgemein verbindlich
angesehen werden k&#246;nnen (Schmitt 2012, S.92 ff.).
75 Das IKRK spielt im HVR eine herausgehobene Rolle. Dies kommt u. a. darin zum Ausdruck, dass das IKRK in den Genfer
Konventionen als einzige Institution explizit als Kontrollorgan genannt wird (siehe z.B. Wikipedia 2003d).
76 Artikel 36 ZP I (1977) der Genfer Konventionen (1949) im Original: &#187;In the study, development, acquisition or adoption of a new
weapon, means or method of warfare, a High Contracting Party is under an obligation to determine whether its employment would,
in some or all circumstances, be prohibited by this Protocol or by any other rule of international law applicable to the High
Contracting Party.&#171;
77 F&#252;r weitere Interpretation/Hintergrundinformationen zum HVR vgl. BMVg (1992).
        
 
 
       
     
   
   
 
    
         
  
     
   
     
       
     
    
    
    
      
     
    
  
    
    
  
    
  
     
    
   
   
 
     
 
     
 
 
     
    
         
      
     
    
      
      
     
   
 
 
7.2
2016, S. 4). Dass auch die USA Waffenpr&#252;fungen durchf&#252;hren, obwohl sie nicht Vertragspartei des ZP I sind, 
darf als Beleg daf&#252;r gelten, dass die in Artikel 36 ZP I formulierte Pr&#252;fverpflichtung als Teil des allgemein
verbindlichen V&#246;lkergewohnheitsrechts anzusehen ist (Boulanin/Verbruggen 2017, S. 73; Nasu/McLaughlin 
2014, S. 26).
Obwohl es keine Verpflichtung zur Offenlegung oder Ver&#246;ffentlichung der Pr&#252;fungsergebnisse gibt, sind 
die Berichte in einigen L&#228;ndern &#246;ffentlich verf&#252;gbar &#8211; sofern keine f&#252;r die nationale Sicherheit sensiblen
Informationen enthalten sind &#8211;, z. B. in den USA und in Schweden (IKRK 2006, S. 27). In Deutschland werden die
Berichte nicht ver&#246;ffentlicht. Als Grund gibt die Bundesregierung zu wahrende Geheimschutzverpflichtungen,
Betriebsgeheimnisse und immaterielle Rechte Dritter an (Deutscher Bundestag 2016, S. 33 f.).
Heutige UWS werden im Rahmen v&#246;lkerrechtlicher Betrachtungen meist den bisherigen Waffensystemen
gleichgestellt, da ihr Einsatz zurzeit &#252;berwiegend ferngesteuert ist und immer ein menschlicher Operator die
Einsatzverantwortung hat (TAB 2011, S. 199 ff.; WD 2012). Andererseits k&#246;nnen unbemannte Systeme auch
als reine Plattformen dienen. Als solche (d. h., wenn sie keine Wirkmittel oder Nutzlasten tragen, die sich auf
die eigenen offensiven oder defensiven F&#228;higkeiten auswirken) w&#252;rden sie per se keine Waffen, Mittel oder
Methoden der Kriegsf&#252;hrung darstellen. F&#252;r diese l&#228;ge eine Pr&#252;fungspflicht gem&#228;&#223; Artikel 36 ZP I demzufolge 
nicht vor (Deutscher Bundestag 2016, S. 34).
Zuk&#252;nftige AWS m&#252;ssen den &#220;berpr&#252;fungen nach Artikel 36 ZP I erst noch unterzogen werden. Hierf&#252;r
m&#252;sste die Konformit&#228;t mit den Prinzipien des HVR in allen intendierten bzw. erwarteten Einsatzszenarien
nachgewiesen werden. Eine notwendige Voraussetzung daf&#252;r ist, dass das Verhalten des AWS sehr hohen
Anforderungen an Vorhersagbarkeit und Verl&#228;sslichkeit gen&#252;gt. Test und Verifikation dieser Eigenschaft stellen
aufgrund der speziellen Eigenschaften von AWS eine besondere, bisher ungel&#246;ste Herausforderung dar
(Davison 2017, S. 9 f.).
AWS im Lichte der Prinzipien des HVR
Zuk&#252;nftige AWS werden sich gegen&#252;ber jedem bisherigen Waffensystem, inklusive der heutigen unbemannten
Waffensysteme, dadurch auszeichnen, dass sie in sich dynamisch ver&#228;ndernden, nicht vorhersehbaren
Umfeldern autonom agieren k&#246;nnen, bis zu einem gewissen Grad also selber Handlungsentscheidungen treffen
m&#252;ssen, ohne dabei einer direkten menschlichen Steuerung und Kontrolle zu unterliegen. Um die zentrale Frage zu
beantworten, ob ein Einsatz von AWS v&#246;lkerrechtskonform sein kann, muss zun&#228;chst untersucht werden, ob
bzw. unter welchen Voraussetzungen die grundlegenden Prinzipien des HVR von AWS eingehalten werden
k&#246;nnen.
Die Natur aktueller Konflikte erschwert dabei die Aufgabe, jederzeit den Bestimmungen des HVR
Rechnung zu tragen. H&#228;ufig finden diese nicht direkt zwischen Staaten und ohne klar definierte geografische
Frontlinie statt. Gefechte nahe oder inmitten ziviler Gebiete mit K&#228;mpfern, die sich absichtlich m&#246;glichst wenig von
Zivilisten unterscheiden, sind mehr und mehr die Regel.
Unterscheidungsgebot
Das HVR gebietet, dass Kampfhandlungen sich nur gegen milit&#228;rische Ziele &#8211; Kombattanten und milit&#228;rische
Objekte &#8211; richten d&#252;rfen. Die Zivilbev&#246;lkerung und zivile Objekte sind zu sch&#252;tzen und zu schonen (Artikel 48
bis 52 ZP I). Insbesondere verbietet Artikel 51 Absatz 2 ZP I jegliche Angriffe sowohl auf die Zivilbev&#246;lkerung
generell als auch auf einzelne Zivilisten. Unterschiedslose Angriffe, die nicht gegen ein bestimmtes
milit&#228;risches Ziel gerichtet werden bzw. bei denen Kampfmethoden und -mittel eingesetzt werden, die nicht gegen ein
bestimmtes milit&#228;risches Ziel gerichtet werden k&#246;nnen, sind gem&#228;&#223; Artikel 51 (4) ZP I verboten. Dar&#252;ber
hinaus d&#252;rfen gegnerische Kr&#228;fte nicht angegriffen werden, die au&#223;er Gefecht (&#187;hors de combat&#171;) sind. Darunter 
fallen u. a. Soldaten, die ihre Intention angezeigt haben, sich zu ergeben, und solche, die aufgrund einer
Verletzung, Bewusstlosigkeit oder &#196;hnlichem nicht in der Lage sind, sich zu verteidigen (Artikel 41 ZP I).
Ein AWS, das v&#246;lkerrechtskonform eingesetzt werden soll, m&#252;sste also legitime milit&#228;rische Ziele
zuverl&#228;ssig und mit hoher Trefferquote unter realen Gefechtsbedingungen identifizieren k&#246;nnen, d. h. bei
ung&#252;nstigen Lichtverh&#228;ltnissen (Tag/Nacht), Wetter- und Umweltbedingungen, sich schnell &#228;ndernden Gefechtslagen,
extremem Zeitdruck sowie trotz m&#246;glicher Gegenma&#223;nahmen des Gegners (einschlie&#223;lich Tarnen, T&#228;uschen,
Blenden von Sensoren).
Da auch Menschen bei dieser komplexen Aufgabe regelm&#228;&#223;ig Fehler unterlaufen, wird man sicherlich bei
AWS in der Praxis auch keine Unfehlbarkeit bei der Identifikation legitimer Ziele erwarten k&#246;nnen. Welche
Fehlerquote dabei im Sinne der Einhaltung der v&#246;lkerrechtlichen Normen als tolerabel angesehen wird, ist
derzeit noch eine offene Frage. Wie im Kapitel 8.1.2 ausgef&#252;hrt wird, vertritt Arkin (2015, S. 46) die Auffassung,
dass AWS diese Aufgabe &#187;besser als ein Mensch&#171; erf&#252;llen k&#246;nnten.
Allerdings darf bezweifelt werden, ob dies als Ma&#223;stab tats&#228;chlich tr&#228;gt, denn erstens spielt nicht nur die
H&#228;ufigkeit, sondern auch die Art der Fehler eine wichtige Rolle f&#252;r die Bewertung ihrer Akzeptabilit&#228;t. Wie in
Kapitel 3.3 beschrieben, k&#246;nnen beispielsweise heutige Verfahren der Bilderkennung bestimmte Aufgaben mit
hoher Trefferquote bew&#228;ltigen. Gleichzeitig k&#246;nnen jedoch krasse Fehler auftreten, die der menschlichen
Intuition diametral entgegenlaufen bzw. die einem Menschen niemals unterlaufen w&#252;rden (in Kap. 3.3.3 wird das
Beispiel genannt, dass von zwei Bildern, die sich lediglich in einem einzigen Pixel unterscheiden, eines korrekt
erkannt wird und das andere nicht).
Zweitens ist unklar, wie &#187;besser als ein Mensch&#171; konkret zu verstehen ist. Da AWS ganz eigene, besondere 
Eigenschaften aufweisen, werden sie voraussichtlich auch in Einsatzszenarien Verwendung finden, die unter
Einbeziehung menschlicher Bediener gar nicht in Betracht k&#228;men. W&#228;re z. B. ein Angriff bei sehr schlechten
Sichtbedingungen durch dichten Nebel legitim, bei dem ein AWS mittels seiner technischen Vorrichtungen nur
gelegentlich ein milit&#228;risches Ziel korrekt identifizieren w&#252;rde, nur weil ein Mensch hier noch schlechter
abschneiden w&#252;rde? Oder ist als Vergleichsma&#223;stab ein menschlicher Operator heranzuziehen, dem s&#228;mtliche
Sensordaten und Hilfssysteme nach dem technologischen State of the Art zur Verf&#252;gung stehen? Aber w&#228;re
dieses technologisch-menschliche Hybridsystem nicht einem rein technischen AWS oftmals &#252;berlegen? Und
w&#228;re in zeitkritischen Situationen, in denen die menschliche Reaktionsf&#228;higkeit schlicht &#252;berfordert w&#228;re, der
Einsatz jeglicher milit&#228;rischer Gewalt durch AWS statthaft, wenn diese auch nur eine rudiment&#228;re
Zieldiskriminierung leisten k&#246;nnten?
Die eigentliche Problematik ist jedoch anders gelagert, denn zur Entscheidung, ob ein Objekt oder eine 
Person ein legitimes milit&#228;risches Ziel darstellt, reicht deren zuverl&#228;ssige Identifizierung bei Weitem nicht aus.
Hierf&#252;r ist ein umfassenderes Lagebild erforderlich sowie die Einsch&#228;tzung von Verhaltensweisen und letzten
Endes der Intentionen des Gegners. So k&#246;nnte zwar ein Kriegsschiff relativ problemlos von einer
Zielerkennungssoftware identifiziert werden. Aber einem erfahrenen Seemann w&#252;rde dabei unter Umst&#228;nden auffallen,
dass die Lage des Rumpfes relativ zu Wind und Str&#246;mung sowie der aus dem Maschinenraum austretende
Rauch daf&#252;r sprechen, dass das Schiff sich in Seenot befindet und daher &#187;hors de combat&#171; ist. W&#228;re eine
Software zur Einbeziehung derart weicher Kontextfaktoren und abstrakter Abw&#228;gung in der Lage? Auch ist schwer
vorstellbar, auf welche Weise sich ein verwundeter Soldat einem AWS ergeben k&#246;nnte. Dies w&#252;rde die korrekte 
Deutung subtiler &#8211; auch emotionaler &#8211; Signale sowie verbaler und nonverbaler Kommunikation erfordern.
Aufgrund dieser &#220;berlegungen sind Zweifel angebracht, ob softwarebasierte Systeme wie AWS
(ausgenommen hypothetische mit menschen&#228;hnlicher, starker k&#252;nstlicher Intelligenz ausgestattete, Sparrow 2016,
S. 99) in absehbarer Zeit in der Lage sein werden, dem Unterscheidungsgebot des HVR Gen&#252;ge zu leisten.
Einige Kritiker gehen noch weiter und bestehen darauf, dass dies letztlich nur von Menschen und niemals von
technologischen Systemen zu leisten ist. Noel Sharkey (2012a) fasst dies wie folgt zusammen: &#187;Menschen
verstehen einander auf eine Weise, die Maschinen nicht m&#246;glich ist. Signale k&#246;nnen sehr subtil sein und es gibt
eine unendliche Anzahl an Umst&#228;nden unter denen der Einsatz t&#246;dlicher Gewalt unangemessen ist&#171;78 
Prinzip der Verh&#228;ltnism&#228;&#223;igkeit
Das Unterscheidungsgebot stellt zwar hohe Anforderungen an milit&#228;rische Angriffe, allerdings bedeutet dies
nicht, dass generell von Angriffen abgesehen werden muss, wenn dabei Zivilisten und/oder zivile Einrichtungen 
in Mitleidenschaft gezogen werden k&#246;nnten. Derartige Kollateralsch&#228;den d&#252;rfen in Kauf genommen werden,
&#220;bersetzung durch das TAB; im Original: &#187;Humans understand one another in a way that machines cannot. Cues can be very subtle,
and there are an infinite number of circumstances where lethal force is inappropriate.&#171;
78
allerdings nur in einem Umfang, der nicht exzessiv ist in Relation zu dem konkreten und direkten milit&#228;rischen
Nutzen der Operation. Dies ist das Prinzip der Verh&#228;ltnism&#228;&#223;igkeit des HVR.79 
Diese Norm anzuwenden ist eine der schwierigsten Aufgaben im Kontext des HVR (Schmitt 2012, S. 190). 
Zun&#228;chst einmal m&#252;ssen h&#246;chst unterschiedliche, im Kern inkommensurable Kategorien &#8722; milit&#228;rische und
humanit&#228;re &#8722; gegeneinander abgewogen werden. Wie bestimmt man beispielsweise den Wert eines Bombers,
Kriegsschiffes, Panzers oder eines Aussichtspostens relativ zur Zahl der daf&#252;r geopferten zivilen
Menschenleben (Schmitt 2012, S. 139)?80 Hinzu kommt, dass die Bewertungsgrundlage w&#228;hrend des Kriegsgeschehens
durch milit&#228;rische und/oder humanit&#228;re Ereignisse einem st&#228;ndigen Wandel ausgesetzt ist. Kommando- und 
Kommunikationsstrukturen des Gegners zu zerst&#246;ren, kann am Anfang eines bewaffneten Konflikts von
entscheidender milit&#228;rischer Bedeutung sein. In einer Situation, wo sich Bem&#252;hungen abzeichnen, die
Auseinandersetzung auf dem Verhandlungsweg zu beenden, w&#228;re dies dagegen in h&#246;chstem Ma&#223;e kontraproduktiv
(Schmitt 2012, S. 139). Dar&#252;ber hinaus existieren keine objektiven und global einheitlichen Standards: Die
Wertebasis, auf deren Grundlage ein Kommandeur entscheidet, welches Ausma&#223; an menschlichem Leid in Bezug auf
ein bestimmtes milit&#228;risches Ziel akzeptabel erscheint, h&#228;ngt entscheidend von dessen kultureller und sozialer
Pr&#228;gung ab.
Dass die Anwendung der Norm nicht, wie oft f&#228;lschlicherweise dargestellt, einem pr&#228;zisen W&#228;geprozess
entspricht, wo bereits ein geringes &#220;berwiegen der einen oder anderen Seite den Ausschlag gibt, sondern dass
lediglich gepr&#252;ft werden muss, ob der zu erwartende Kollateralschaden bezogen auf den milit&#228;rischen Nutzen
exzessiv ist, d. h. in keiner vern&#252;nftigen Relation steht, erleichtert die Aufgabe nicht wesentlich. Zwar wird
damit die kaum je erf&#252;llbare Anforderung beseitigt, den milit&#228;rischen Vorteil sowie den Kollateralschaden
pr&#228;zise messen bzw. bestimmen zu m&#252;ssen. Allerdings wird gefordert, dass s&#228;mtliche relevanten Umst&#228;nde
einzubeziehen sind und das Resultat der Abw&#228;gung eben vern&#252;nftig ist. Der Internationale Strafgerichtshof f&#252;r das
ehemalige Jugoslawien hat dies 2003 so konkretisiert: H&#228;tte &#187;eine durchschnittlich informierte Person (&#8250;a 
reasonably well-informed person&#8249;) unter den zum fraglichen Zeitpunkt vorherrschenden Umst&#228;nden und unter
vern&#252;nftiger Ber&#252;cksichtigung der verf&#252;gbaren Informationen [...] erwarten k&#246;nnen, dass es durch den Angriff
zu einer exzessiven Anzahl ziviler Opfer kommen w&#252;rde&#171; (Gei&#223; 2015, S. 16 ff.)?
Deswegen bestehen begr&#252;ndete Zweifel, ob softwaregest&#252;tzte Systeme wie AWS in der Lage sein k&#246;nnen,
die erforderlichen Abw&#228;gungen zur Verh&#228;ltnism&#228;&#223;igkeit eines Angriffs zufriedenstellend und zuverl&#228;ssig
durchzuf&#252;hren. Der Umfang von Kontext- und Weltwissen sowie die F&#228;higkeit, die Aktionen von Menschen zu
interpretieren und zu verstehen, die hierf&#252;r erforderlich sind, sind auf jeden Fall auf absehbare Zeit nicht in ihrer
Reichweite. Analog wie beim Unterscheidungsgebot m&#252;sste auch hierf&#252;r nach Sparrow (2016, S. 99 ff.) starke
k&#252;nstliche Intelligenz vorausgesetzt werden.
Eine gegens&#228;tzliche Position vertritt Arkin (2008, S. 126) (Kap. 6.1), der computerbasierten Systemen mit
Verweis auf ihre &#252;berlegenen sensorischen und Simulationsf&#228;higkeiten zutraut, die Konsequenzen eines
Waffeneinsatzes auf ein bestimmtes Ziel effektiver bestimmen zu k&#246;nnen als jeder Mensch. In der Tat existieren f&#252;r
diesen Zweck bereits Softwaretools. So setzen beispielsweise die Streitkr&#228;fte der USA das Programm &#187;
Bugsplat&#171; ein, um dasjenige Wirkmittel auszuw&#228;hlen und so zu steuern, dass ein m&#246;glichst geringer
Kollateralschaden verursacht wird. Allerdings wird damit keineswegs die gesamte Breite der Abw&#228;gungen abgedeckt, die
vom Verh&#228;ltnism&#228;&#223;igkeitsgebot gefordert sind, sondern nur der Teil, den Sharkey (2012b, S. 789 f.) als das
&#187;easy proportionality problem&#171; bezeichnet. Dieses ist erheblich leichter durch Software zu l&#246;sen als das &#187;hard
proportionality problem&#171;, bei dem qualitative und subjektive Entscheidungen eine wesentlich dominantere 
Rolle spielen. Somit bleibt Arkin letztlich eine Antwort auf die Frage schuldig, ob er das &#187;hard proportionality
problem&#171; f&#252;r durch Software l&#246;sbar h&#228;lt.
Vorsorgeprinzip
Nachdem ein milit&#228;risches Ziel identifiziert wurde und dieses nach Abw&#228;gung der Verh&#228;ltnism&#228;&#223;igkeit als
legitim angesehen wird, ist als letzter Schritt vor einem Angriff dem Vorsorgeprinzip Rechnung zu tragen. Dieses
verpflichtet den Angreifer, dasjenige Mittel zu w&#228;hlen, das der Zivilbev&#246;lkerung bzw. zivilen Objekten den
79 Artikel 51 Absatz 5b ZP I im Original: &#187;Among others, the following types of attacks are to be considered as indiscriminate: [...]
an attack which may be expected to cause incidental loss of civilian life, injury to civilians, damage to civilian objects, or a
combination thereof, which would be excessive in relation to the concrete and direct military advantage anticipated.&#171;
80 Sharkey (2012b, S.789) nennt dies das &#187;hard proportionality problem&#171;.
geringsten Schaden zuf&#252;gt (Artikel 57 Absatz 2aii ZP I). Dar&#252;ber hinaus besteht die Verpflichtung, die
Zivilbev&#246;lkerung vor Angriffen zu warnen, die sie tangieren k&#246;nnen, soweit dem kein gewichtiger Grund
entgegensteht (Artikel 57 Absatz 2c ZP I).
Der Ma&#223;stab dieser Verpflichtung ist subjektiv, da die konkret in der Situation verf&#252;gbaren Optionen
abgewogen werden m&#252;ssen. Streitkr&#228;fte, die hochpr&#228;zise Wirkmittel zur Verf&#252;gung haben, w&#228;ren somit einem
h&#246;heren Standard unterworfen als schlechter ausger&#252;stete Gegner ohne diese M&#246;glichkeiten. Es ist auch nicht 
auszuschlie&#223;en, dass in einer Gefechtssituation nur eine einzige Option zur Verf&#252;gung steht und somit eine
Abw&#228;gung gar nicht sinnvoll durchgef&#252;hrt werden kann. Eine explizite Verpflichtung, durch Entwicklung und
Einsatz neuer Technologien Optionen zu schaffen, die m&#246;glichst wenige Kollateralsch&#228;den verursachen, besteht
nicht (Schmitt 2012, S. 159 ff.). Dessen ungeachtet wird dies insbesondere in Staaten mit hohem
Verantwortungsbewusstsein oft als Argument f&#252;r die Entwicklung von immer pr&#228;ziseren Waffen und Methoden der
Kriegsf&#252;hrung angef&#252;hrt.
Die Abw&#228;gungen gem&#228;&#223; dem Vorsorgeprinzip &#228;hneln dem oben angesprochenen &#187;easy proportionality 
problem&#171; und w&#228;ren von den hier untersuchten Prinzipien des HVR wohl am ehesten durch AWS erf&#252;llbar, die
hier ihre Vorteile ausspielen k&#246;nnten, die im voranstehenden Kapitel zum Unterscheidungsgebot aufgef&#252;hrt
worden sind.
Allerdings ist diese &#220;berlegung eher theoretischer Natur, da die hier zum Zwecke der Analyse
vorgenommene Trennung in drei Abw&#228;gungsschritte zu Unterscheidungsgebot, Verh&#228;ltnism&#228;&#223;igkeit und Vorsorgeprinzip 
in der Praxis nicht aufrechterhalten werden kann. Im Verlauf der Planung bis zur tats&#228;chlichen Ausf&#252;hrung
eines Angriffs k&#246;nnen sich wesentliche Umst&#228;nde jederzeit auf unvorhersehbare Weise &#228;ndern, die dessen
Konformit&#228;t mit den v&#246;lkerrechtlichen Anforderungen infrage stellen. In einem solchen Fall m&#252;ssten Angriffe
abgebrochen werden k&#246;nnen. Daher ist eine integrierte Bewertung aller drei Aspekte unmittelbar vor der
Ausl&#246;sung eines Angriffs unerl&#228;sslich.
Staatenverantwortlichkeit
Die fundamental wichtige Frage, wer die Verantwortung tr&#228;gt, wenn ein AWS einen v&#246;lkerrechtswidrigen
Angriff durchf&#252;hrt, wird kontrovers diskutiert. Oft wird hier eine Verantwortungsl&#252;cke diagnostiziert (Sparrow
2007). Die Debatte dar&#252;ber wird in Kapitel 8.3 ausf&#252;hrlich dargestellt. An dieser Stelle soll sie lediglich aus der
Perspektive der Staatenverantwortlichkeit im Kontext des HVR beleuchtet werden.
Allgemein gilt, dass Konfliktparteien f&#252;r jegliche Handlungen verantwortlich sind, die von Personen
begangen werden, die Teil ihrer Streitkr&#228;fte sind (Artikel 91 ZP I).81 Da AWS keine Personen, sondern Maschinen
sind, k&#246;nnte man sich auf den simplen Standpunkt stellen, dass f&#252;r von AWS begangene Handlungen der
zugeh&#246;rige Staat demzufolge nicht verantwortlich ist.
Diese Haltung ist allerdings nicht &#252;berzeugend, denn das Ingangsetzen eines AWS durch einen
menschlichen Bediener w&#252;rde zweifellos eine Handlung darstellen, f&#252;r die der zugeh&#246;rige Staat in letzter Konsequenz
die Verantwortung zu tragen h&#228;tte.82 Dass die Folgen dieser Handlung nicht vollst&#228;ndig &#252;berschaubar sind, da
das Verhalten eines AWS dem Prinzip nach zumindest punktuell unvorhersehbar ist, entlastet den menschlichen
Verantwortungstr&#228;ger nicht. Denn &#187;wenn tats&#228;chlich die Beziehung zwischen dem menschlichen Handeln und
der Wirkung, die &#252;ber autonome Waffensysteme erfolgt, sich so weit erstreckt, dass eine
Verantwortlichkeitsbeziehung v&#246;llig willk&#252;rlich wirkt, dann ist die ethische Frage wohl weniger die, ob hier eine
Verantwortlichkeitsl&#252;cke entsteht, sondern vielmehr die, ob es verantwortbar ist, einen Prozess in Gang zu setzen, der sich
offenkundig der Kontrolle g&#228;nzlich entzieht&#171; (Koch/Rinke 2017, S. 159). Somit w&#228;re eine durch ein AWS
begangene Verletzung des V&#246;lkerrechts dem Staat zuzurechnen, dessen Streitkr&#228;fte das Ger&#228;t eingesetzt haben
(AIV/CAVV 2015, S. 27 ff.). Ob diese Argumentation im internationalen Rahmen greift, ist derzeit noch
strittig.
Ebenso verh&#228;lt es sich mit der Frage, ob es f&#252;r die Verantwortlichkeit und letzten Endes die Haftbarkeit
des Staates f&#252;r Sch&#228;den darauf ankommt, dass die unmittelbare Handlung vors&#228;tzlich oder zumindest fahrl&#228;ssig
81 Artikel 91 ZP I im Original: &#187;A Party [...] shall be responsible for all acts committed by persons forming part of its armed forces.&#171;
82 Nach Ma&#223;gabe der Artikel 4 bzw. 86 Absatz 2 ZP I, die die Verantwortung des Staates f&#252;r Handlungen seiner Organe und die
Verantwortung eines Kommandierenden f&#252;r die Handlungen eines Untergebenen regeln.
begangen wurde.83 Wenn dies bejaht wird, erg&#228;ben sich insbesondere im Falle der Fahrl&#228;ssigkeit
Schwierigkeiten, die analog bei der strafrechtlichen Verantwortlichkeit von einzelnen Personen greifen: Bei
Fahrl&#228;ssigkeitsdelikten ist n&#228;mlich Vorhersehbarkeit eine Voraussetzung f&#252;r die Strafbarkeit. Wenn diese aber aus den
genannten Gr&#252;nden, dass AWS eben unvorhersehbar reagieren k&#246;nnen, nicht gegeben ist, besteht die Gefahr, dass
sich jedes Fehlverhalten von autonomen Systemen letztlich wie h&#246;here Gewalt darstellt, also wie ein Ereignis,
das durch Menschenhand nicht zu verhindern war.
Wie hier Unf&#228;lle oder Fehlfunktionen des Ger&#228;ts aufgrund eines Defektes (die keine Verletzung des HVR
darstellen w&#252;rden, selbst wenn dabei Zivilisten zu Schaden k&#228;men; CCW GGE 2017b, Punkt 30) von
Fehlverhalten im Zuge spezifikationsgem&#228;&#223;en Funktionierens (das hochproblematisch sein kann) unterschieden werden
k&#246;nnten, ist v&#246;llig unklar.
Um diese Problematik zu umgehen, hat das IKRK (1987, RN 3661) in seinem Kommentar zu Artikel 91 
ZP I die M&#246;glichkeit ins Spiel gebracht, auf eine verschuldensunabh&#228;ngige Gef&#228;hrdungshaftung (&#187;no fault or
strict liability&#171;) zur&#252;ckzugreifen.84 Dies ist ein probates Mittel, um das Gefahrenpotenzial bestimmter (im
Prinzip erw&#252;nschter) Handlungen oder Technologien einzud&#228;mmen, und findet in Deutschland z. B. im
Stra&#223;enverkehr, beim Betrieb von Kernkraftwerken oder bei der Produkthaftung Anwendung.
Als Vorbild kann beispielsweise das 1972 geschlossene &#220;bereinkommen &#252;ber die v&#246;lkerrechtliche
Haftung f&#252;r Sch&#228;den durch Weltraumgegenst&#228;nde dienen. Dort hei&#223;t es: &#187;Ein Startstaat haftet unbedingt f&#252;r die 
Leistung von Schadensersatz wegen eines von seinem Weltraumgegenstand auf der Erdoberfl&#228;che oder an
Luftfahrzeugen im Flug verursachten Schadens&#171;.
Da AWS technologische Risiken mit sich bringen, die heute im Einzelnen nicht &#252;berschaubar sind, w&#252;rde
sich die explizite Einf&#252;hrung einer strikten Gef&#228;hrdungshaftung im Zuge eines internationalen Abkommens
anbieten. Gegebenenfalls k&#246;nnte es f&#252;r einzelne Staaten auch in Betracht kommen, als vertrauensbildende
Ma&#223;nahme die &#220;bernahme einer Gef&#228;hrdungshaftung unilateral zu erkl&#228;ren.
83 Die nachstehenden Ausf&#252;hrungen fassen die Analysen von Gei&#223; (2015, S.21 f.) zu diesem Thema zusammen.
84 Gem&#228;&#223; der verschuldensunabh&#228;ngigen Gef&#228;hrdungshaftung (&#187;strict liability&#171; in der engliscchen Rechtsterminologie) kann eine
Person f&#252;r einen Sachverhalt verantwortlich sein, den sie kausal nicht einmal beeinflussen konnte &#8211; oder zumindest nicht
unmittelbar beeinflussen konnte (dazu und zum Folgenden Koch/Rinke 2017, S. 159). Ein Verschulden im strengen Sinne liegt also nicht
vor. In manchen F&#228;llen widerspricht die Gef&#228;hrdungshaftung unseren Intuitionen, aber in manchen F&#228;llen scheint sie der einzige
Weg zu sein, rechtliche Verantwortlichkeit zu sichern. Die Person, die absehbar nach der Gef&#228;hrdungshaftung verantwortlich
gemacht w&#252;rde, steht somit unter gro&#223;em Druck, im Vorfeld daf&#252;r zu sorgen, dass keine Fehler geschehen.
Kasten 7.1 &#187;Boxed autonomy&#171;
Aus den vorstehenden Ausf&#252;hrungen wird deutlich, dass erhebliche Zweifel angebracht sind, ob AWS den
vom HVR gestellten Anforderungen, aber auch ganz allgemein den h&#246;chsten Standards an Zuverl&#228;ssigkeit 
und Sicherheit unter allen m&#246;glichen operationellen und Umweltbedingungen gen&#252;gen k&#246;nnen. So 
schlussfolgern beispielsweise zwei Beratungsgremien der niederl&#228;ndischen Regierung in einem
gemeinsamen Bericht: &#187;It is clear from the above that humans will remain responsible for making assessments
regarding distinction, proportionality and precaution for at least the next 10 years. The deployment of
autonomous weapons will only be permitted in cases where it is almost certain that IHL [International
Humanitarian Law] will not be violated&#171; (AIV und CAVV 2015, S. 26).
Zurzeit wird in Fachkreisen unter dem Schlagwort &#187;boxed autonomy&#171; intensiv diskutiert, ob eine strikte
r&#228;umliche und zeitliche Beschr&#228;nkung des Einsatzgebietes eines AWS die zuvor ausgef&#252;hrten Kritikpunkte
nicht zumindest deutlich entsch&#228;rfen k&#246;nnte (iPRAW 2017a; Sparrow 2016, S. 100 ff.). Die Bewertung des
Kontexts und die Entscheidung, ob ein Angriff dem HVR gen&#252;gt, w&#252;rden in Bezug auf ein begrenztes
Operationsgebiet und einen bestimmten Zeitraum quasi auf Vorrat von einem menschlichen Kommandeur
getroffen. Das AWS k&#246;nnte sodann in diesem vorgegebenen Rahmen agieren. Dies k&#246;nnte vor allem f&#252;r
bestimmte Operationsgebiete infrage kommen, bei denen nicht mit der Anwesenheit von Zivilisten zu rechnen
ist: z. B. auf hoher See, tief unter Wasser, im Luftraum von Flugverbotszonen oder im Weltraum. Ob es
ausreicht, Zivilisten vor einem Angriff lediglich zu warnen und gen&#252;gend Zeit zum Abziehen zu gew&#228;hren,
ist dagegen umstritten (Sparrow 2016, S. 103).
Beispielsweise kann man die Funktionsweise des bei der Bundeswehr g&#228;ngigen Flugabwehrsystems
&#187;MANTIS&#171; im automatischen Modus als &#187;boxed autonomy&#171; auffassen (Kap. 4.1.2). Der Operationsraum
erstreckt sich hier auf die Reichweite der Sensoren, einen Radius von etwa 20 km, und die Zeit kann vom
menschlichen Bediener durch An- und Abschalten des automatischen Modus bestimmt werden. Ein anderes
Beispiel sind Operationen auf rein milit&#228;risch genutztem Gebiet wie etwa innerhalb eines Hauptquartiers.
Diese sind vergleichbar mit dem Artilleriebeschuss eines als milit&#228;risches Ziel ausgewiesenen Gebiets
(Delegation der USA zur CCW GGE 2018j, S. 3).
Eine ungekl&#228;rte Frage ist allerdings, wie weit das Gebiet (in r&#228;umlicher und zeitlicher Ausdehnung)
denn sein d&#252;rfte, um erstens sicher zu sein, dass das AWS sich in der vorher bestimmten Weise verh&#228;lt (d. h. 
nicht &#187;aus dem Ruder l&#228;uft&#171;), und zweitens, dass keine Ver&#228;nderungen des Kriegsgeschehens bzw. im
Operationsgebiet den Angriff obsolet machen. Was w&#228;re beispielsweise, wenn in unvorhergesehener Weise doch
Zivilisten im Operationsgebiet auftauchen oder wenn das anvisierte Ziel sich ergeben will? Hier w&#252;rde sich
ein erheblicher Graubereich auftun. Hinzu kommen Zweifel, wie realistisch die Beschr&#228;nkung auf einen
Boxed-Autonomy-Betriebsmodus angesichts heute dominierender Konfliktformen w&#228;re. Gerade in komplexen
und un&#252;bersichtlichen Situationen sind menschliche Soldaten ja besonders gef&#228;hrdet, wodurch ein sehr hoher
Anreiz besteht, sie durch AWS zu ersetzen (Gei&#223; 2015, S. 17).

        
 
 
    
     
    
     
   
     
  
  
 
   
   
     
 
     
      
  
 
 
     
  
      
   
 
      
   
  
  
  
   
  
    
   
  
 
  
     
   
    
      
    
                                              
               
        
    
    
  
  
8
Ethische Fragestellungen im Kontext autonomer Waffensysteme
Obwohl noch gar nicht abzusehen ist, wann autonome Waffensysteme einsatzf&#228;hig sein werden, wird bereits
seit mindestens 10 Jahren intensiv &#252;ber ihre ethischen Implikationen diskutiert &#8211; und das nicht nur in
Fachkreisen, sondern zunehmend auch in der &#214;ffentlichkeit.85 Die Debatte schlie&#223;t dabei zum Teil an normative Fragen
an, die sich in ganz &#228;hnlicher Weise auch in zivilen Anwendungskontexten stellen (wie etwa der Pflege; TAB 
2016, S. 146 ff.), die ebenfalls von Autonomisierungstendenzen betroffen sind: Gibt es Kernbereiche des
Menschlichen, die es vor maschinellen Zugriffen zu sch&#252;tzen gilt? Wer ist f&#252;r Sch&#228;den verantwortlich, die von
autonom agierenden Robotern verursacht werden? Der besondere Einsatzzweck autonomer Waffensysteme,
n&#228;mlich die milit&#228;rische Gewaltanwendung, bringt es allerdings mit sich, dass sich die ethischen
Herausforderungen zunehmender maschineller Autonomie auf eine Weise zuspitzen, wie es im zivilen Bereich nicht der
Fall ist. Der Einsatz milit&#228;rischer Kampfroboter stellt gewisserma&#223;en ein Extremszenario der Entgrenzung von
Mensch und Maschine dar, das eine Frage von existenzieller Tragweite aufwirft: Sollen Maschinen &#252;ber Leben
oder Tod von Menschen entscheiden d&#252;rfen?
Klar ist, dass sich mit dem Auftauchen autonomer Waffensysteme nicht nur eine &#187;echte Revolution und
[ein] Paradigmenwechsel im Bereich der Milit&#228;rtechnologie&#171; (Gei&#223; 2015, S. 3) ank&#252;ndigt, sondern
grunds&#228;tzliche normative Fragen zu den Grenzen maschineller Autonomie aufgeworfen werden. Ethische Argumente und
Erw&#228;gungen sind deshalb in der AWS-Debatte von besonderem Gewicht, auch und gerade in den politischen
Auseinandersetzungen, die derzeit auf internationaler Ebene &#252;ber ein Verbot solcher Waffensysteme gef&#252;hrt
werden. Es ist in diesem Zusammenhang wichtig, darauf hinzuweisen, dass nicht die technisch vermittelte
T&#246;tung von Menschen das ethisch Brisante an AWS ist &#8211; dieser Aspekt ist selbstverst&#228;ndlicher Bestandteil
moderner Kriegsf&#252;hrung. Kritisch ist vielmehr der Umstand, dass Waffensysteme am Horizont auftauchen, die
menschlicher Kontrolle weitgehend entzogen sind, also autonom agieren, und zwar insbesondere in Bezug auf
Zielauswahl sowie den Einsatz letaler Gewalt.
Im Folgenden wird, basierend auf dem Gutachten von Koch und Rinke (2017), die weitverzweigte ethische
Debatte &#252;ber AWS dargestellt. Dabei ist es alleine schon aus Platzgr&#252;nden nicht m&#246;glich, alle Erw&#228;gungen und
&#220;berlegungen umfassend abzuhandeln. Stattdessen werden drei wichtige Diskussionsstr&#228;nge herausgegriffen,
die sich aus unterschiedlichen Blickwinkeln mit der Frage befassen, ob und ggf. inwiefern die eigentliche
Bestimmung von AWS, n&#228;mlich die autonome Anwendung t&#246;dlicher Gewalt, moralisch zul&#228;ssig erscheint. In
einem ersten Schritt werden einflussreiche Argumente betrachtet, die sich mit den m&#246;glichen Auswirkungen auf
die Ethik der Kriegsf&#252;hrung und insbesondere den humanit&#228;ren Folgen besch&#228;ftigen. Anschlie&#223;end geht es um
die sehr grunds&#228;tzliche Frage, ob es &#252;berhaupt mit der Menschenw&#252;rde zu vereinbaren ist, die Entscheidung 
&#252;ber Leben und Tod an Maschinen zu &#252;bertragen. Schlie&#223;lich stehen Bef&#252;rchtungen im Fokus, der Einsatz von
autonomer Waffentechnologie schaffe eine Verantwortungsl&#252;cke, die mit rechtlichen sowie moralischen
Rechenschaftspflichten nicht in Einklang zu bringen ist.
8.1 AWS und die Ethik des Krieges
Im Zentrum der ethischen Debatte &#252;ber AWS stehen Fragen, die die Implikationen dieser neuen
Waffentechnologie f&#252;r die Ethik des Krieges betreffen. Eine der wichtigsten theoretischen Bezugspunkte dabei ist die
sogenannte Lehre vom gerechten Krieg,86 die antike Wurzeln hat und im Laufe der abendl&#228;ndischen Geschichte
(wesentliche Impulse lieferte u. a. Thomas von Aquin) zu einem detaillierten, wenn auch keineswegs
einheitlichen Regelwerk ausgearbeitet wurde, das sich mit den legitimen und illegitimen Formen zwischenstaatlicher
85 So haben Tausende Forscher aus den Bereichen KI und Robotik einen offenen Brief aus dem Jahr 2015 unterschrieben, in dem ein
Verbot von autonomen Waffensystemen gefordert (ca. 4.502 aus KI/Robotik und 26.215 andere; Stand Oktober 2020; FLI o. J.a).
86 Die Lehre vom gerechten Krieg dominiert nahezu unangefochten die theoretische Debatte &#252;ber die ethisch gerechtfertigte
Anwendung milit&#228;rischer Gewalt, die stark vom englischen Raum bestimmt ist (Koch/Rinke 2017, S.30). Andere Denkschulen, etwa die
Lehre vom gerechten Frieden, wie sie die beiden deutschen Gro&#223;kirchen in ihren Lehrtexten vertreten (Hoppe/Werkner 2017), sind 
allenfalls von marginaler Bedeutung.
Gewaltanwendung befasst (dazu und zum Folgenden Koch/Rinke 2017, S. 29 u. 32).87 Ziel ist es, Kriterien zu
finden, unter denen die Anwendung milit&#228;rischer Gewalt als Mittel der Konfliktl&#246;sung im &#228;u&#223;ersten Fall
gerechtfertigt werden kann.88 Die Sto&#223;richtung ist somit streng genommen weniger eine kriegsethische und mehr
eine friedensethische: Das Bestreben ist die Einschr&#228;nkung milit&#228;rischer Gewalt und nicht ihre Legitimierung.
Wesentliche Postulate der Lehre vom gerechten Krieg haben deshalb, wie unten zu sehen sein wird, ihren
Niederschlag im humanit&#228;ren V&#246;lkerrecht gefunden. Die diesbez&#252;gliche ethische Debatte ist aus diesem Grund
sehr eng mit der v&#246;lkerrechtlichen verkn&#252;pft und kaum trennscharf von dieser abgrenzbar.
8.1.1 Die Lehre vom gerechten Krieg
Die Lehre vom gerechten Krieg besteht aus mindestens drei Teilen, die sich auf die haupts&#228;chlichen Aspekte
einer kriegerischen Auseinandersetzung beziehen: das Recht zum Krieg (&#187;ius ad bellum&#171;), das Recht im Krieg
(&#187;ius in bello&#171;) sowie das Recht nach dem Krieg (&#187;ius post bellum&#171;). Diese Teilaspekte sind wiederum mit
jeweils unterschiedlichen, wenn auch eng miteinander verflochtenen ethischen Fragestellungen verkn&#252;pft, die
sich mit Blick auf AWS folgenderma&#223;en grob charakterisieren lassen:
1. Beim &#187;ius ad bellum&#171; geht es um die Frage, wann es gerechtfertigt ist, einen Krieg zu beginnen. In der
Debatte wurden zentrale Anforderungen formuliert wie das Vorliegen eines legitimen, gerechten
Kriegsgrundes sowie das Scheitern anderweitiger M&#246;glichkeiten der Konfliktl&#246;sung. Dass Krieg nur als Ultima
Ratio zul&#228;ssig ist, wird im Zusammenhang mit AWS besonders kontrovers diskutiert. Bef&#252;rchtet wird,
dass die Hemmschwelle zur Gewaltanwendung sinken und es zu einer &#187;Normalisierung des Krieges&#171;
(Franke/Leveringhaus 2015) kommen kann, sollten mit der Verf&#252;gbarkeit von AWS Kriege risikoloser
sein und zu geringeren gesellschaftlichen Kosten (vor allem zu weniger toten und verletzten Soldaten)
f&#252;hren (Kap. 6.1; Gei&#223; 2015, S. 13 f.; Koch/Rinke 2017, S. 48 ff.).
2. Das &#187;ius in bello&#171; besch&#228;ftigt sich mit den Anforderungen an eine ethische Kriegsf&#252;hrung. Im Fokus steht
hierbei die Frage, welche Mittel bei milit&#228;rischen Konfliktl&#246;sungen erlaubt sind und wie diese eingesetzt
werden d&#252;rfen. Zu beachten sind dabei vor allem drei, auch v&#246;lkerrechtlich verankerte Gebote (Kap. 7.2).
Erstens: Es d&#252;rfen nur Kombattanten, nicht jedoch die Zivilbev&#246;lkerung direkt angegriffen werden
(Unterscheidungsgebot). Zweitens: Der Einsatz von Waffengewalt muss verh&#228;ltnism&#228;&#223;ig sein, d. h., tote
Zivilisten und anderweitige Kollateralsch&#228;den m&#252;ssen in einem angemessenen Verh&#228;ltnis zum milit&#228;rischen
Nutzen stehen (Verh&#228;ltnism&#228;&#223;igkeitsgebot). Und drittens: Alles Erforderliche muss getan werden, um die 
Zivilbev&#246;lkerung auch vorsorglich zu sch&#252;tzen (Vorsorgeprinzip). Kampfmittel, die diese Kriterien
systematisch verletzen, gelten als ethisch unzul&#228;ssig und sind v&#246;lkerrechtswidrig. Insofern ist die Frage,
inwieweit AWS in der Lage sind, diese Kriterien einzuhalten, sowohl von zentraler v&#246;lkerrechtlicher als auch
ethischer Bedeutung &#8211; die entsprechende Debatte wird in Kapitel 8.1.2 genauer dargestellt (bzw. aus
v&#246;lkerrechtlicher Sicht in Kapitel 7.2).
3. Das &#187;ius post bellum&#171; befasst sich mit der Zeit nach der kriegerischen Auseinandersetzung, speziell mit 
all jenen Aktivit&#228;ten, die f&#252;r den &#220;bergang in eine stabile Nachkriegsordnung sowie f&#252;r deren
Aufrechterhaltung erforderlich sind (Frank 2009). Die dazugeh&#246;rigen Fragen sind daher eng mit denen des &#187;ius ad
bellum&#171; sowie &#187;ius in bello&#171; verkn&#252;pft, da nicht zuletzt von der Legitimit&#228;t des Krieges sowie der Wahl
der kriegerischen Mittel wesentlich abh&#228;ngt, ob sich nach Beendigung der Kriegshandlungen eine
nachhaltige Friedensperspektive er&#246;ffnet (Koch/Rinke 2017, S. 155). So wird auch mit Blick auf AWS
diskutiert, inwiefern deren Einsatz friedensf&#246;rderlich sein k&#246;nnte oder nicht (z. B. Leveringhaus 2016, S. 20).
Allerdings ist festzustellen, dass das &#187;ius post bellum&#171; generell zu den eher vernachl&#228;ssigten Aspekten im
Rahmen der Lehre vom gerechten Krieg geh&#246;rt und insofern auch in der ethischen Debatte &#252;ber AWS nur
eine Nebenrolle spielt (Koch/Rinke 2017, S. 155).
Fragen des &#187;ius in bello&#171; nehmen in der ethischen AWS-Debatte bei Weitem den gr&#246;&#223;ten Raum ein, weshalb
sie im Folgenden auch vertieft behandelt werden. Diese Sonderstellung hat ihren Grund nicht zuletzt in dem
87 Traditionell geh&#246;ren zum Gegenstandsbereich der Lehre vom gerechten Krieg bewaffnete Konflikte, die sich zwischen zwei
souver&#228;nen Staaten abspielen. Andere Formen milit&#228;rischer Gewalt, B&#252;rgerkriege etwa, nehmen hingegen eher eine Randstellung ein 
(Frank 2009, S.734).
88 Theoretische Gegenpositionen sind der Bellizismus, der kriegerische Gewalt grunds&#228;tzlich bef&#252;rwortet, sowie der Pazifismus, der
sie a priori verurteilt.
Umstand, dass das Spezifikum von AWS, n&#228;mlich die Entscheidungsgewalt &#252;ber den potenziell t&#246;dlichen
Waffeneinsatz au&#223;erhalb menschlicher Kontrolle zu stellen, sich nur im Rahmen von direkten Kriegshandlungen
manifestieren kann &#8211; weshalb sich mithin auch die damit verkn&#252;pften ethischen Fragen prim&#228;r auf das &#187;ius in
bello&#171; beziehen (Sparrow 2016).
Mit Blick auf das &#187;ius ad bellum&#171; hingegen werfen AWS keine moralischen Probleme auf, die sie in
besonderer Weise von anderen modernen Waffensystemen abheben. So wird die (im &#220;brigen eher
sicherheitspolitisch als ethisch relevante) Frage, ob durch AWS die Hemmschwelle f&#252;r kriegerische Gewaltanwendung sinkt,
derzeit bereits im Zusammenhang mit dem sogenannten Drohnenkrieg intensiv diskutiert (Franke/Leveringhaus
2015, S. 307 f.; Gei&#223; 2015, S. 13). Eine bestimmte Sonderrolle nimmt ein Szenario ein, in dem Aspekte des
&#187;ius ad bellum&#171; sowie des &#187;ius in bello&#171; gewisserma&#223;en verschwimmen und das von Altmann (2017, S. 799)
wie folgt beschrieben wird: &#187;In einer schweren Krise w&#252;rden sich zwei Flotten autonomer Waffensysteme
intensiv gegenseitig beobachten; unklare Ereignisse k&#246;nnten als Angriff missverstanden werden, zum
vermeintlichen Gegenangriff f&#252;hren und damit eine milit&#228;rische Eskalation ausl&#246;sen.&#171; Hier ist die Erh&#246;hung der
Kriegswahrscheinlichkeit direkte Folge einer Autonomisierung der Kriegsf&#252;hrung und des damit verbundenen
menschlichen Kontrollverlustes.89 Das Szenario verdeutlicht, dass Aspekte des &#187;ius ad bellum&#171; sowie des &#187;ius
in bello&#171; h&#228;ufig nicht isoliert voneinander, sondern vielmehr in ihrer wechselseitigen Verschr&#228;nkung zu
betrachten sind (Koch/Rinke 2017, S. 39).
Inwiefern AWS den Regeln des &#187;ius post bellum&#171; gem&#228;&#223; eine echte Friedensperspektive zu bef&#246;rdern
imstande sind, h&#228;ngt wiederum entscheidend davon ab, ob und inwiefern sie &#252;berhaupt die Bestimmungen des
humanit&#228;ren V&#246;lkerrechts einhalten und sich folglich in Kampfsituationen ethisch angemessen verhalten
k&#246;nnen. Die diesbez&#252;gliche Debatte, die im Folgenden genauer beleuchtet werden soll, kreist ma&#223;geblich um
Argumente, die von Ronald C. Arkin, Robotikforscher am Georgia Institute of Technology, vorgebracht wurden.
8.1.2 Erm&#246;glichen AWS eine ethischere Kriegsf&#252;hrung?
Aus milit&#228;rischer Sicht bieten AWS, es wurde bereits darauf hingewiesen (Kap. 5), etliche Vorteile: Computer
sind Menschen bei der Verarbeitung gro&#223;er Datenmengen deutlich &#252;berlegen, sodass von ausgereiften
Kampfrobotern prinzipiell schnelle Reaktionszeiten sowie eine effiziente und pr&#228;zise Kampff&#252;hrung zu erwarten w&#228;re. 
Verf&#252;gen diese Systeme zudem &#252;ber ausreichende Autonomie, sodass sie auch in unzug&#228;nglichen Gebieten
sowie weitgehend unabh&#228;ngig von menschlicher Intervention einsatzf&#228;hig w&#228;ren, k&#246;nnten sie die milit&#228;rische
Reichweite bzw. die Gefechtsdistanz substanziell erweitern. Dies wiederum w&#252;rde die M&#246;glichkeit er&#246;ffnen, 
&#187;die Gefahr [eigener] menschlicher Verluste&#171; (Gei&#223; 2015, S. 4) zu reduzieren, indem &#187;jene Aufgaben von
Robotern &#252;bernommen werden, die den Soldaten einer zu gro&#223;en Gefahr aussetzen&#171; (Dickow 2015, S. 12). Dieses
Argument hat nun nicht nur eine milit&#228;rische, sondern durchaus eine ethische Komponente, da die Verringerung
menschlicher Verluste moralisch in den meisten F&#228;llen geboten scheint. Vor diesem Hintergrund hat der
Philosoph Bradley Strawser (2010) im Kontext der Drohnendebatte die Auffassung vertreten, dass es unter
Umst&#228;nden sogar die moralische Pflicht eines Staates sein kann, Systeme wie Drohnen einzusetzen, welche die eigenen
Soldaten unn&#246;tigen Risiken entziehen. Ganz &#228;hnlich, wenn auch deutlich breiter, argumentiert mit Blick auf
AWS Ronald C. Arkin. Die von ihm vertretene und kontrovers diskutierte These lautet, dass AWS nicht nur in
der Lage sein k&#246;nnten, das Leben der eigenen Soldaten zu sch&#252;tzen, sondern dass sie insgesamt eine humanere
Kriegsf&#252;hrung erm&#246;glichen.
Die Position Arkins
Arkin spielt insofern eine &#187;singul&#228;re Rolle&#171; (Koch/Rinke 2017, S. 70), als er praktisch der einzige namhafte
Experte ist, der sich im Rahmen der ethischen Debatte dezidiert f&#252;r AWS ausspricht (z. B. Arkin 2010).90 Seine 
Argumentation ist allerdings weniger ethisch, denn rechtlich orientiert: Arkins zentrale These lautet, dass AWS
technisch so entworfen werden k&#246;nnen, dass sie die zuvor beschriebenen Regeln des V&#246;lkerrechts insgesamt
89 Laut dem Philosophen Peter Asaro (2008) w&#228;re ein solch versehentlich ausgel&#246;ster Krieg (&#187;accidental war&#171;) im &#220;brigen nicht
rechtm&#228;&#223;ig, da er auf keinem legitimen Kriegsgrund basiert.
90 Ebenfalls ein starker Bef&#252;rworter von AWS ist der deutsche Philosoph Vincent C. M&#252;ller (2016).
besser einzuhalten verm&#246;gen als menschliche Soldatinnen und Soldaten. Sollte dies der Fall sein, w&#228;re ihr
Einsatz moralisch geradezu geboten, so Arkin (2013, S. 7), da Kriege dadurch insgesamt humaner und mithin
ethischer gestaltet werden k&#246;nnten.
Arkin (2013, S. 9) selber hat immer wieder darauf hingewiesen, dass er AWS nicht per se bef&#252;rwortet und
insofern auch kein prinzipieller Gegner eines Verbots ist. Sein Standpunkt ist vielmehr gepr&#228;gt von einem
grundlegenden Pessimismus, sowohl was die &#220;berwindbarkeit von kriegerischen Auseinandersetzungen angeht 
als auch hinsichtlich der F&#228;higkeit des Menschen, sein regelm&#228;&#223;ig von Grausamkeit und Verbrechen gepr&#228;gtes
Verhalten im Krieg im Sinne des humanit&#228;ren V&#246;lkerrechts nachhaltig zu ver&#228;ndern (Koch/Rinke 2017, S. 71). 
Es sind also letztlich Negativaspekte wie die Unvermeidlichkeit des Krieges sowie vor allem die gut
dokumentierte Unzul&#228;nglichkeit des Menschen auf dem Schlachtfeld, die laut Arkin (2010, S. 334 ff.) f&#252;r AWS sprechen
und diese insgesamt als das kleinere &#220;bel erscheinen lassen. Konkret weist Arkin (z. B. 2010, S. 333, u. 2013,
S. 6 ff.) u. a. auf folgende Vorteile autonomer Kampfsysteme (gegen&#252;ber menschlichen Kombattanten) hin:
&#8250; Kein Selbsterhaltungstrieb: Anders als Menschen setzen AWS auf dem Schlachtfeld nicht ihr Leben aufs
Spiel, wodurch sie sich deutlich zur&#252;ckhaltender verhalten k&#246;nnen. Dies gilt insbesondere hinsichtlich der
Anwendung von &#252;berm&#228;&#223;iger und unverh&#228;ltnism&#228;&#223;iger Gewalt, die bei Soldatinnen und Soldaten h&#228;ufig
zum Selbstschutz erfolgt.
&#8250; Fehlende Emotionen: Bei Maschinen besteht nicht die Gefahr, dass das Verhalten durch starke Emotionen
wie Stress, Angst oder Zorn beeinflusst wird, die Menschen in Gefechtssituationen oftmals zu einem
unethischen Verhalten (z. B. aus Rache oder Frust) provozieren.
&#8250; Bessere Sensorik: Die Wahrnehmungsf&#228;higkeiten von Menschen sind aus biologischen Gr&#252;nden
eingeschr&#228;nkt und technisch nicht beliebig erweiterbar. Dahingegen sind den sensorischen F&#228;higkeiten von
AWS prinzipiell keine technischen Grenzen gesetzt, zudem lassen sie sich spezifisch auf die im Gefecht
ben&#246;tigten Anforderungen anpassen. AWS sollten deshalb besser als Menschen in der Lage sein
(zumindest potenziell), sich in komplexen und un&#252;bersichtlichen Gefechtssituationen zurechtzufinden.
&#8250; &#220;berlegene kognitive Fertigkeiten: Roboter sind weit besser darin, umfangreiches Datenmaterial zu
prozessieren und auszuwerten &#8211; nicht nur, was die Geschwindigkeit angeht, sondern auch hinsichtlich der
Integration unterschiedlicher Datentypen. Au&#223;erdem leiden sie nicht an kognitiver Voreingenommenheit
und Verzerrung, von der die menschliche Urteilskraft gerade in hierarchisch gepr&#228;gten
Entscheidungssituationen oftmals befallen ist.91 
Arkin kommt vor diesem Hintergrund zu dem bereits erw&#228;hnten Schluss, dass AWS gegen&#252;ber dem
beklagenswerten Status quo der Kriegsrealit&#228;t grunds&#228;tzliche Vorteile versprechen, und zwar nicht nur in milit&#228;rischer,
sondern auch und vor allem in humanit&#228;rer Hinsicht. So sind sie ihm zufolge durch die zuvor genannten
Merkmale insgesamt besser als menschliche K&#228;mpfer in der Lage, zwischen Kombattanten und unbeteiligten
Zivilisten zu unterscheiden (Unterscheidungsgebot) und Waffengewalt so pr&#228;zise und gleichzeitig zur&#252;ckhaltend
einzusetzen, dass unverh&#228;ltnism&#228;&#223;ige Kollateralsch&#228;den ausgeschlossen sind (Verh&#228;ltnism&#228;&#223;igkeitsgebot).
Wohlgemerkt, Arkin (2015, S. 46) erwartet von AWS diesbez&#252;glich keine Perfektion. Er ist jedoch der festen
&#220;berzeugung, dass sie dem Menschen in dieser Hinsicht prinzipiell weit &#252;berlegen sein k&#246;nnen, besonders unter
den zeitkritischen und &#252;beraus komplexen Bedingungen des modernen Schlachtfeldes.
Kasten 8.1 Zwei Grundmodelle ethischen Argumentierens
In der Ethik lassen sich grunds&#228;tzlich folgen- und handlungsorientierte Argumentationen unterscheiden. Die
jeweiligen Positionen werden als Konsequentialismus bzw. Deontologie bezeichnet:
&#8250; Konsequentialistische Argumentationsmuster bewerten eine Handlung prim&#228;r danach, wie ihre
Konsequenzen ausfallen. Die bekannteste konsequentialistische Theorie ist der Utilitarismus, der Handlungen
danach bemisst, wie sie zur Vermehrung des allgemeinen Nutzens beitragen.
Arkin (2015, S.46) spricht in diesem Zusammenhang vom psychologischen Ph&#228;nomen des &#187;scenario fulfilment syndrome&#171;:
Informationen, die nicht in ein einmal gew&#228;hltes Interpretationsschema passen, werden ignoriert. Es wird vermutet, dass der Abschuss
des Iran-Air-Flugs 655 durch das US-Kriegsschiff &#187;USS Vincennes&#171; 1988 auf dieses Ph&#228;nomen zur&#252;ckzuf&#252;hren sei (Wikipedia
2006b).
91
&#8250; Deontologische Argumentationsmuster verweisen dagegen auf den Handlungstyp selbst und bewerten
dessen inh&#228;rente Zu- oder Unzul&#228;ssigkeit. Einfache Beispiele sind S&#228;tze wie &#187;Du sollst nicht t&#246;ten!&#171; &#8211;
ein Gebot, dessen Richtigkeit aus deontologischer Perspektive unabh&#228;ngig davon gilt, welche Folgen
sich aus einer T&#246;tung ergeben und ob darunter auch g&#252;nstige Folgen zu finden sind.
Wie sich die beiden Argumentationsformen genau voneinander abtrennen lassen, ist Gegenstand vielf&#228;ltiger
Diskussionen. So ist zwar keine ethische Theorie gegen&#252;ber den Handlungsfolgen v&#246;llig indifferent, d. h., 
auch deontologische Ans&#228;tze beziehen Folgenabw&#228;gungen in die Rechtfertigungsargumente mit ein.
Allerdings gibt es &#8211; wie der erw&#228;hnte Utilitarismus &#8211; rein konsequentialistische Herangehensweisen.
Quelle: Koch/Rinke 2017, S. 137 f.
Deutlich wird, dass Arkins Argumentation utilitaristisch ausgerichtet ist, insofern seine ethische Beurteilung
von AWS prim&#228;r auf den potenziellen Nutzen rekurriert, der von ihrem Einsatz zu erwarten ist. Demnach gelten
AWS u. a. deshalb als ethisch vorteilhaft, da sie dabei helfen k&#246;nnen, zivile Opfer zu reduzieren. Ethische
Argumentationen dieser Art machen die Frage der Zul&#228;ssigkeit der Nutzung eines technischen Instruments
ausschlie&#223;lich von den Konsequenzen abh&#228;ngig, die bei der Nutzung des Instruments (bzw. bei Verzicht darauf)
zu erwarten sind (Kasten 8.1).
Das Problem dabei ist, dass diese Konsequenzen nicht nur vielschichtig, sondern oft unklar sind und auf
mehr oder weniger unsicheren Prognosen beruhen; dies gilt besonders f&#252;r Technologien, die sich &#8211; wie AWS &#8211;
erst in einem fr&#252;hen Stadium der Entwicklung befinden (zur Problematik konsequentialistischer Ans&#228;tze vgl.
Koch/Rinke 2017, S. 145 ff.). So ist auch die Aussage, dass der Einsatz von autonomen Waffensystemen zu 
weniger Kollateralopfern f&#252;hren w&#252;rde, derzeit noch ein reines Zukunftsversprechen.
Klar ist: Zum jetzigen Zeitpunkt ist der technologische Entwicklungsstand in den relevanten Bereichen
(KI, Robotik und Sensorik etc.) bei Weitem noch nicht ausgereift genug, um ethisch vertretbare bzw.
v&#246;lkerrechtskonforme Kampfroboter zu konstruieren. Die entsprechenden technischen H&#252;rden h&#228;lt Arkin jedoch f&#252;r
&#252;berwindbar &#8211; vorausgesetzt, es wird ausreichend in FuE investiert. Einem Verbot, wie es derzeit im Rahmen
der CCW diskutiert wird (Kap. 9.2), steht er &#228;u&#223;erst kritisch gegen&#252;ber, weil es genau dies unterbindet: Es sei
voreilig und basiere auf &#187;reiner Angst und Pathos&#171; (&#187;pure fear and pathos&#171;; Arkin 2013, S. 10). Stattdessen
pl&#228;diert Arkin (2015, S. 47) f&#252;r ein Moratorium, das sich auf den Bau und den Einsatz von Kampfrobotern
beschr&#228;nkt. Diese Vorgehensweise w&#252;rde es erm&#246;glichen, FuE zu AWS weiter voranzutreiben und so die
Chancen auszuloten (und sp&#228;ter ggf. zu nutzen), welche die Technologie hinsichtlich einer humaneren Kriegsf&#252;hrung
bietet.
Arkin (2009, S. xii ff.) selbst hat als Robotiker viele Jahre an der Entwicklung &#187;moralischer&#171; Kampfroboter
geforscht, u. a. im Auftrag des US-Verteidigungsministeriums. Seine Arbeiten lassen sich der neuen Disziplin
der Maschinenethik zuordnen, die zum Ziel hat, &#187;k&#252;nstliche Systeme mit der F&#228;higkeit zu moralischem
Entscheiden und Handeln auszustatten&#171; (Misselhorn 2018, S. 29; Kasten 8.2). In diesem Sinne hat Arkin (2008 u.
2009) detaillierte Vorschl&#228;ge gemacht, wie Kampfroboter zu gestalten sind, damit sie sich rechtlich und
moralisch m&#246;glichst untadelig verhalten. Kernst&#252;ck der von ihm propagierten AWS-Architektur ist der &#187;ethical
governor&#171;, der als eine Art k&#252;nstliches Gewissen fungiert und zwischen die deliberativen Komponenten des
Systems sowie dessen ausf&#252;hrende Instanzen geschaltet ist. Zweck des &#187;governor&#171; ist zu pr&#252;fen, ob die vom
System generierten Entscheidungen (zumindest soweit sie die Aus&#252;bung von Waffengewalt betreffen) mit dem
humanit&#228;ren V&#246;lkerrecht und den milit&#228;rischen Einsatzregeln konform gehen (Koch/Rinke 2017, S. 72); falls
dies nicht der Fall ist, k&#246;nnte der &#187;governor&#171; die Ausf&#252;hrung der geplanten Aktion unterbinden. Gleichzeitig
er&#246;ffnet dies grunds&#228;tzlich die M&#246;glichkeit, so Arkin (2009, S. 203 ff.), dass die ethischen Schlussfolgerungen
des Roboters von einem menschlichen Bediener &#252;berwacht werden und so ein gewisses Ma&#223; an menschlicher
Kontrolle sichergestellt wird &#8211; der Bediener k&#246;nnte z. B. immer dann eingreifen, wenn er antizipiert, dass der
Roboter unethisch zu agieren versucht (Koch/Rinke 2017, S. 72).92 
Arkins Perspektive ist also prim&#228;r eine designtechnische, die von dem Bestreben gepr&#228;gt ist, ethische
Erw&#228;gungen so zu formalisieren, dass sie sich maschinell implementieren lassen. Mit diesem maschinenethischen
Vorhaben sind nicht zuletzt umstrittene Annahmen &#252;ber die Natur moralischen Entscheidens und Handelns
Auch der andere Fall ist m&#246;glich: Wenn der &#187;governor&#171; eine Handlung aus ethischen Gr&#252;nden zu unterbinden sucht, so k&#246;nnte der
menschliche Operator deren Ausf&#252;hrung dennoch zulassen und m&#252;sste daf&#252;r dann aber auch letzten Endes die Verantwortung 
&#252;bernehmen.
92
verbunden (Kasten 8.2). Unter anderem ist eine unabdingbare Voraussetzung f&#252;r die Realisierung eines &#187;ethical
governor&#171;, dass sich v&#246;lkerrechtliche Vorgaben, kontextspezifische Einsatzregeln sowie ethische Prinzipien
sinnvoll in Programmcodes abbilden lassen (Arkin 2008, S. 39 ff.). Besonders dieser Anspruch st&#246;&#223;t von
verschiedenen Seiten auf massive Kritik.
Kasten 8.2 Moralische Maschinen? Die neue Disziplin der Maschinenethik
Im Zuge des Fortschritts im Bereich der KI und Robotik hat sich die Maschinenethik in den letzten Jahren
als &#187;neues Forschungsgebiet an der Schnittstelle von Informatik und Philosophie&#171; (Misselhorn 2018, S. 29)
etabliert. Ausgangspunkt maschinenethischer &#220;berlegungen bildet die Beobachtung, dass maschinelle
Systeme immer autonomer werden und also nicht mehr blo&#223; Objekte der Moral sind &#8211; wie herk&#246;mmliche
Maschinen, die sich eindeutig menschlichen Zwecken unterordnen und deshalb &#252;ber keine eigene Moral
verf&#252;gen. Vielmehr treten Maschinen, je eigenst&#228;ndiger sie werden, verst&#228;rkt als Subjekte der Moral in
Erscheinung, d. h. als k&#252;nstliche Akteure, deren Agieren moralische Implikationen hat und entsprechend auch nach 
ethischen Gesichtspunkten beurteilt werden kann (und eventuell sogar muss). Vor diesem Hintergrund
besch&#228;ftigt sich die Maschinenethik sowohl in theoretischer als auch gestalterischer Absicht mit der Moral
autonomer Systeme, u. a. mit dem Ziel, moralische Maschinen zu konstruieren, die sich auch in komplexen
Entscheidungssituationen m&#246;glichst im Einklang mit unseren Wertvorstellungen verhalten (Anderson/
Anderson 2007; Wallach/
Allen 2008). Interessante Anwendungsgebiete sind vor allem jene gesellschaftlichen Bereiche, in denen
autonome Systeme zuk&#252;nftig eng mit Menschen interagieren k&#246;nnten: Beispiele sind etwa die Pflege
(Pflegerobotik), der Verkehr (automatisiertes Fahren) oder das Milit&#228;r (AWS).
Die skizzierte maschinenethische Programmatik ist mit der grundlegenden Frage konfrontiert, ob und
inwiefern es &#252;berhaupt zul&#228;ssig ist, moralische Kategorien wie gut und b&#246;se auf Maschinen zu &#252;bertragen.
Selbst wenn diese scheinbar intelligentes Verhalten zeigen: Ist es nicht so, dass moralisches Handeln die
F&#228;higkeit zur Handlungsurheberschaft, Willensfreiheit und folglich genuine Autonomie im Kant&#8217;schen Sinne
voraussetzt (Kasten 2.1)? Maschinenethiker wie Ronald C. Arkin (2009, S. 37) oder Oliver Bendel (2018)
bestreiten dies nicht. Sie gestehen zu, dass Maschinen nur &#252;ber Autonomie im operationellen Sinne verf&#252;gen
&#8211; und damit nicht &#252;ber eine &#187;vollumf&#228;ngliche moralische Handlungsf&#228;higkeit, wie sie Menschen
typischerweise besitzen&#171; (Misselhorn 2018, S. 31). Auch wenn es sich derzeit bei der maschinellen Moral noch um
eine k&#252;nstliche Moral handelt, also eine reine Simulation menschlicher Moralf&#228;higkeiten (analog zur
k&#252;nstlichen Intelligenz), so w&#228;re es dennoch sinnvoll &#8211; alleine schon aus anwendungspraktischen Gr&#252;nden &#8211;,
teilautonome und autonome Maschinen so zu konstruieren, dass sie mit moralisch problematischen Situationen
umgehen k&#246;nnen.
F&#252;r Kritiker wie Noel Sharkey (2012b, S. 793 ff.) und andere hingegen beruht das maschinenethische
Unterfangen auf einem unzul&#228;ssigen Anthropomorphismus, der von der f&#228;lschlichen Vorstellung ausgeht,
dass Moral etwas ist, was logisch formalisierbar und maschinell implementierbar ist (Royakkers/van Est 
2015, S. 268). Das mag f&#252;r bestimmte ethische Theorien wie den Utilitarismus sogar zutreffen; die
zugrundeliegende F&#228;higkeit zum moralischen Urteil ist jedoch nicht auf explizite Regeln reduzierbar, sondern hat
ihren Ursprung in der Subjektivit&#228;t des Menschen, die das Reflektieren &#252;ber Werte, das Handeln aus
moralischen Gr&#252;nden sowie moralische Emotionen wie Mitgef&#252;hl oder Schuldgef&#252;hle erst erm&#246;glicht (Misselhorn
2018, S. 31 f.). Insofern birgt die anthropomorphisierende Rede von moralischen Maschinen die Gefahr, das
menschliche Fundament zu untergraben, auf dem die Moralit&#228;t &#252;berhaupt erst gr&#252;ndet (Beavers 2011). In 
diesem Sinne konstatiert Bernhard Koch (2017, S. 10): &#187;Die Gefahr in der ganzen Debatte um k&#252;nstliche
Intelligenz besteht meines Erachtens darin, dass wir angefangen haben, die Prozesse von Maschinen mit
Begriffen zu bezeichnen, die wir aus unseren lebensweltlichen Bez&#252;gen kennen [...] und dass wir dann
anfangen, unser eigenes Handeln und Verhalten nach dem Modell der Maschine zu beschreiben.&#171;
Die Gegenargumente
Kaum zu bestreiten ist: Sollte sich zeigen, dass AWS nicht nur dem Schutz der eigenen Soldaten dienen, sondern 
auch dabei helfen, unbeteiligte Dritte zu sch&#252;tzen &#8211; wie Arkin behauptet &#8211;, w&#228;re das sicherlich ein triftiges
ethisches Argument f&#252;r deren Einsatz. Doch kann man Roboter tats&#228;chlich so programmieren, dass sie die
Regeln des &#187;ius in bello&#171; (bzw. des V&#246;lkerrechts) zuverl&#228;ssig einhalten? Zum jetzigen Zeitpunkt ist dies schwierig
zu beurteilen, da es vollautonome Waffensysteme ja noch gar nicht gibt und vieles von der weiteren technischen
Entwicklung abh&#228;ngt. Dennoch wird in der Literatur mehrheitlich die Ansicht vertreten, dass Arkin die
technischen Herausforderungen systematisch untersch&#228;tzt, die mit der Entwicklung und Konstruktion von &#187;
moralischen&#171; Kampfrobotern einhergehen (Koch/Rinke 2017, S. 73 ff.). Dies betrifft vor allem die Unterscheidung
von legitimen und illegitimen Angriffszielen, die ohne ein umfassendes Situationsverst&#228;ndnis nicht angemessen
durchf&#252;hrbar ist (Kap. 7.2). Hinzu kommen die inh&#228;renten Interpretationsspielr&#228;ume, die das humanit&#228;re
V&#246;lkerrecht besonders hinsichtlich der Absch&#228;tzung der Verh&#228;ltnism&#228;&#223;igkeit eines Angriffs aufwirft und die starke
Zweifel wecken, ob sich dessen Regeln so operationalisieren lassen, dass sie von einem Computerprogramm
verarbeitet werden k&#246;nnen (z. B. Gei&#223; 2015, S. 14 ff.; Sharkey 2012b, S. 789).
Vor diesem Hintergrund wird darauf hingewiesen, dass autonome Waffensysteme neue und in ihrem
Ausma&#223; noch nicht klar bestimmbare Risiken schaffen werden (Koch/Rinke 2017, S. 148). Zum Wesenskern von 
AWS geh&#246;rt ja, dass sie sich autonom verhalten und mithin nach Aktivierung menschlicher Kontrolle g&#228;nzlich
entzogen sind. Mit AWS kommt somit ein Element der Unvorhersehbarkeit ins Spiel, das zur Folge hat, dass
inakzeptable Konsequenzen, wie z. B. unverh&#228;ltnism&#228;&#223;ige Kollateralsch&#228;den, nie ganz auszuschlie&#223;en w&#228;ren
(IKRK 2018a, S. 16 f.). Auch die nichtintendierte Ausl&#246;sung eines Krieges oder bewaffneten Konfliktes, z. B. 
durch sensorische Fehlinterpretation eines autonomen Waffensystems (ohne dass ein menschlicher Bediener
noch die M&#246;glichkeit hat einzugreifen), stellt eine ernstzunehmende M&#246;glichkeit dar (Koch/Rinke 2017,
S. 149). Zwar geh&#246;rt die Bestimmung derartiger Risiken streng genommen nicht zum Bereich der Ethik,
sondern ist eine technische bzw. sicherheitspolitische Aufgabe. Mit Blick darauf, dass diese Ger&#228;te auf T&#246;tung hin
ausgelegt sind, erscheint aus ethischer Sicht dennoch zumindest naheliegend, wie Koch und Rinke (2017, S. 148 
u. 158 f.) betonen, auf ein &#187;Ingangsetzen&#171; unkontrollierbarer Vollz&#252;ge aus einem allgemeinem Vorsichtsprinzip
heraus zu verzichten &#8211; und zwar selbst dann, wenn die Eintrittswahrscheinlichkeit inakzeptabler Folgen eher
gering erscheint.93 
Es ist fraglich, ob der von Arkin vorgeschlagene L&#246;sungsansatz, n&#228;mlich Roboter mit einem k&#252;nstlichen
Gewissen auszustatten &#8211; einem &#187;ethical governor&#171;, der alle Entscheidungen des Systems noch einmal auf ihre
rechtliche und ethische Angemessenheit pr&#252;ft &#8211;, hier Abhilfe zu schaffen vermag. F&#252;r Kritiker Arkins wie Asaro
(2012, S. 701 f.) oder Sparrow (2016, S. 101) liegt dieser Idee die f&#228;lschliche Vorstellung zugrunde, dass sich
aus moralischen wie auch rechtlichen Ma&#223;st&#228;ben eindeutige Verhaltensregeln ableiten lassen. Dies ist jedoch
nicht der Fall; selbst das moralisch wie rechtlich sehr hoch verankerte T&#246;tungsverbot gilt nicht absolut (wie ja
gerade das Kriegsv&#246;lkerrecht zeigt), sondern kennt Ausnahmen und ist deshalb immer auslegungsbed&#252;rftig.
Moralisches und rechtliches Erw&#228;gen sind aus diesen Gr&#252;nden wohl noch auf l&#228;ngere Zeit auf menschliche
Urteilskraft angewiesen, so Sparrow (2016, S. 99) &#8211; zumindest noch so lange, bis es gelingt, eine starke KI zu
erzeugen, die sich nicht nur in funktioneller, sondern auch moralischer Hinsicht als autonom erweist
(Kasten 2.1).
Bis es soweit ist &#8211; und ob es jemals so weit kommen wird, ist derzeit reine Spekulation &#8211;, scheint sich 
folgende Schlussfolgerung aufzudr&#228;ngen: Ohne ad&#228;quate menschliche Supervision und Kontrolle sollten AWS
bis auf Weiteres nicht eingesetzt werden d&#252;rfen, da &#8211; wie skizziert &#8211; kaum sicherzustellen ist, dass sie sich
ethisch und v&#246;lkerrechtlich einwandfrei verhalten, und verheerende Folgen deshalb nie ganz auszuschlie&#223;en
sind (Asaro 2012, S. 708; Sparrow 2016, S. 102). Ein autonomes Waffensystem, dessen Aktivit&#228;ten unter
konstanter menschlicher &#220;berwachung zu stehen h&#228;tte, w&#228;re jedoch nicht wirklich autonom. Nun k&#246;nnte man zwar
den un&#252;berwachten Einsatz von AWS auf solche Kampfgebiete beschr&#228;nken, die weitgehend frei von
Zivilbev&#246;lkerung und damit moralisch problematischen Entscheidungssituationen sind (Sparrow 2016, S. 102 ff.).
Abgesehen davon, dass sich eine solche Begrenzung des Aktionsradius kaum kontrollieren lie&#223;e, st&#252;nde sie in
gewissem Widerspruch zu den postulierten milit&#228;rischen Vorteilen von AWS, die ja u. a. in der Erweiterung
des Gefechtsraumes sowie der milit&#228;rischen Reichweite zu sehen sind (Koch/Rinke 2017, S. 80). Und auch ein
weiterer Vorschlag, der in die Diskussion eingebracht wurde, scheint wenig zielf&#252;hrend, n&#228;mlich den &#187;ethical
governor&#171; als Steuerungsmodul so auszugestalten, dass er immer dann einen menschlichen Bediener involviert,
wenn moralisch kritische Entscheidungen auf dem Spiel stehen (Brutzman et al. 2013; Koch/Rinke 2017, S. 77 
f.; Sparrow 2016, S. 101 f.). Voraussetzung w&#228;re, dass der &#187;ethical governor&#171; zuverl&#228;ssig einsch&#228;tzen kann,
wann er mit moralischen Entscheidungssituationen konfrontiert ist, die er nicht selber zu l&#246;sen vermag; dies ist
von einer Instanz, deren ethische Urteilskraft generell fragw&#252;rdig ist, freilich nicht zu erwarten.
Insgesamt bestehen also deutliche Zweifel an Arkins Postulat, dass AWS mit v&#246;lkerrechtlichen Standards
vereinbar sind. Unabh&#228;ngig davon stellt sich die wesentlich grunds&#228;tzlichere Frage, ob die bestehenden Regeln 
Zur ethischen Fundierung des Vorsichtsprinzips vgl. Rath et al. 2012, S.105 ff.93 
        
 
 
   
     
   
     
  
      
    
     
         
     
     
   
     
 
          
    
  
    
   
   
  
  
   
   
       
     
        
  
    
  
     
    
     
  
      
   
 
         
   
    
   
  
     
     
   
  
   
    
 
  
8.2
des Kriegsv&#246;lkerrechts &#187;&#252;berhaupt noch die richtigen Regeln sind, wenn autonome Kampfsysteme in die
Konfliktf&#252;hrung integriert sind&#171; (Gei&#223; 2015, S. 17 f.). Ausgangspunkt derartiger &#220;berlegungen ist die Feststellung,
dass die aktuellen Normen des humanit&#228;ren V&#246;lkerrechts von einem fundamentalen Anthropozentrismus
gepr&#228;gt sind, basierend auf der Pr&#228;misse, dass alle kritischen (insbesondere letalen) Entscheidungen von
Menschen getroffen werden. Mit dem Aufkommen autonomer Waffen w&#228;re hingegen eine milit&#228;rtechnologische
Zeitenwende eingeleitet, die diesbez&#252;glich eine ganz neue Situation schaffen w&#252;rde. So kann von autonomen
Waffensystemen eine au&#223;erordentlich hohe Pr&#228;zision erwartet werden. Au&#223;erdem verf&#252;gen sie weder &#252;ber
menschliche Emotionalit&#228;t und Irrtumsanf&#228;lligkeit noch agieren sie unter Einsatz des Lebens. Deshalb k&#246;nnte
es laut dem V&#246;lkerrechtler Robin Gei&#223; (2015, S. 17 ff.) geboten sein, ihren Einsatz an deutlich &#187;h&#246;here
humanit&#228;re Schutzstandards zu binden&#171;, als es f&#252;r menschliche K&#228;mpferinnen und K&#228;mpfer geboten erscheint &#8211; was 
bis hin zu der Pflicht reichen k&#246;nnte, AWS &#187;grunds&#228;tzlich nur als nichtt&#246;dliche Systeme einzusetzen&#171;. Gei&#223;
beruft sich dabei ma&#223;geblich auf eine Studie des Internationalen Komitees vom Roten Kreuz, das die These
vertritt, dass im Krieg der Einsatz letaler Gewalt nur insoweit gerechtfertigt ist, als dies milit&#228;risch unbedingt 
erforderlich ist und keine anderweitigen nichtletalen Mittel zur Verf&#252;gung stehen (Melzer 2009). K&#228;me es
zuk&#252;nftig nun zu einem Kampf Roboter gegen Mensch, best&#252;nde f&#252;r die Maschine, die ja kein Leben zu riskieren
hat, &#187;so gut wie nie eine zwingende Notwendigkeit, t&#246;dliche Gewalt anzuwenden&#171; (Gei&#223; 2015, S. 21).
Jedenfalls ist das T&#246;ten einer Person alleine deshalb, weil diese eine Maschine zu besch&#228;digen versucht,
wahrscheinlich unverh&#228;ltnism&#228;&#223;ig (Koch/Rinke 2017, S. 154).
Damit ist eine zentrale Pr&#228;misse der Arkin&#8217;schen Argumentation infrage gestellt, n&#228;mlich, dass die
tradierten Regeln einer ethischen Kriegsf&#252;hrung auch dann noch angemessen sind, sollten AWS dereinst zu einem
ma&#223;geblichen Faktor in milit&#228;rischen Auseinandersetzungen werden. Doch moralische und rechtliche Ma&#223;st&#228;be 
sind nicht in Stein gemei&#223;elt, sondern haben sich an neue Gegebenheiten anzupassen, wobei insbesondere der
technische Fortschritt neue Herausforderungen aufwirft. Mit der Autonomisierung von Waffensystemen k&#246;nnte
sich die Kriegsf&#252;hrung so fundamental ver&#228;ndern, dass auch die traditionellen Prinzipien des &#187;ius in bello&#171; mit 
Blick auf AWS ggf. neu justiert werden m&#252;ssten &#8211; etwa hinsichtlich der Frage, welche Ziele (mittels AWS) 
angegriffen werden d&#252;rfen oder wie die Verh&#228;ltnism&#228;&#223;igkeit eines Einsatzes zu bewerten ist. Wie Koch und
Rinke (2017, S. 154) etwa betonen, sind die Verh&#228;ltnism&#228;&#223;igkeitsanforderungen im Laufe der Zeit aufgrund
technologischer Verbesserungen stetig und deutlich angestiegen. Was im Zweiten Weltkrieg m&#246;glicherweise 
noch als verh&#228;ltnism&#228;&#223;ig gelten konnte, kann heute v&#246;llig unangemessen sein, da mit der Verf&#252;gbarkeit von
neuen, pr&#228;ziseren Kampfmitteln auch die Anforderungen an Verh&#228;ltnism&#228;&#223;igkeit neu zu bewerten sind. Ob f&#252;r
AWS &#252;berhaupt die gleichen v&#246;lkerrechtlichen Standards gelten sollen wie f&#252;r menschliche Kombattanten, ist
also alles andere als klar. Deshalb d&#252;rfte auch Arkins These, dass AWS dereinst besser als Menschen in der
Lage sein werden, sich gem&#228;&#223; den bestehenden v&#246;lkerrechtlichen Standards zu verhalten, zu kurz greifen.
AWS und die W&#252;rde des Menschen
Die bisher vorgestellten Argumente kreisten um die im Wesentlichen technische Frage, ob und inwiefern AWS
so konstruierbar sind, dass sie den Regularien des humanit&#228;ren V&#246;lkerrechts Folge zu leisten verm&#246;gen. Doch 
angenommen, es gel&#228;nge, eine technisch perfekte T&#246;tungsmaschine zu entwickeln, die diesbez&#252;glich fehlerfrei 
agiert (zu diesem Gedankenexperiment Leveringhaus 2016, S. 89 ff., und Koch/Rinke 2017, S. 127 ff.). Stellt
sich dann nicht dennoch &#187;die ganz grunds&#228;tzliche Frage, ob Computeralgorithmen &#252;ber Leben und Tod
entscheiden sollten, ohne dass ein Mensch diese Entscheidung zumindest mittr&#228;gt und auf sein Gewissen l&#228;dt&#171;
(Sch&#246;rnig 2014, S. 33)?
Im Gegensatz zu einer konsequentialistischen Denkweise, die lediglich die Folgen eines AWS-Einsatzes
in den Blick nimmt, werden ethische Bedenken, die grunds&#228;tzlicher auf die Handlung selbst verweisen,
deontologisch genannt (Kasten 8.1). Deontologische Argumente spielen im Zusammenhang mit AWS eine wichtige
Rolle. Zur Debatte steht in diesem Kontext n&#228;mlich nichts weniger als der grunds&#228;tzliche Zweck derartiger
Waffensysteme: die F&#228;higkeit, autonom &#252;ber Leben und Tod zu entscheiden. Dabei wird vor allem &#252;ber die
Frage diskutiert, ob und inwiefern die T&#246;tung von Menschen durch autonome Systeme mit der Menschenw&#252;rde 
vereinbar ist. Ins Spiel kommen damit &#220;berlegungen genuin ethischer Art, die weniger auf empirischen
Erw&#228;gungen beruhen (wie es bei konsequentialistischen Argumenten der Fall ist), stattdessen st&#228;rker von normativen
Vorgaben und begrifflichen Festlegungen geleitet sind.
8.2.1 Der Begriff der Menschenw&#252;rde94 
Der Begriff der Menschenw&#252;rde geht wie derjenige der Autonomie ma&#223;geblich auf die Philosophie der
Aufkl&#228;rung zur&#252;ck und hat von dort den Weg in die Allgemeine Erkl&#228;rung der Menschenrechte sowie das deutsche
Grundgesetz (GG) gefunden. So hei&#223;t es in Artikel 1 GG, dass die W&#252;rde des Menschen unantastbar ist,
w&#228;hrend sich die Erkl&#228;rung der Menschenrechte in der Pr&#228;ambel (und an verschiedenen anderen Stellen) auf die
angeborene W&#252;rde aller Menschen beruft. Interessanterweise wird jedoch in beiden F&#228;llen, trotz der zentralen
Stellung des Begriffs, nicht weiter konkretisiert oder definiert, was unter der W&#252;rde des Menschen zu verstehen 
ist. Zur&#252;ckzuf&#252;hren ist das darauf, dass sich der Gehalt des W&#252;rdebegriffs als Rechtsbegriff &#187;mangels einer
juristischen Tradition seiner Verwendung&#171; (Stoecker 2010, S. 19) prim&#228;r in moralischen Intuitionen seinen
Ursprung hat, die unter dem Eindruck der Grausamkeiten des Zweiten Weltkrieges &#8211; also in der Nachkriegszeit,
als die beiden Dokumente entstanden &#8211;, besonders gegenw&#228;rtig waren (Gutmann 2010, S. 3).
Trotz &#8211; oder vielleicht gerade wegen &#8211; seiner intuitiven Plausibilit&#228;t haben wir es bei der Menschenw&#252;rde 
mit einem Begriff zu tun, der &#228;u&#223;erst voraussetzungsreich, jedenfalls nicht einfach definierbar ist (Stoecker
2010). In der christlichen Tradition besteht die W&#252;rde des Menschen darin, dass er als Gottes Ebenbild
geschaffen ist. Hingegen h&#228;ngt die aufkl&#228;rerische Pr&#228;gung des Begriffs, die vor allem auf Immanuel Kant zur&#252;ckgeht,
untrennbar mit der spezifischen menschlichen F&#228;higkeit zusammen, moralisch autonom, d. h.
selbstgesetzgebend zu sein (Kasten 2.1). Als Vernunftwesen ist der Mensch nach Kant dem &#187;Reich der Zwecke&#171; zugeh&#246;rig,
was bedeutet, dass Menschen f&#228;hig sind &#8211; im Unterschied zu Maschinen oder nicht vernunftbegabten
Lebewesen &#8211;, ihre Ziele zu reflektieren und ihr Handeln selbst zu bestimmen (Ulgen 2017b). Damit ist eine
Einzigartigkeit des Menschen angesprochen, die diesen gegen&#252;ber anderen Lebewesen auszeichnet und jedem
menschlichen Individuum somit &#8211; v&#246;llig unabh&#228;ngig von pers&#246;nlichen Eigenheiten &#8211; einen moralischen Status verleiht,
der Respekt verdient. Kant bringt dies auf folgende Formel: &#187;Handle so, dass du die Menschheit sowohl in
deiner Person, als in der Person eines jeden andern jederzeit zugleich als Zweck, niemals blo&#223; als Mittel
brauchest&#171; (Kant, Grundlegung zur Metaphysik der Sitten). Mit anderen Worten, der Mensch ist Zweck an sich und
sollte deshalb nie blo&#223;es Mittel sein, da dies seine Autonomie missachtet. Nach dieser kantischen Lesart95 hei&#223;t
die W&#252;rde des Menschen achten, also zun&#228;chst anzuerkennen, dass ein Mensch einen inh&#228;renten Wert besitzt 
und deshalb nicht instrumentalisiert, d. h. zum Spielball subjektiver Interessen und Bed&#252;rfnisse gemacht werden
darf.
Dass Menschen immer auch einen materiellen Aspekt besitzen, ist davon unbenommen. So werden
Menschen von anderen Menschen zu bestimmten wirtschaftlichen Zwecken eingesetzt, beispielsweise im Rahmen
einer Besch&#228;ftigung, oder sie sind auf ihre K&#246;rperlichkeit zur&#252;ckgeworfen, z. B. wenn sie krank sind und
medizinischer Behandlung bed&#252;rfen. Insofern ist das Zuf&#252;gen von Schmerz nicht notgedrungen entw&#252;rdigend,
denn die betroffene Person k&#246;nnte der Handlung, die die Schmerzen verursacht, zugestimmt haben. Anders
sieht es aus, wenn jemandem gegen seinen eigenen Willen Leid zugef&#252;gt wird; in vielen F&#228;llen stellt dies eine
W&#252;rdeverletzung dar &#8211; allerdings nicht wegen des zugef&#252;gten Leids, sondern weil der Wille dieser Person
eklatant missachtet wurde. Das Gebot der Menschenw&#252;rde verlangt also im Kern, dass ungeachtet der physischen
Aspekte des Menschseins die Anerkennung eines Menschen als selbstbestimmtes Wesen niemals verlorengehen
darf.
Insbesondere in der deutschen bioethischen Diskussion spielt der Verweis auf eine m&#246;gliche
W&#252;rdeverletzung des Menschen eine wichtige Rolle. Dies ist der kantischen Tradition und nat&#252;rlich der besonderen,
unantastbaren Stellung der Menschenw&#252;rde im Grundgesetz geschuldet. Die W&#252;rde des Menschen fungiert
dabei als zentraler Grundwert und moralischer Leitbegriff, der sich gerade aufgrund seiner intuitiven Kraft und
semantischen Offenheit in vielen politisch-moralischen Handlungskontexten als anschlussf&#228;hig erwiesen hat
(Birnbacher 2016). Dadurch besteht allerdings auch die Gefahr, dass W&#252;rdeverweise in normativen Debatten
fast schon inflation&#228;r in Anschlag gebracht werden, was den Begriff auszuh&#246;hlen und ihn auf eine &#187;Leerformel&#171;
(Birnbacher 2001, S. 243) zu reduzieren droht. Angesichts der zahlreichen philosophischen Kontroversen, die
um den Begriff der Menschenw&#252;rde und dessen konkurrierende Deutungen entbrannt sind, erscheint elementar
wichtig, in ethischen Debatten nicht nur pauschal auf m&#246;gliche W&#252;rdeverst&#246;&#223;e hinzuweisen, sondern deutlich 
zu machen, welche elementaren Momente des Menschseins jeweils auf dem Spiel stehen.
94 Die Ausf&#252;hrungen in diesem Abschnitt lehnen sich an das Gutachten von Koch/Rinke 2017, S.166 ff. an.
95 Daneben existieren diverse alternative philosophische Auslegungen von Menschenw&#252;rde, die besonders in bioethischen Debatten 
&#8211; etwa &#252;ber den moralischen Status von ungeborenem menschlichem Leben &#8211; aufeinanderprallen (z.B. Birnbacher 2001; Kettner
2004).
8.2.2 Verletzt der Einsatz autonomer Waffensysteme die Menschenw&#252;rde?
Aus der Idee der Menschenw&#252;rde lassen sich die Menschen- und Grundrechte ableiten, zu denen auch das Recht
auf Leben und k&#246;rperliche Unversehrtheit geh&#246;rt (Artikel 2 GG und Artikel 3 der Allgemeinen Erkl&#228;rung der
Menschenrechte). Jemandem das Recht auf Leben abzusprechen, ist offensichtlich nicht mit der Achtung seiner
W&#252;rde vereinbar. Ein generelles T&#246;tungsverbot, wie es etwa im 5. biblischen Gebot formuliert wird (&#187;Du sollst
nicht t&#246;ten&#171;), l&#228;sst sich daraus jedoch nicht ableiten. Denn es sind durchaus Umst&#228;nde denkbar, unter denen die
T&#246;tung eines Menschen moralisch und rechtlich legitim erscheint. Dazu geh&#246;ren insbesondere Notwehr- oder 
Nothilfesituationen, die auch staatlicherseits den Einsatz von Gewalt als Ultima Ratio rechtfertigen k&#246;nnen &#8211;
wie im Falle des finalen Rettungsschusses durch die Polizei. Auch Kriege sind gem&#228;&#223; der Lehre vom gerechten
Krieg (sowie entsprechend dem V&#246;lkerrecht) nur dann gerechtfertigt, wenn sie sich als Akt kollektiver Notwehr
(oder Nothilfe) deuten lassen. Festzuhalten bleibt also, dass v&#246;lker- und auch verfassungsrechtlich gesehen
T&#246;tungen unter ganz bestimmten Bedingungen durchaus erlaubt und mit dem Prinzip der Menschenw&#252;rde
vereinbar sind. Die entscheidende Frage lautet dann, ob sich daran etwas &#228;ndert, wenn auf AWS als Gewaltmittel 
zur&#252;ckgegriffen wird.
Um es vorwegzunehmen: Wie die Literaturanalyse von Koch/Rinke (2017, S. 105 ff.) zeigt, trifft die
These, dass es die W&#252;rde des Menschen verletzt, autonome Waffensysteme gegen menschliche Ziele
einzusetzen, auf breite Zustimmung.96 Allerdings wird auch deutlich, dass Aspekte der Menschenw&#252;rde im Rahmen der
AWS-Debatte eher marginal behandelt werden und die dazugeh&#246;rigen Argumente weit weniger ausdifferenziert
sind als jene, die sich mit Fragen der ethischen Kriegsf&#252;hrung besch&#228;ftigen. Dies hat sicherlich zum einen damit
zu tun, dass die Debatte &#252;ber AWS vor allem im englischsprachigen Raum gef&#252;hrt wird, wo der W&#252;rdebegriff
nicht die gleiche Bedeutung hat wie im deutschsprachigen Kontext; zum anderen aber auch mit den
konzeptionellen Schwierigkeiten, die der schwer fassbare Begriff der Menschenw&#252;rde aufwirft, der christlich-
theologische, philosophische und rechtliche Facetten aufweist.
W&#228;hrend die christliche Perspektive in der Debatte keine nennenswerte Rolle spielt, wird daf&#252;r umso mehr
auf den philosophischen Kern des W&#252;rdegedankens rekurriert. Laut Gei&#223; (2015, S. 18) ist er darin zu sehen,
dass &#187;jeder Mensch als Individuum wahrgenommen und dementsprechend behandelt werden muss, als
einzigartiges, nicht austauschbares Wesen&#171;. Dies gilt selbstverst&#228;ndlich auch &#8211; und vielleicht sogar ganz besonders &#8211;
dann, wenn es darum geht, das Leben eines Menschen zu beenden. Gem&#228;&#223; einer Entscheidung des
Bundesverfassungsgerichts (BVerfG, Urteil des Ersten Senats vom 15. Februar 2006 &#8211; 1 BvR 357/05 &#8211;, Rn. 1-156) zur
Verfassungskonformit&#228;t des &#167; 14 Absatz 3 Luftsicherheitsgesetzes ist es beispielsweise entw&#252;rdigend, ein
entf&#252;hrtes Flugzeug und dessen unschuldige Insassen zum Abschuss freizugeben, um dadurch andere Personen zu 
sch&#252;tzen: &#187;Eine solche Behandlung missachtet die Betroffenen als Subjekte mit W&#252;rde und unver&#228;u&#223;erlichen
Rechten. Sie werden dadurch, dass ihre T&#246;tung als Mittel zur Rettung anderer benutzt wird, verdinglicht und
zugleich entrechtlicht, [...].&#171; Dabei spielt laut dem BVerfG weder eine Rolle, ob die betroffenen Passagiere
sowieso mit gr&#246;&#223;ter Wahrscheinlichkeit dem Tod geweiht sind, noch wie gro&#223; die Zahl der Personen ist, die
sich auf diese Weise retten lassen. Die Quintessenz lautet: Selbst im Angesicht des Todes d&#252;rfen Menschen also
nicht als blo&#223;e Objekte, als Mittel zum Zweck behandelt werden, da dies den inh&#228;renten Wert negiert, der ihnen
als Menschen zukommt.
Nat&#252;rlich lassen sich Kriegssituationen nicht mit zivilen Rettungsaktionen gleichsetzen, wie sie das
Bundesverfassungsgerichtsurteil zum Gegenstand hat. Dennoch l&#228;sst sich aus dem Prinzip der Menschenw&#252;rde
unmittelbar folgende ethische Minimalanforderung an T&#246;tungshandlungen ableiten, die auch auf milit&#228;rische
Kontexte zutrifft: Wer einen Menschen seines Lebens beraubt, sollte dies zumindest in der Anerkenntnis tun, dass
es sich bei dem Opfer um einen Menschen, also ein Wesen von inh&#228;rentem Wert handelt (Koch/Rinke 2017,
S. 168 ff.; iPRAW 2018a, S. 12). Ohne direkt auf den W&#252;rdebegriff Bezug zu nehmen, weist Sparrow (2016)
ganz in diesem Sinne darauf hin, dass es auch im Krieg und zwischen Feinden in moralischer Hinsicht essenziell
ist, eine zumindest rudiment&#228;re Form des zwischenmenschlichen Respekts aufrechtzuerhalten &#8211; selbst und
gerade dann, wenn t&#246;dliche Gewalt im Spiel ist.
Die Debatte, wie sie im Folgenden nachgezeichnet wird, bezieht sich also auf einen spezifischen Einsatzzweck von AWS, n&#228;mlich 
das T&#246;ten von Menschen. Darum l&#228;sst sich aus den Argumenten nicht ableiten, dass es ethisch falsch ist, autonome Waffen zu
entwickeln, die f&#252;r andere Zwecke verwendet werden, z.B. den Angriff auf nichtmenschliche Ziele wie die Erfassung und
Zerst&#246;rung unbemannter Kampfdrohnen (Koch/Rinke 2017, S.130).
96
Was folgt daraus f&#252;r den m&#246;glichen Einsatz von AWS? Zum jetzigen Zeitpunkt ist mehr als zweifelhaft,
ob derartige Systeme je in der Lage sein werden, die skizzierten moralischen Anforderungen an
T&#246;tungshandlungen zu erf&#252;llen. Zwar k&#246;nnen Kampfroboter ihre Ziele anhand bestimmter Merkmale abstrakt als Menschen
klassifizieren, aber sie verf&#252;gen nicht &#252;ber die Empathie, sich in Menschen einzuf&#252;hlen. Das w&#228;re aber
erforderlich, um nachvollziehen zu k&#246;nnen, was es hei&#223;t, ein Mensch zu sein, und damit auch, um den inh&#228;renten
Wert menschlichen Lebens achten zu k&#246;nnen. Die meisten Autorinnen und Autoren stimmen deshalb darin
&#252;berein, dass der Respekt und die Anerkennung, die der W&#252;rdebegriff einfordert, im Wesentlichen eine
interpersonale Beziehung voraussetzen, die verlorenzugehen droht, sobald Maschinen auf dem Schlachtfeld das
Regiment &#252;bernehmen (z. B. Asaro 2012; Heyns 2016; HRW 2014; Sparrow 2016; Ulgen 2017a). Die Folge einer 
derart dehumanisierten Kriegsf&#252;hrung w&#228;re, so Gei&#223; (2015, S. 19), dass &#187;der Mensch [...] dann gerade nicht
mehr als Individuum wahrgenommen [wird], sondern als blo&#223;es Objekt einer mathematisch kalkulierten
T&#246;tungsentscheidung&#171;. Das T&#246;ten von Menschen geriete, mit anderen Worten, zu einem puren, mit &#187;gnadenloser
Konsequenz ausgef&#252;hrten&#171; Automatismus. Nicht zuletzt w&#252;rden so &#187;Faktoren wie Gnade oder Mitgef&#252;hl [...]
aus der Gleichung entfernt&#171; (Gei&#223; 2015, S. 19), die f&#252;r menschliche T&#246;tungsentscheidungen konstitutiv sind 
oder es zumindest sein sollten, damit &#252;berhaupt ein Anspruch auf moralische Zul&#228;ssigkeit besteht.
An dieser Stelle wird in der W&#252;rdedebatte eine argumentative Br&#252;cke zur Ethik des Krieges und zum
humanit&#228;ren V&#246;lkerrecht geschlagen. Dessen inh&#228;rente Interpretationsspielr&#228;ume (Kap. 7) sind nicht nur als
Mangel zu sehen &#8211; als rechtliche Unsch&#228;rfen, die der Auslegung bed&#252;rfen &#8211;, sondern als positiver Wert, insofern
sich dadurch erst der Raum f&#252;r moralisches Urteilen und damit die Anerkennung der menschlichen W&#252;rde
er&#246;ffnen (zum Folgenden Koch/Rinke 2017, S. 112 ff.). So ist Asaro (2012, S. 700 ff.) zufolge die
Interpretations- und Auslegungsbed&#252;rftigkeit des humanit&#228;ren V&#246;lkerrechts auch als Appell an die Menschlichkeit der
Kombattanten zu verstehen. Ganz &#228;hnlich argumentiert Sparrow (2016, S. 107), der darauf hinweist, dass durch
das im humanit&#228;ren V&#246;lkerrecht verankerte Recht, nicht zur Zielscheibe eines milit&#228;rischen Angriffs zu werden
(z. B. durch Aufgeben), die Humanit&#228;t der potenziellen Opfer gew&#252;rdigt wird. Umgekehrt gilt dann wiederum: 
Die algorithmische Formalisierung der v&#246;lkerrechtlichen Bestimmungen, wie sie der &#187;ethical governor&#171; Arkins
vorsieht und auch voraussetzt, w&#252;rde das deliberative Moment und damit die M&#246;glichkeit f&#252;r eine &#187;
h&#246;chstpers&#246;nliche Gewissensentscheidung bzw. -pr&#252;fung&#171; (Gei&#223; 2015, S. 19) durch den Angreifer eliminieren.
Das Fazit derartiger &#220;berlegungen lautet, dass Menschlichkeit gegen&#252;ber den Opfern von Gewalt letztlich
auch nur von menschlichen Gewaltanwendern ausge&#252;bt werden kann &#8211; zumindest so lange, bis auch Maschinen
&#252;ber die F&#228;higkeit zur Empathie, ein Gewissen und moralisches Bewusstsein verf&#252;gen. Und solange dies nicht
der Fall ist, ist es unethisch, den menschlichen Faktor aus Entscheidungsprozessen zu eliminieren, welche die
Anwendung t&#246;dlicher Gewalt zum Ziel haben. Deshalb stimmen die meisten Autoren, die zur W&#252;rdefrage
Stellung genommen haben, darin &#252;berein, dass AWS intrinsisch (d. h. von der Sache her) mit dem W&#252;rdegebot
unvereinbar sind.
Allerdings bleibt vieles diffus und ungekl&#228;rt: beispielsweise die Kriterien, die vorliegen m&#252;ssen, damit
unter den moralischen Extrembedingungen des modernen Krieges von einem menschenw&#252;rdigen T&#246;tungsakt
die Rede sein kann. Zu Recht wenden die Philosophen Ryan Jenkins und Duncan Purves (2016) ein, dass beim
Einsatz hochtechnologischer Massenvernichtungs- und Distanzwaffen von wechselseitiger
zwischenmenschlicher Anerkennung &#8211; als zentraler Grundbedingung eines moralisch akzeptablen Waffeneinsatzes &#8211; ebenfalls 
nicht die Rede sein kann. Wenn dem so ist, kann auf dieser Basis kaum mehr von einer spezifischen moralischen
Verwerflichkeit von AWS gesprochen werden, da das Argument einer W&#252;rdeverletzung auf praktisch alle
modernen Waffensysteme zutrifft. Der deutsche Philosoph Dieter Birnbacher (2016) stimmt dem zu und sieht das
Problem in der inflation&#228;ren und zu unspezifischen Verwendung des W&#252;rdekonzepts, die auch in der AWS-
Debatte zum Ausdruck komme. Seiner Ansicht nach sind autonome Waffensysteme nicht grunds&#228;tzlich mit der
W&#252;rde des Menschen inkompatibel, sondern nur, insofern sie in bestimmter, n&#228;mlich entw&#252;rdigender Weise
eingesetzt werden. Ausschlaggebend sind dabei die psychologischen und physischen Folgen f&#252;r die
Betroffenen. Die Funktionen und Charakteristika, die dem&#252;tigende Einsatzszenarien wahrscheinlich machen (und im
Falle von AWS besonders wahrscheinlich, wie Birnbacher zugesteht), w&#252;rden autonome mit vielen
konventionellen Waffensystemen teilen. Dazu z&#228;hlt Birnbacher (2016, S. 116 ff.) etwa die Unvorhersehbarkeit im
Verhalten (analog zu versteckten Landminen oder ferngesteuerten Drohnen), welche die potenziellen Opfer gro&#223;em
psychischem und potenziell traumatisierendem Stress aussetzt; dieser Aspekt der Heimt&#252;cke wird dadurch
verst&#228;rkt, dass AWS das Unterscheidungs- und das Verh&#228;ltnism&#228;&#223;igkeitsgebot vermutlich nicht zuverl&#228;ssig
einhalten k&#246;nnen (was aber z. B. beim Einsatz von Distanzwaffen in analoger Weise zutreffen kann). Ob ein
Waffeneinsatz die Menschenw&#252;rde verletzt, l&#228;sst sich laut Birnbacher demzufolge nicht an intrinsischen Merkmalen
des eingesetzten Systems festmachen (z. B. ob autonom oder nicht). Ausschlaggebend ist vielmehr alleine die
        
 
 
    
  
 
 
      
   
      
     
      
     
     
   
       
     
   
   
 
    
     
 
  
   
    
   
    
   
  
  
   
    
   
     
   
   
       
       
  
      
   
    
    
 
 
   
    
        
     
    
   
    
    
8.3
subjektive Wirkung (etwa Traumatisierung durch andauerndes Bedrohungsgef&#252;hl), die bei den Betroffenen
unter den konkreten Umst&#228;nden erzeugt wird.
Insgesamt machen derartige Einw&#228;nde deutlich, dass stark von der inhaltlichen Bestimmung des
W&#252;rdebegriffs abh&#228;ngt, inwiefern eine W&#252;rdeverletzung von AWS plausibel erscheint. So vertritt der Utilitarist
Birnbacher, der dem W&#252;rdekonzept insgesamt eher skeptisch gegen&#252;bersteht, einen ethischen Ansatz, der die W&#252;rde
des Menschen vornehmlich bed&#252;rfnisorientiert deutet &#8211; relevantes und einziges Kriterium f&#252;r eine
W&#252;rdeverletzung ist dann, ob eine St&#246;rung des subjektiven Wohlbefindens vorliegt (Birnbacher 2016, S. 120). Koch und
Rinke heben hingegen hervor, dass erst dort, wo Menschen sich auf andere Menschen beziehen, W&#252;rde aktuell
werden kann (dazu und zum Folgenden Koch/Rinke 2017, S. 167 f.). F&#252;r T&#246;tungshandlungen folgt daraus, dass
immer noch anerkannt werden sollte, dass es ein Mensch ist, der get&#246;tet wird. Dabei liegt auf der Hand, dass es
keine definitiven &#228;u&#223;eren Kriterien geben kann, anhand derer sich in der Praxis ablesen l&#228;sst, ob dieses
Anerkennungsmoment in ausreichender Weise erf&#252;llt ist oder nicht. Ebenfalls wird eine T&#246;tungshandlung dadurch 
nat&#252;rlich noch lange nicht zu einer legitimen Handlung. Doch l&#228;sst sich daraus zumindest ableiten, dass dort,
wo Menschen komplett die M&#246;glichkeit genommen wird, in ihrer W&#252;rde durch andere geachtet zu werden &#8211;
und zwar ganz unabh&#228;ngig von ihren subjektiven Bed&#252;rfnissen &#8211;, diese fundamental bedroht ist. Insofern AWS
diese basale Anerkennung nicht erbringen k&#246;nnen, erscheint plausibel, ihren Einsatz als entw&#252;rdigende Form
roher technischer Gewalt anzusehen. Wie gezeigt h&#228;ngt die &#220;berzeugungskraft dieses Gedankengangs
allerdings zentral davon ab, ob man die zugrundeliegenden moralischen Intuitionen und theoretischen Festlegungen
teilt. Zu konstatieren ist deshalb, dass die Herleitung einer W&#252;rdeverletzung durch AWS an enge argumentative
Grenzen st&#246;&#223;t.
Die Frage der Verantwortung
Trotz der Schwierigkeiten, den Autonomiebegriff definitorisch einzugrenzen (Kap. 2), ist eines gewiss: Ein
Gewinn an maschineller Autonomie geht schon alleine aus begrifflichen Gr&#252;nden immer mit einem Verlust an
menschlicher Kontrolle einher (Noorman/Johnson 2014, S. 52). Dieser Kontrollverlust ist grunds&#228;tzlich
erw&#252;nscht, da von autonomen Systemen ja erwartet wird, dass sie sich flexibel und unabh&#228;ngig von menschlicher
Einflussnahme in neuen und unvorhergesehenen Situationen zurechtfinden (Leveringhaus 2016, S. 79). 
Dadurch l&#228;sst sich ein umfangreicheres Funktionsspektrum realisieren und neue maschinelle
Anwendungsgebiete erschlie&#223;en, womit sich im milit&#228;rischen Bereich perspektivisch ganz neue strategische, operative und
taktische M&#246;glichkeiten er&#246;ffnen (Kap. 5.3).
Die Kehrseite davon ist jedoch, dass die Frage der Verantwortungszuschreibung deutlich an Brisanz
gewinnt. Denn je autonomer Systeme agieren, also je weniger sie direkter menschlicher Steuerung unterliegen,
desto weniger eindeutig lassen sich ihre Aktionen einem menschlichen Akteur zuordnen. Das wird immer dann
problematisch, wenn solche Systeme Schaden anrichten: Wer tr&#228;gt dann die Verantwortung? Diese Frage wird
virulent, da die Maschinen selbst nat&#252;rlich nicht zur Rechenschaft gezogen werden k&#246;nnen, zumindest solange 
sie nur &#252;ber operationelle Autonomie und deshalb nicht &#252;ber Handlungsverm&#246;gen im strengen philosophischen
Sinne verf&#252;gen (Kasten 2.1; Hilgendorf 2012; Koch/Rinke 2017, S. 157 f.). Insofern es also immer schwieriger
wird, &#187;menschliche Akteure zu den Vollz&#252;gen des Roboters in Verbindung zu bringen&#171; (Koch/Rinke 2017,
S. 156), wird zunehmend unklar, wer f&#252;r die Folgen geradezustehen hat &#8211; mit weitreichenden rechtlichen und
moralischen Implikationen. Dieses Problem ist auch als &#187;Verantwortungsl&#252;cke&#171; (&#187;responsibility gap&#171;, siehe
z. B. Johnson 2015; Sparrow 2007) bekannt. Es wird generell intensiv diskutiert, vor allem mit Bezug auf
Fragen der zivilrechtlichen Haftung (f&#252;r einen &#220;berblick Hilgendorf 2014). Letztlich geht es um die Frage, wer die 
in ihrem Ausma&#223; teils noch unbekannten Haftungsrisiken, die mit dem Einsatz dieser neuen Technologien
verbunden sind, zu tragen hat.
Mit Blick auf AWS ist die Verantwortungsthematik sogar von besonderer Brisanz, da wir es hier mit
Ger&#228;ten zu tun haben, die mit einer enormen Eingriffstiefe ausgestattet sind. Was auf dem Spiel steht, ist das Leben
von Menschen. Wie bereits dargelegt wurde, ist u. a. fraglich, ob und inwiefern AWS in der Lage sein werden,
die Regeln des humanit&#228;ren V&#246;lkerrechts einzuhalten. Auch wenn es gelingen sollte, ein System zu schaffen,
das den diesbez&#252;glich relevanten Anforderungen Folge zu leisten vermag, sind Fehlfunktionen aufgrund von 
Programmierfehlern oder technischem Versagen selbstverst&#228;ndlich nie ganz auszuschlie&#223;en &#8211; mit den
entsprechenden t&#246;dlichen Folgen bis hin zu m&#246;glichen Kriegsverbrechen. Insofern ist es nicht erstaunlich, dass die
Frage der Verantwortung auch in der ethischen Debatte &#252;ber AWS gr&#246;&#223;eren Raum einnimmt. Selbst Arkin
(2010, S. 339), der ja bekanntlich die These vertritt, dass ein Einsatz von AWS die Zahl an unschuldigen Opfern
verringern k&#246;nnte, sieht die Frage der Verantwortung als eines der gro&#223;en ungel&#246;sten Probleme an.
In der Debatte wird allerdings nicht immer klargestellt, was genau mit Verantwortung gemeint ist (dazu
und zum Folgenden Koch/Rinke 2017, S. 157). Verantwortung f&#228;llt einem Verantwortungstr&#228;ger &#8211; also einem
moralischen Akteur &#8211; nicht einfach zu, sondern sie ist nur inhaltlich bestimmbar innerhalb von sozialen
Verantwortungskontexten. Das hei&#223;t, es bedarf stets mehrerer Personen, die in eine soziale Praxis des
Verantwortlichmachens involviert sind. Gerade auch mit Blick auf AWS ist dabei wichtig, zwischen rechtlichen und
moralischen Kontexten der Verantwortungszuschreibung zu unterscheiden. Denn nicht alles, was rechtlich legitim
ist, erscheint auch moralisch richtig; und umgekehrt ist nicht alles, was rechtlich geahndet wird, auch moralisch
verwerflich. Aus rechtlicher und moralischer Sicht geh&#246;rt es zwar zentral zur Verantwortungspraxis,
Verantwortlichkeit an Handlungsurheberschaft (alternativ an die Urheberschaft von Handlungsunterlassungen) zu
binden. W&#228;hrend die rechtliche Verantwortung jedoch st&#228;rker auf juristischen Normsetzungen beruht und vor allem
die Folgen von Handlungen bewertet (auf Basis empirischer Evidenz), ist moralische Verantwortung prim&#228;r
(wenn auch nicht ausschlie&#223;lich) auf die Gr&#252;nde von Handlungen bezogen (Vo&#223;enkuhl 1983, S. 137).
Unterschiede bestehen auch hinsichtlich des Umgangs mit Schuld und Verantwortung: Rechtliche Vergehen sind
zwar &#252;blicherweise sanktioniert, die resultierende Schuld l&#228;sst sich daf&#252;r aber im Rahmen der daf&#252;r
vorgesehenen institutionalisierten Verfahren tilgen; bei moralischer Verantwortung besteht diese M&#246;glichkeit nicht.
8.3.1 Rechtliche Sicht
Das humanit&#228;re V&#246;lkerrecht kann seine Funktion &#8211; den Schutz von Menschen und Sachen vor
Kriegsauswirkungen &#8211; letztlich nur erf&#252;llen, wenn Rechtsverst&#246;&#223;e geahndet und sanktioniert werden (Gei&#223; 2015, S. 21). Die
Identifizierung von Verantwortlichen ist daf&#252;r zentral. Wenn nun AWS f&#252;r die von ihnen begangenen Vergehen
selbst nicht zur Rechenschaft gezogen werden k&#246;nnen, wie bereits argumentiert wurde, stellt sich die Frage,
wer ansonsten haftbar gemacht werden kann. Naheliegend ist, sich dabei an die Staaten zu wenden, die die
Systeme eingesetzt haben &#8211; schlie&#223;lich ist das humanit&#228;re V&#246;lkerrecht ein Regelwerk, das zwischenstaatliche
Beziehungen zum Gegenstand hat und somit auch nur Staaten als Rechtssubjekte adressiert. Der Fall der
Staatenverantwortlichkeit wurde bereits in Kapitel 7.2 erl&#228;utert. Hier soll nun etwas genauer beleuchtet werden,
inwiefern es m&#246;glich ist, im Rahmen des Strafrechts oder des Zivilrechts auch individuelle Akteure
verantwortlich zu machen (zum Folgenden Gei&#223; 2015, S. 21 ff.):
&#8250; Kommt es durch AWS zu Verletzungen des V&#246;lkerrechts, so kommt unter bestimmten Bedingungen eine
strafrechtliche Verantwortung derjenigen Einzelpersonen in Betracht, welche die Systeme zum Einsatz
gebracht haben &#8211; das sind in der Regel die verantwortlichen milit&#228;rischen Befehlshaber. F&#252;r die
Verfolgung von V&#246;lkermord, schweren Kriegsverbrechen sowie Verbrechen gegen die Menschlichkeit ist seit
2002 der Internationale Strafgerichtshof in Den Haag zust&#228;ndig, der allerdings nur dann t&#228;tig wird, wenn
diese Delikte auf nationaler Ebene nicht verfolgt werden (Steinke 2018). Um das deutsche Strafrecht an
die Statuten des Internationale Strafgerichtshofs anzupassen (sogenanntes Rom-Statut),97 wurde 2002 das
V&#246;lkerstrafgesetzbuch (VStGB) erlassen. Demnach sind milit&#228;rische Befehlshaber insoweit f&#252;r
V&#246;lkerrechtsverst&#246;&#223;e verantwortlich, als sie v&#246;lkerrechtswidrige Aktionen entweder selbst direkt anordnen &#8211; und 
zwar in vollem Bewusstsein der Konsequenzen (&#167; 11 Absatz 1 VStGB) &#8211; oder sich vors&#228;tzliche oder
fahrl&#228;ssige Verletzungen der Aufsichtspflicht zuschulden kommen lassen (&#167; 14 VStGB). Ein Kommandeur,
der ein autonomes Waffensystem ins Feld schickt, m&#252;sste also vorab wissen, dass es Kriegsverbrechen
begehen wird oder diese zumindest fahrl&#228;ssig in Kauf genommen haben &#8211; nur dann ist er strafrechtlich f&#252;r
dessen Verfehlungen verantwortlich.98 Da sich AWS gerade durch weitgehende Eigenst&#228;ndigkeit bei der
Wahl ihrer Ziele auszeichnen, d&#252;rfte es mit zunehmender Verbreitung dieses Waffentyps immer schwerer
97 Das Rom-Statut wurde bislang von 123 Staaten ratifiziert (Stand 2020; ICC o. J.). Gerade diejenigen Staaten, die &#252;ber die gr&#246;&#223;te
milit&#228;rische Macht verf&#252;gen und auch bei der Entwicklung von AWS eine entscheidende Rolle spielen, n&#228;mlich China, Russland 
und die USA, haben sich bislang geweigert, den Internationalen Strafgerichtshof anzuerkennen.
98 Beispielsweise durch Verletzung von Sorgfaltspflichten wie der mangelhaften Wartung/Instandhaltung oder durch Missachtung der
vorgesehenen Zweckbestimmung; &#196;hnliches gilt im &#220;brigen f&#252;r die Hersteller (oder Programmierer) der Waffensysteme, die
ebenfalls f&#252;r deren Verfehlungen im Einsatz strafrechtlich zur Verantwortung gezogen werden k&#246;nnen &#8211; vorausgesetzt, sie haben diese
Verfehlungen vors&#228;tzlich oder zumindest fahrl&#228;ssig herbeigef&#252;hrt (z.B. durch absichtliche Fehlprogrammierung oder fahrl&#228;ssige
Missachtung von Sicherheitsstandards etc.; Gei&#223; 2015, S.22).
fallen, milit&#228;rischen Befehlshabern eine strafrechtliche Verantwortung in diesem Sinne eindeutig
nachzuweisen (siehe dazu Gei&#223; 2015, S. 22; Sparrow 2007, S. 69 ff.).
&#8250; Au&#223;erdem k&#246;nnen gegen den Hersteller/Programmierer eines AWS auch auf zivilrechtlicher Ebene
Haftungsanspr&#252;che geltend gemacht werden, beispielsweise im Rahmen einer verschuldensunabh&#228;ngigen
Produkthaftung, wie sie in den EU-Staaten gilt.99 Voraussetzung ist, dass ein v&#246;lkerrechtlicher Versto&#223;
eindeutig auf ein fehlerhaftes Produkt zur&#252;ckzuf&#252;hren ist;100 unabh&#228;ngig davon, ob der Hersteller f&#252;r den
Produktfehler verantwortlich ist oder nicht, kann er dann prinzipiell f&#252;r den resultierenden Schaden haftbar
gemacht werden. Derartige Haftungsregeln k&#246;nnten die Hersteller zur Einhaltung hoher Qualit&#228;ts- und 
Sicherheitsstandards motivieren (Gei&#223; 2015, S. 23). Die Schwierigkeit liegt jedoch darin, dass es im
Zivilrecht den Gesch&#228;digten obliegt, eine Haftungsklage gegen den Hersteller anzustrengen und, schwieriger
noch, auch den urs&#228;chlichen Zusammenhang zwischen Produktfehler und V&#246;lkerrechtsversto&#223; zu
beweisen. Gerade Letzteres ist bei autonomen Waffensystemen, deren technisches Innenleben von hoher
Komplexit&#228;t ist und zudem noch weitreichenden Geheimhaltungspflichten unterliegen d&#252;rfte, f&#252;r
Einzelpersonen praktisch ein Ding der Unm&#246;glichkeit.
Folglich l&#228;sst sich konstatieren, dass sich auf rechtlicher Ebene im Umgang mit AWS eine
Verantwortlichkeitsl&#252;cke deutlich abzuzeichnen beginnt. Gei&#223; (2015, S. 22) sieht darin ein &#187;strukturelles Problem&#171;, da die
Zuschreibung von Verantwortung bedingt, dass eine Form von Kontrolle ausge&#252;bt wird (Betzler/Scherrer 2016).
Je mehr sich Waffensysteme also menschlicher Steuerung entziehen, desto schwieriger wird es fallen,
menschliche Entscheidungstr&#228;ger f&#252;r die von ihnen begangenen Vergehen rechtlich zur Rechenschaft zu ziehen.
Es w&#228;re jedoch voreilig, daraus ein grunds&#228;tzliches Rechtsproblem abzuleiten. Tatsache ist: Die
bestehenden straf- und zivilrechtlichen Normen stammen noch aus einer Zeit, in der das Verhalten von Maschinen
mechanisch weitgehend determiniert und damit vorhersehbar war. Die beschriebene Problematik verweist
demzufolge vor allem auf die Defizite des bestehenden Rechtsrahmens sowie die Notwendigkeit, diesen an die neuen
technologischen Entwicklungen anzupassen (Hilgendorf 2012). Das Ziel m&#252;sste sein, neue Rechtspraktiken zu
entwickeln, die der technischen Komplexit&#228;t autonomer Systeme angemessen sind, und zu verlangen, dass
maschinelle Autonomie transparent ausgestaltet wird (Noorman/Johnson 2014). M&#246;gliche L&#246;sungsans&#228;tze werden
bereits seit einiger Zeit intensiv diskutiert, u. a. vom Europ&#228;ischen Parlament (EP 2017) oder der Ethik-
Kommission Automatisiertes und vernetztes Fahren (Ethik-Kommission 2017). Zwar geschieht dies vor allem mit
Blick auf zivile Einsatzbereiche autonomer Systeme, die zugrundeliegenden Konzepte lassen sich in der Regel 
problemlos auf milit&#228;rische Einsatzbereiche &#252;bertragen; so etwa die Idee, Roboter mit einem Modul
auszustatten, in dem &#187;die Daten &#252;ber jede von der Maschine ausgef&#252;hrte Aktion &#8211; einschlie&#223;lich der logischen Abfolgen,
die zu etwaigen Entscheidungen gef&#252;hrt haben &#8211; gespeichert sind&#171; (EP 2017, S. 11).101 Zusammen mit dem
verst&#228;rkten Fokus auf ethische Technikgestaltung w&#252;rde es diese Ma&#223;nahme erleichtern, nachtr&#228;glich
nachzuvollziehen, ob der Schadensfall auf menschliches Versagen (z. B. Programmierfehler) oder sogar Vorsatz
zur&#252;ckzuf&#252;hren ist (M&#252;ller 2016). Zudem scheint naheliegend, dass sich k&#252;nftig die rechtliche Haftbarkeit im
Zusammenhang mit autonomen Systemen weniger an konkretem Verschulden und mehr an der abstrakten
Gef&#228;hrdung festmachen sollte, eine Praxis, die z. B. in Bezug auf Tierhalter bereits g&#228;ngig ist (&#167; 833 B&#252;rgerliches
Gesetzbuch). Das w&#252;rde dann demjenigen, der autonome Systeme zum Einsatz bringt, bei AWS also dem
verantwortlichen Kommandeur, automatisch &#8211; also im Sinne einer verschuldensunabh&#228;ngigen Gef&#228;hrdungshaftung
(Kap. 7.2) &#8211; die Verantwortung f&#252;r die Risiken aufb&#252;rden, die damit verbunden sind (Gei&#223; 2015, S. 22 f.).
99 Auf Basis der Richtlinie 85/374 EWG zur Angleichung der Rechts- und Verwaltungsvorschriften der Mitgliedstaaten &#252;ber die 
Haftung f&#252;r fehlerhafte Produkte, die in Deutschland mittels des Gesetzes &#252;ber die Haftung f&#252;r fehlerhafte Produkte
(Produkthaftungsgesetz &#8211; ProdHaftG) umgesetzt wurde.
100 Ob Soft- oder Hardware betroffen sind, spielt dabei keine Rolle, wenn beides vom selben Hersteller stammt. Ansonsten b&#252;rgt f&#252;r
Softwarefehler der Softwarehersteller, f&#252;r Hardwarefehler der Hersteller der Hardware.
101 Die milit&#228;rische Geheimhaltung steht diesem Verfahren zun&#228;chst entgegen. Eine L&#246;sungsm&#246;glichkeit k&#246;nnte darin bestehen, die 
Maschinendaten f&#228;lschungssicher aufzuzeichnen, wie von Gubrud/Altmann (2013) vorgeschlagen, sodass sie sp&#228;ter von einer
internationalen Organisation vertraulich &#252;berpr&#252;ft werden k&#246;nnten (Altmann 2017, S.802). Dieses Vorgehen lie&#223;e sich jedoch nur
im Rahmen eines internationalen Abkommens etablieren.
8.3.2 Moralische Sicht
Was in der juristischen Praxis m&#246;glich ist, n&#228;mlich die soziale Verantwortung f&#252;r einen Schaden jemandem
zuzuweisen, der keine konkrete Schuld daf&#252;r tr&#228;gt, ist aus moralischer Sicht keine Option. Grundlage f&#252;r
moralische Verantwortung ist die bereits angesprochene Handlungsurheberschaft, die wiederum eng mit der Idee
der Willensfreiheit zusammenh&#228;ngt (Kasten 2.1). Nur vor dem Hintergrund der Vorstellung, dass man auch 
h&#228;tte anders handeln k&#246;nnen, ergibt moralische Verantwortlichkeit Sinn. Koch und Rinke (2017, S. 157) geben 
folgendes Beispiel: Wenn jemand geschubst, also blo&#223; als K&#246;rper bewegt wird, wird man ihm die k&#246;rperliche 
Bewegung nicht zuschreiben und ihn dementsprechend auch nicht f&#252;r deren Folgen verantwortlich machen
(allenfalls wird man ihn daf&#252;r verantwortlich machen, dass er sich in eine Situation gebracht hat, in der er leicht
geschubst werden konnte). Das ist der Grund, wieso es auch keinen Sinn ergibt, autonome Maschinen f&#252;r ihre
Taten zu bestrafen: Sie verf&#252;gen nicht &#252;ber eine Form der Selbstbestimmung, die von Freiwilligkeit und Wissen
um die Konsequenzen getragen ist (Fischer/Ravizza 1998).
Was w&#228;re die Folge, sollte die Autonomisierung der Waffentechnologie weiter voranschreiten? Bis auf
wenige Ausnahmef&#228;lle, in denen sich die t&#246;dlichen Aktivit&#228;ten autonom agierender Waffensysteme klar auf
menschlichen Vorsatz oder Fahrl&#228;ssigkeit zur&#252;ckf&#252;hren lassen (sei es des Programmierers oder des
Kommandeurs), w&#252;rde die Frage der moralischen Verantwortung f&#252;r die Opfer zunehmend diffus. Jemanden f&#252;r die
Opfer die moralische Verantwortung zuzuschieben, der keine konkrete Schuld daran tr&#228;gt, vermag die
Verantwortungsl&#252;cke zwar aus rechtlicher (vgl. das Konzept der Gef&#228;hrdungshaftung), nicht jedoch aus moralischer
Sicht zu schlie&#223;en. Denn schuldig im moralischen Sinne f&#252;hlen sich Menschen aus den zuvor genannten
Gr&#252;nden in der Regel nur f&#252;r Dinge, die sie selbst getan haben (oder zu denen sie durch Unterlassen beigetragen
haben) (Koch/Rinke 2017, S. 129).
Angesichts dessen wird in der Literatur argumentiert, dass mit dem Aufkommen von AWS eine
Verantwortungsl&#252;cke zu entstehen droht, die vor allem aus ethischer Sicht problematische Konsequenzen mit sich
bringt (z. B. Sparrow 2007). Denn offensichtlich ist die Entscheidung, einen anderen Menschen zu t&#246;ten, eine
der moralisch schwerwiegendsten, die man treffen kann. Insofern ist die Vorstellung, dass niemand f&#252;r eine
solche Entscheidung (bzw. die anschlie&#223;ende Tat) die moralische Verantwortung zu &#252;bernehmen h&#228;tte,
durchaus beunruhigend. W&#252;rden AWS eingesetzt, entsteht laut Sparrow (2007, S. 68) eine Situation, in der sich das
technisch vermittelte T&#246;ten von Menschen wie ein zuf&#228;lliges Naturereignis oder ein Unfall ausnimmt. Es gibt 
dann niemanden mehr, der dies mit seinem Gewissen vereinbaren muss, was eine tiefe Missachtung des Wertes
eines Menschenlebens zum Ausdruck bringt (Sparrow 2016, S. 108). Aus &#228;hnlichen Gr&#252;nden pl&#228;diert der
Philosoph Alexander Leveringhaus (2016, S. 89 ff.) daf&#252;r, die menschliche Entscheidungsf&#228;higkeit in einem
bewaffneten Konflikt f&#252;r ein hohes und sch&#252;tzenswertes Gut anzusehen (dazu und zum Folgenden Koch/Rinke
2017, S. 127 ff.). Gerade weil menschliches Leben kostbar und generell sch&#252;tzenswert ist, so Leveringhaus, soll
es immer auch die M&#246;glichkeit geben, von einer T&#246;tungsentscheidung zur&#252;ckzutreten. Die F&#228;higkeit,
barmherzig zu sein und Gnade zu zeigen, ist zweifelsohne eine hohe moralische Tugend. Das hei&#223;t nicht, dass es immer
richtig ist, dieser Tugend Folge zu leisten (es w&#228;re z. B. falsch, einen Attent&#228;ter nicht zu erschie&#223;en, bevor er
seinen Sprengsatz zur Detonation bringen kann). Aber selbst wenn es in einem kriegerischen Ernstfall zur
Anwendung von Waffengewalt kommt, sollte dies laut Leveringhaus (2016) immer eine direkte menschliche
Entscheidung sein, f&#252;r die ein menschlicher Akteur letztlich auch moralisch einzustehen hat (dazu auch Asaro 2012,
S. 701).
Vor diesem Hintergrund vertreten die zitierten Experten die Ansicht, dass AWS ethisch verwerflich sind
und deshalb nicht eingesetzt werden sollten. Dies zeigt, dass die Implikationen der moralischen
Verantwortlichkeitsl&#252;cke deutlich weitreichender sind als diejenigen ihres rechtlichen Gegenst&#252;cks: Die rechtliche
Verantwortungsl&#252;cke wird erst relevant, wenn ein Versto&#223; gegen Recht und Gesetz vorliegt (beispielsweise ein
Kriegsverbrechen, das es strafrechtlich zu ahnden gilt). Ein generelles Verbot von AWS l&#228;sst sich daraus schwerlich
ableiten, wie bereits argumentiert wurde, sofern es gelingt, die rechtliche Praxis der
Verantwortungszuschreibung an die Herausforderungen autonomer Technologien anzupassen (dazu auch Koch/Rinke 2017, S. 159 f.). 
Aus moralischer Sicht stellt sich die Frage der Verantwortlichkeit jedoch ganz grunds&#228;tzlich und insbesondere 
unabh&#228;ngig davon, ob der T&#246;tungsakt unter den gegebenen Umst&#228;nden legal ist oder nicht. Das T&#246;ten selbst ist
ein moralisch problematischer Akt, &#252;ber den es zumindest vor dem eigenen Gewissen Rechenschaft abzulegen 
gilt. T&#246;tungsmaschinen zu konstruieren und in Gang zu setzen, die mittels ihrer Autonomie genau diese
menschliche Praxis des Verantwortlichmachens unterminieren, erscheint demnach als fundamental unmoralisch (Asaro
2012, S. 695).
Bei genauerer Betrachtung wird deutlich, dass die Frage nach der moralischen Verantwortlichkeit und die 
Frage nach der W&#252;rde des Menschen auf &#228;hnlichen Intuitionen beruhen und eng miteinander verbunden sind
(Koch/Rinke 2017, S. 105). Das Argument von der moralischen Verantwortungsl&#252;cke, wie es zuvor entfaltet
wurde, l&#228;sst sich auch folgenderma&#223;en auf den Punkt bringen: Menschen zu t&#246;ten, ohne dass daf&#252;r jemand
verantwortlich zeichnet, hei&#223;t, ihnen die moralische Anerkennung und Wertsch&#228;tzung zu versagen, die ihnen
als Wesen mit W&#252;rde zusteht. Im Umkehrschluss hei&#223;t das aber auch, dass dieses Argument ebenso
voraussetzungsreich und in seiner Reichweite gleicherma&#223;en begrenzt ist wie jenes, das eine W&#252;rdeverletzung durch
AWS abzuleiten versucht. Letztlich bleiben in beiden F&#228;llen gro&#223;e Unsicherheiten, die sich mit argumentativen
Mitteln nicht restlos aufl&#246;sen lassen (Koch/Rinke 2017, S. 166). So sieht etwa Birnbacher (2016, S. 120), der 
wie geschildert dem Argument einer spezifischen W&#252;rdeverletzung durch AWS skeptisch gegen&#252;bersteht, auch
unter der Perspektive der Verantwortungsproblematik keinen zwingenden Grund, der gegen einen Einsatz
autonomer Waffensysteme spricht. Und andere Autoren wie der Philosoph Vincent C. M&#252;ller (2016) bestreiten
generell, dass die Existenz einer Verantwortungsl&#252;cke den Einsatz autonomer Technologien im Allgemeinen
und autonomer Waffensysteme im Speziellen moralisch ausschlie&#223;t.
8.4 Fazit
Die Entwicklung und der m&#246;gliche Einsatz von immer autonomer agierenden Waffensystemen schafft gro&#223;e
normative Unsicherheiten, die sich in der Kernfrage zuspitzen, ob und inwiefern es erlaubt sein soll, Maschinen
&#252;ber Tod oder Leben von Menschen entscheiden zu lassen. Es ist v&#246;llig klar, dass diese Fragestellung nur mit
Blick auf die besondere Situation, wie sie im Kriegsfall herrscht, &#252;berhaupt zul&#228;ssig erscheint. Doch selbst in
Kriegen gibt es Grenzen des Erlaubten, die sicherstellen sollen, dass ein gewisses Ma&#223; an Menschlichkeit
gewahrt bleibt. Autonome Waffensysteme fordern diese Grenzen heraus, indem sie zum einen die Entscheidungs-
und Handlungshoheit des Menschen in diesem ethisch-humanit&#228;ren Ausnahmebereich zu untergraben drohen
(Heyns 2016, S. 13) und zum anderen den Anwender milit&#228;rischer Gewalt g&#228;nzlich der Gegengewalt entziehen
(Koch/Rinke 2017, S. 7). Damit stellen sie letztlich den normativen Hintergrund infrage, der bis heute f&#252;r
bewaffnete Konflikte geradezu selbstverst&#228;ndlich angenommen wird.
Der &#220;berblick &#252;ber die weitverzweigte Debatte in Koch und Rinke (2017) zeigt, dass AWS aus ethischer
Sicht zwar kontrovers diskutiert werden, insgesamt aber doch die Zweifel an ihrer Zul&#228;ssigkeit und Legitimit&#228;t
deutlich &#252;berwiegen. Im Kern dreht sich die ethische Debatte um zwei zentrale Streitpunkte:
4. Inwiefern verm&#246;gen AWS die v&#246;lkerrechtlich geforderten Standards im Krieg einzuhalten? Bef&#252;rworter
autonomer Waffentechnologie sehen die Chance, dass AWS dank ihrer &#252;berlegenen sensorischen und
datenverarbeitenden F&#228;higkeiten sogar zu einer Verbesserung der humanit&#228;ren Situation gegen&#252;ber dem
Status quo beitragen k&#246;nnten. Die Kritiker wenden dagegen ein, dass diese Annahme auf der f&#228;lschlichen
Pr&#228;misse beruht, die komplexen und interpretationsbed&#252;rftigen Regularien des humanit&#228;ren V&#246;lkerrechts
lie&#223;en sich eins zu eins in Computercodes transferieren. Dies erscheint als starkes Argument gegen die
M&#246;glichkeit v&#246;lkerrechtskonformer AWS, das es erst einmal zu widerlegen g&#228;lte. Letztlich beruht die
Entscheidung in dieser konsequentialistischen Frage jedoch weniger auf ethischen Einsch&#228;tzungen als auf
solchen technischer Art, die zum jetzigen Zeitpunkt schwierig zu treffen sind, da sich autonome
Waffensysteme erst in einem fr&#252;hen Stadium der Entwicklung befinden und deshalb noch nicht klar absehbar ist,
was sie zu leisten verm&#246;gen und was nicht.
5. Inwiefern ist der Einsatz von AWS mit der W&#252;rde des Menschen vereinbar? Mit der Idee der
Menschenw&#252;rde, die in Deutschland und in vielen anderen freiheitlich-demokratischen Gesellschaften als besonders
sch&#252;tzenswerter Grundwert gilt, ist eine zentrale Verpflichtung verbunden: Der Mensch darf nicht zum
Objekt gemacht werden. In der Debatte wird aus deontologischer Sicht das Argument vorgebracht, dass
der Einsatz letaler Gewalt durch AWS ethisch grunds&#228;tzlich inakzeptabel ist, weil er genau dies impliziert.
Die Opfer werden entw&#252;rdigt, indem sie in einem rein technischen Prozess zu Zielobjekten degradiert
werden, ohne dass dabei die Aussicht auf Achtung ihrer W&#252;rde besteht. Das Argument bringt die starken
moralischen Vorbehalte zum Ausdruck, die gegen&#252;ber einer Dehumanisierung des Krieges bestehen. Es
ist jedoch theoretisch und begrifflich sehr voraussetzungsreich und seine Reichweite entsprechend
umstritten.
Die Gr&#252;nde und Erw&#228;gungen, die gegen AWS vorgebracht werden, sind in ihrer Gesamtheit &#228;u&#223;erst
vielgestaltig und nicht auf einen einfachen Nenner zu bringen. Aus Sicht des TAB sprechen gegen einen Einsatz von
AWS vor allem zwei &#220;berlegungen, wobei die eine konsequentialistisch ausgerichtet, die andere relativ zu
bestimmten Wertegesichtspunkten ist: Erstens gilt es die unkalkulierbaren Risiken zu bedenken, die mit dem
Einsatz autonom agierender Kampfmaschinen verbunden sind (Koch/Rinke 2017, S. 171 f.). Diese Risiken
betreffen technische Fehlfunktionen, die ja nie ganz auszuschlie&#223;en sind und im Falle von autonom agierenden 
Waffensystemen dramatische Folgesch&#228;den nach sich ziehen k&#246;nnen. In Betracht zu ziehen ist aber auch eine
Destabilisierung der internationalen Sicherheitsarchitektur, ausgel&#246;st durch einen m&#246;glichen R&#252;stungswettlauf
und erh&#246;hte Kriegsrisiken (zu diesen sicherheitspolitischen Fragen siehe Kap. 6). Angesichts dessen ist h&#246;chst
fraglich, ob der mit der Einf&#252;hrung dieser Systeme verbundene Kontrollverlust verantwortbar ist, auch wenn
die Risiken in ihrem Umfang zum jetzigen Zeitpunkt noch nicht eindeutig zu bemessen sind. Zweitens hat sich
Deutschland verfassungsrechtlich dazu verpflichtet, das Gebot der Menschenw&#252;rde uneingeschr&#228;nkt zu achten
und zu sch&#252;tzen. Das Argument von der W&#252;rdeverletzung ist f&#252;r den deutschen Staat also von besonderem
Gewicht, und selbst wenn Zweifel an der Reichweite des zugrundeliegenden Arguments bestehen, sollte die
blo&#223;e M&#246;glichkeit eines W&#252;rdeversto&#223;es f&#252;r die Bundesregierung und andere staatliche Akteure Anlass genug
sein, von der Entwicklung und dem Einsatz dieses Waffentyps abzusehen und auf ein internationales Verbot
hinzuwirken.
Gerade der letzte Punkt macht allerdings auch deutlich, dass es nicht die Aufgabe von Ethik ist, im Sinne 
eines &#187;Ethik-T&#220;Vs&#171; (Riecke 2018) oder nach Art einer &#187;Genehmigungsbeh&#246;rde&#171; (Grunwald 2013, S. 6)
kategorische Urteile &#252;ber die ethisch-moralische Bedenklichkeit oder Unbedenklichkeit einer Technologie zu f&#228;llen
(dazu auch Koch/Rinke 2017, S. 6 ff.). Dies ist schon alleine deshalb so, weil ethische Urteile immer von
bestimmten theoretischen und normativen Pr&#228;missen abh&#228;ngen und deshalb niemals eindeutig sein k&#246;nnen. Die
Ethik vermag deshalb die mit dem technologischen Wandel verbundenen normativen Unsicherheiten nicht
aufzul&#246;sen, sie kann nur deren moralische Hintergr&#252;nde reflektieren und zu kl&#228;ren versuchen (Grunwald 2013).
Mit Blick auf AWS ist die Ethik dabei allerdings mit der besonderen Schwierigkeit konfrontiert, dass das
zentrale Definitions- und Abgrenzungsmerkmal dieses Waffentyps, n&#228;mlich die Autonomie, von gro&#223;en
begrifflichen Unsch&#228;rfen gepr&#228;gt ist. So h&#228;ngt die ethische Bewertung von AWS ma&#223;geblich davon ab, welches
Verst&#228;ndnis (moralisch oder operationell) und welcher Grad von Autonomie (teilautonom oder vollautonom)
zugrunde gelegt wird. Dass man es hier mit flie&#223;enden &#220;berg&#228;ngen zu tun hat, erschwert die ethische
Beurteilung au&#223;erordentlich. Klar ist, dass sich die meisten der vorab diskutierten ethischen Bedenken in Luft aufl&#246;sen,
sollte es gelingen, AWS zu konstruieren, bei denen eine ad&#228;quate menschliche Kontrolle des Waffeneinsatzes
(inklusive Zielauswahl) sichergestellt ist. Doch wie die CCW-Verhandlungen zeigen (dazu Kap. 9.2), ist die
Rede von der ad&#228;quaten menschlichen Kontrolle bislang eine ebenso unklare Formel wie die Rede von starker
KI, die &#252;ber menschliche Handlungsqualit&#228;ten verf&#252;gt. Diese Situation erschwert nicht nur die ethische
Bewertung, sondern auch die rechtliche Regulierung der Technologie. Denn sowohl f&#252;r ein generelles Verbot
autonomer Waffen als auch f&#252;r die Etablierung technischer und/oder operationeller Standards, die einen ethischen
Umgang mit den technologischen M&#246;glichkeiten sicherstellen sollen,102 werden Kriterien ben&#246;tigt, die
Waffensysteme gem&#228;&#223; ihren Autonomief&#228;higkeiten &#8211; und damit ihrer moralischen Qualit&#228;t &#8211; differenziert einzuordnen 
verm&#246;gen (Hellstr&#246;m 2013). Sich diesbez&#252;glich auf relevante Unterscheidungs- und Klassifikationsmerkmale
zu einigen, w&#228;re deshalb ein wichtiger erster Schritt, um mit den anstehenden ethischen Herausforderungen
umgehen zu k&#246;nnen.
102 M&#246;gliche Ma&#223;nahmen umfassen beispielsweise die Ausstattung mit einer Blackbox, die alle relevanten Betriebsdaten aufzeichnet
(Kap. 9.3.3), oder technische Vorkehrungen, welche die letalen Einsatzm&#246;glichkeiten begrenzen oder bestimmte riskante
Einsatzzwecke g&#228;nzlich unterbinden (siehe die Ausf&#252;hrungen zu &#187;boxed autonomy&#171; in Kasten 7.1).

        
 
 
  
  
 
  
   
   
  
   
 
   
   
     
  
    
  
     
 
  
  
  
   
     
  
  
 
   
  
  
  
                                              
         
9
9.1
M&#246;glichkeiten der R&#252;stungskontrolle
Autonome Waffensysteme (AWS) werden bislang von keinem R&#252;stungskontrollvertrag explizit erfasst. Dies
ist nachvollziehbar, denn die bestimmende Eigenschaft, die AWS von anderen Waffensystemen abgrenzt,
n&#228;mlich ihre Autonomie, war zum Zeitpunkt der Aushandlung dieser Abkommen im Bereich von Science-Fiction 
angesiedelt und spielt deshalb in den bestehenden Abkommen keine Rolle. Dennoch lassen sich AWS &#8722;
genauso wie unbemannte (fernpilotierte bzw. -&#252;berwachte) Waffensysteme (UWS) &#8722; analog zu
Marschflugk&#246;rpern, Kampfflugzeugen oder Kampfpanzern als Tr&#228;gersysteme auffassen, die unter bestimmten
Voraussetzungen, beispielsweise oberhalb einer bestimmten Gr&#246;&#223;e oder bei Best&#252;ckung mit bestimmten Wirkmitteln, unter
bestehende internationale Abkommen subsumiert werden k&#246;nnen.
Der Fragenkomplex, ob bzw. in welcher Hinsicht Autonomie in Waffensystemen problematisch f&#252;r die 
Erhaltung von Frieden, Sicherheit und Menschenw&#252;rde ist und wie diese ggf. reguliert werden k&#246;nnte, ist derzeit
Gegenstand eines intensiven Gedankenaustauschs auf internationaler Ebene. Dieser wird mit Fokus auf die
Frage der Vereinbarkeit von AWS mit dem humanit&#228;ren V&#246;lkerrecht im Rahmen der CCW in Genf gef&#252;hrt.
R&#252;stungs- und Exportkontrollabkommen mit Relevanz f&#252;r AWS
Im Folgenden werden die relevanten Abkommen daraufhin gepr&#252;ft, ob sie Sachverhalte bzw. Regelungen
enthalten, die auch auf AWS anwendbar sein k&#246;nnen.103 Die in Bezug auf Tr&#228;gersysteme oder Waffenplattformen
relevanten R&#252;stungskontrollvertr&#228;ge sind:
&#8250; der KSE-Vertrag,
&#8250; der New-START-Vertrag,
&#8250; der INF-Vertrag sowie
&#8250; das Chemie- und das Biowaffen&#252;bereinkommen.
Im Bereich der Transparenz sowie vertrauens- und sicherheitsbildender Ma&#223;nahmen (VSBM) sind zu nennen:
&#8250; das Wiener Dokument und
&#8250; das UN-Waffenregister.
Au&#223;erdem sind im Hinblick auf die Nichtverbreitung und Exportkontrolle einschl&#228;gig:
&#8250; das Tr&#228;gertechnologie-Kontrollregime,
&#8250; der Haager Verhaltenskodex gegen die Proliferation ballistischer Raketen,
&#8250; das Wassenaar-Abkommen und
&#8250; der Vertrag &#252;ber den Waffenhandel.
103 Das folgende Kapitel st&#252;tzt sich wesentlich auf das Gutachten von Alwardt et al. 2017.
9.1.1 R&#252;stungskontrollvertr&#228;ge
KSE-Vertrag
Der Vertrag &#252;ber konventionelle Streitkr&#228;fte in Europa (KSE-Vertrag) wurde 1990 unterzeichnet und trat 1991
in Kraft. Er gilt innerhalb eines geografischen Bereichs, der sich &#187;vom Atlantik bis zum Ural&#171; erstreckt.
Ausgehandelt durch die beiden Milit&#228;rallianzen des Kalten Krieges, die NATO und den Warschauer Pakt, wurden
im KSE-Vertrag u. a. gleiche numerische Obergrenzen f&#252;r Hauptwaffensysteme festgeschrieben. Diese
umfassen Kampfpanzer, gepanzerte Kampffahrzeuge, Artilleriewaffen, Kampfflugzeuge und Angriffshubschrauber.
Der KSE-Vertrag kann als der erste und bisher einzige R&#252;stungskontrollvertrag angesehen werden, der
konventionelle Waffensysteme umfassend einschr&#228;nkt und &#252;ber Verifikationsmechanismen (z. B.
Informationsaustausch und Inspektionen) verf&#252;gt. Im November 1999 wurde der Versuch unternommen, den KSE-Vertrag an
die ver&#228;nderten Realit&#228;ten nach dem Ende des Ost-West-Konflikts, der Aufl&#246;sung des Warschauer Pakts und 
der NATO-Erweiterung anzupassen (A-KSE-Vertrag). Der A-KSE-Vertrag wurde von den NATO-Staaten
jedoch nicht ratifiziert, da nach ihrer Auffassung Russland nicht alle vertraglichen Verpflichtungen erf&#252;llt hatte.
Russland suspendierte den urspr&#252;nglichen KSE-Vertrag 2007. Als Gr&#252;nde wurden die A-KSE-
Nichtratifizierung und die US-Raketenabwehrpl&#228;ne in Europa angegeben. Am 11. M&#228;rz 2015 wurde der Vertrag seitens der 
Russischen F&#246;deration aufgek&#252;ndigt.104 
Der KSE-Vertrag ist somit de facto obsolet. Im Rahmen eines strukturierten Dialogs sollen im Kontext der
Organisation f&#252;r Sicherheit und Zusammenarbeit in Europa (OSZE) M&#246;glichkeiten f&#252;r Nachfolgeregelungen
ausgelotet werden. Der strukturierte Dialog wurde auf deutsche Initiative hin etabliert und stellt ein Forum dar,
in dem sich die OSZE-Mitglieder regelm&#228;&#223;ig &#252;ber Bedrohungswahrnehmungen, milit&#228;rische &#220;bungen und
R&#252;stungskontrollmechanismen austauschen (Heinrich 2018). Die Bundesregierung moderiert diesen Prozess.
Unbemannte Waffensysteme werden im KSE-Vertrag nicht explizit beschrieben. Da bei der Definition der
jeweiligen aufgef&#252;hrten Waffensysteme aber auch keine Besatzung erw&#228;hnt oder festgeschrieben wird, k&#246;nnen
die Definitionen sowohl f&#252;r bemannte als auch f&#252;r unbemannte Waffensysteme als g&#252;ltig angesehen werden.105 
Im Rahmen eines Nachfolgeregimes k&#246;nnten UWS/AWS explizit aufgenommen werden. Angesichts der
aktuellen Spannungen zwischen dem Westen und Russland ist eine Neuregelung zurzeit jedoch nicht sehr
wahrscheinlich.
New-START-Vertrag
Der New-START-Vertrag (Treaty between the United States of America and the Russian Federation on
Measures for the Further Reduction and Limitation of Strategic Offensive Arms) ist ein bilateraler
R&#252;stungskontrollvertrag zu strategischen Nuklearwaffen106 zwischen den USA und Russland, unterzeichnet im April 2010
und in Kraft getreten im Februar 2011. Er limitiert die Anzahl der stationierten strategischen Tr&#228;gersysteme
(&#187;intercontinental ballistic missile&#171; &#8211; ICBM; landgest&#252;tzte ballistische Interkontinentalraketen, auf U-Booten 
stationierte ballistische Raketen &#8211; SLBM und schwere Bomber) auf 700 St&#252;ck pro Land. Auf Basis der
stationierten Tr&#228;gersysteme wird die entsprechende Anzahl der stationierten strategischen Nukleargefechtsk&#246;pfe
berechnet, die 1.550 St&#252;ck pro Seite nicht &#252;berschreiten darf. Der New-START-Vertrag listet unter Artikel 3
Absatz 8 die bis dato vorhandenen strategischen Tr&#228;gersystemtypen beider Seiten auf. Der Vertrag sieht
Verifikationsmechanismen vor und hat eine 10-j&#228;hrige Laufzeit (bis 2021), die im Konsens um weitere 5 Jahre
verl&#228;ngert werden kann.
Im Zentrum des Vertrags stehen Bomber und ballistische Raketen mit gro&#223;er Reichweite.
Hyperschallflugk&#246;rper, die von Raketen in den Weltraum transportiert werden und auf einer eigenst&#228;ndigen Flugbahn ihr
Bodenziel ansteuern, fallen nicht unter die Regelungen des New-START-Vertrags. Besonders von russischer
Seite besteht die Sorge, dass dies eine L&#252;cke bei der strategischen bilateralen R&#252;stungskontrolle darstellt, die 
die Stabilit&#228;t der Abschreckungspotenziale unterminiert. Solche Tr&#228;gersysteme mit gro&#223;er Reichweite w&#228;ren
104 Formal handelt es sich nicht um einen offiziellen R&#252;cktritt vom Vertrag nach Artikel XIX Absatz 2 oder Absatz 3, sondern um den
Beschluss, &#187;seine Handlungen im Vertrag ab 11. M&#228;rz 2015 vollst&#228;ndig einzustellen&#171; (RT 2015).
105 Artikel II KSE-Vertrag, offenbar wurde seinerzeit bewusst auf eine Erw&#228;hnung von Besatzung verzichtet, da w&#228;hrend der
Verhandlungen bef&#252;rchtet wurde, k&#252;nftige unbemannte Systeme k&#246;nnten die Vertragsbegrenzungen aushebeln (Richter 2013, S.2).
106 Dies bezeichnet &#187;Kernwaffen mit gro&#223;er Sprengkraft, die nicht auf dem Gefechtsfeld eingesetzt werden, sondern Ziele im
gegnerischen Hinterland zerst&#246;ren sollen&#171; (Wikipedia 2002).
&#228;hnlich wie Marschflugk&#246;rper pr&#228;destiniert f&#252;r die Integration von teilautonomen Komponenten, die
selbstst&#228;ndig Ziele ansteuern.
Neue Tr&#228;gersysteme m&#252;ssen auf Verlangen einer Vertragspartei nach Artikel 5 des Vertrags daraufhin 
gepr&#252;ft werden, ob sie die Kriterien eines strategischen Tr&#228;gersystems erf&#252;llen und daher auch unter den New-
START-Vertrag fallen. Dieses w&#252;rde auch f&#252;r unbemannte Tr&#228;gersysteme wie UCAVs oder UUVs gelten, die
strategische Reichweiten aufweisen und nuklear bewaffnet werden k&#246;nnen. Solche unbemannten strategischen
Flugk&#246;rper oder unbemannten, mit Raketen best&#252;ckten Unterseeboote m&#252;ssten, falls sie diese Kriterien erf&#252;llen,
im Rahmen des New-START-Vertrags oder eines Nachfolgevertrags ber&#252;cksichtigt werden. Die Zukunft von
New START &#252;ber 2021 hinaus ist ungewiss, da beide Vertragsparteien eine Verl&#228;ngerung derzeit offenlassen
(Gramer/Seligman 2019, siehe auch AMF 2020).
INF-Vertrag
Der INF-Vertrag (Treaty Between The United States Of America And The Union Of Soviet Socialist Republics
On The Elimination Of Their Intermediate-Range And Shorter-Range Missiles &#8211; INF Treaty) zwischen den 
USA und Russland, 1987 unterzeichnet und 1988 in Kraft getreten, ist der bisher einzige
R&#252;stungskontrollvertrag, der eine komplette Kategorie an Tr&#228;gersystemen komplett verbietet. Er umfasst bodengest&#252;tzte Kurz- und 
Mittelstreckenraketen sowie Marschflugk&#246;rper mit Reichweiten zwischen 500 und 5.500 km, deren Best&#228;nde
von den USA und Russland bis 1991 vollst&#228;ndig und verifiziert abger&#252;stet wurden. Russland reklamiert, dass
UCAVs, die im INF-Vertrag zwar nicht explizit erw&#228;hnt werden, unter die Definition bodengest&#252;tzter
Marschflugk&#246;rper fallen w&#252;rden (Thielmann/Zagorski 2017, S. 3 f.). Die USA widersprechen dem mit der
Argumentation, dass unbemannte Systeme &#187;die nicht bodengest&#252;tzt sind oder aber ohne Hilfe von Startvorrichtungen
abheben k&#246;nnen und die daf&#252;r vorgesehen sind, von einer Mission zur&#252;ckzukehren&#171;,107 nicht in die Kategorie
eines bodengest&#252;tzten Marschflugk&#246;rpers fallen (DOD 2009, S. 42). Insofern ist umstritten, ob UCAVs vom
Geltungsbereich des INF-Vertrags erfasst werden. Nachdem sich die USA und Russland gegenseitig
vorgeworfen hatten, Vertragsinhalte zu verletzen (Thielmann/Zagorski 2017), suspendierten Anfang Februar 2019 zuerst
die USA und sodann auch Russland den INF-Vertrag (BBC News 2019). Nach Ablauf der 6-monatigen
K&#252;ndigungsfrist zogen sich die USA am 2. August 2019 aus dem Vertrag zur&#252;ck. Die Gefahr eines neuen
Wettr&#252;stens in Europa ist somit akuter denn je (Meier 2019).
Chemie- und Biowaffen&#252;bereinkommen
Das Chemiewaffen&#252;bereinkommen (CW&#220;) von 1993 und das Biowaffen&#252;bereinkommen (BW&#220;) von 1972 
verbieten die Entwicklung, die Herstellung, den Besitz, die Weitergabe und den Einsatz chemischer bzw.
biologischer Waffen. Dem CW&#220; sind bisher 192 Staaten und dem BW&#220; 178 Staaten beigetreten.108 Beide
Abkommen beinhalten auch ein Verbot von Munition, Ger&#228;t, Ausr&#252;stung oder anderer Einsatzmittel, falls diese dazu
bestimmt sind, diese Waffen auszubringen und in feindseliger Absicht einzusetzen. Entsprechend ausger&#252;stete 
oder bewaffnete unbemannte Tr&#228;gersysteme, die dazu bestimmt sind, chemische oder biologische Waffen zum
Einsatz zu bringen, sind somit laut dem CW&#220; bzw. dem BW&#220; verboten. Von diesen Verboten w&#228;ren auch alle
UWS oder AWS betroffen, die zur Ausbringung von chemischen oder biologischen Waffen vorgesehen sind.
Im Sinne der Einhaltung dieser Bestimmungen ist es problematisch, dass viele Systeme modular konstruiert
sind und beispielsweise Tanks und Spr&#252;hvorrichtungen zum Zweck der Ausbringung chemischer und/oder
biologischer Agenzien relativ kurzfristig nachger&#252;stet werden k&#246;nnen. Im Gegensatz zum BW&#220; verf&#252;gt das CW&#220;
mit der Organisation f&#252;r das Verbot chemischer Waffen (Organisation for the Prohibition of Chemical
Weapons &#8211; OPCW) &#252;ber ein aktives Implementierungs- und Verifikationsregime.
In Tabelle 9.1 sind die R&#252;stungskontrollvertr&#228;ge und die f&#252;r AWS relevanten Regelungstatbest&#228;nde in der
&#220;bersicht aufgef&#252;hrt.
107 Im Original: &#187;which are not ground launched, or take off without the aid of launching equipment, and are designed to return from
mission&#171;.
108 F&#252;r weitere Informationen www.auswaertiges-amt.de/DE/Aussenpolitik/Themen/Abrues
tung/BioChemie/Uebersicht-BCWaffen_node.html (1.9.2020)
Tab. 9.1 &#220;bersicht der f&#252;r AWS relevanten R&#252;stungskontrollvertr&#228;ge
Vertrag Rahmen relevanter
Regelungstatbestand
AWS Bestandteil des
Vertrags?
Verifikation
Status
KSE
multilateral
Obergrenzen f&#252;r 
konventionelle
Waffen
ja, falls AWS der
Definition einer der
Hauptwaffentypen
entsprechen
ja obsolet
seit 2015 
New
START
bilateral Begrenzung
strategischer
Offensivwaffentr&#228;ger
ja, autonome UCAVs
oder UUVs, die &#252;ber eine 
strategische Reichweite 
verf&#252;gen und zur
nuklearen Bewaffnung
vorgesehen sind
ja in Kraft
(Laufzeit
bis 2021)
INF bilateral Abr&#252;stung von
Marschflugk&#246;rpern
(500 bis 5500 km 
Reichweite)
umstritten, ob UCAVs 
ggf. Marschflugk&#246;rpern
gleichzustellen sind
(ja) gilt
seit
1991 als
umgegek&#252;ndigt
mit
Wirkung zum 
2.8.2019
setzt
CW&#220;
und
BW&#220;
UN Verbot des
Einsatzes chemischer
und biologischer 
Waffen
ja, falls sie dazu
bestimmt sind, am
Einsatz chemischer
oder biologischer
Waffen mitzuwirken
ja (CW&#220;);
nein 
(BW&#220;)
in Kraft
Quelle: Alwardt et al. 2017, S. 78; TAB 2011, S. 191
9.1.2 Transparenz und vertrauens- und sicherheitsbildende Ma&#223;nahmen
Wiener Dokument
Mit dem Wiener Dokument 2011 &#252;ber vertrauens- und sicherheitsbildende Ma&#223;nahmen verpflichten sich alle
57 Mitgliedstaaten109 der Organisation f&#252;r Sicherheit und Zusammenarbeit in Europa (OSZE 2011), regionale 
Ma&#223;nahmen zur Transparenz und Vertrauensbildung im Bereich der konventionellen R&#252;stung umzusetzen. Das
Ursprungsdokument wurde 1990 von der Vorg&#228;ngerorganisation der OSZE, der Konferenz &#252;ber Sicherheit und 
Zusammenarbeit in Europa (KSZE), verabschiedet und seither regelm&#228;&#223;ig &#252;berarbeitet und erg&#228;nzt.
Die Staaten verpflichten sich mit dem Wiener Dokument in Bezug auf ihre Landstreitkr&#228;fte und
landstationierten Luftkr&#228;fte u. a. zu einem Informationsaustausch &#252;ber die Anzahl, Stationierung und Bewegung von 
Truppen und Hauptwaffentypen, die Vorabmeldung von Gro&#223;man&#246;vern sowie die Vorf&#252;hrung neuer Typen
von Hauptwaffensystemen. Hiermit einhergehend sind auch &#220;berpr&#252;fungs- und Verifikationsmechanismen
vereinbart. Dar&#252;ber hinaus besteht im Hauptsitz der OSZE in der Wiener Hofburg bis heute ein w&#246;chentlich
tagendes Diskussionsforum zur R&#252;stungskontrolle, Vertrauensbildung, Wahlbeobachtung und Konfliktpr&#228;vention.
Im Wiener Dokument werden insbesondere die Hauptwaffentypen des KSE-Vertrags wie Kampfpanzer,
gepanzerte Kampffahrzeuge, Artillerie, Kampfflugzeuge und Angriffshubschrauber aufgef&#252;hrt. Unbemannte
Waffensysteme finden keine gesonderte Erw&#228;hnung. Da aber auch nirgendwo explizit von ausschlie&#223;lich
bemannten Systemen die Rede ist, w&#252;rden AWS (bzw. bewaffnete unbemannte Systeme) jeweils unter die
Waffenkategorien fallen, die in den Ziffern 10.2.5 (Landstreitkr&#228;fte) bzw. 10.5 (Luftstreitkr&#228;fte) spezifiziert werden.
109 Die Mongolei trat der OSZE erst am 21. November 2012 bei, &#252;bernahm aber mit dem Beitritt s&#228;mtliche im Wiener Dokument
festgeschriebenen Verpflichtungen.
Daten zu AWS (bzw. unbemannten Systemen) m&#252;ssten gem&#228;&#223; Ziffer 11.2 (&#187;Daten &#252;ber neue Typen oder
Versionen von Hauptwaffensystemen und Gro&#223;ger&#228;t&#171;) sp&#228;testens dann &#252;bermittelt werden, wenn das betreffende
System erstmals in Dienst gestellt wird.
UN-Waffenregister
Mehr Offenheit und Transparenz beim globalen Transfer von Waffen zu schaffen, ist das Ziel der UN (o. J.c)
mit dem UN-Waffenregister (UN Register of Conventional Arms &#8211; UNROCA), das mit der Resolution der UN-
Generalversammlung 46/36L vom 6. Dezember 1991 etabliert wurde (UN 1991). Das Register wird vom UN-
Generalsekret&#228;r gef&#252;hrt und allen Mitgliedstaaten steht es frei, ihre Im- und Exporte im Bereich konventioneller
Waffen zu melden. Dies bezieht sich prim&#228;r auf die f&#252;nf Hauptwaffentypen des KSE-Vertrags sowie bestimmte
Kriegsschiffe und Raketen/Raketenstartsysteme. Mehr als 170 Staaten haben seitdem entsprechende Daten
geliefert. Seit 2006 sind auch leichte und Kleinwaffen mit einbezogen (von Revolvern &#252;ber Maschinengewehre
bis zu portablen Raketenwerfern). Nach eigener Angabe deckt UNROCA seit zwei Jahrzehnten etwa 90 % des 
globalen Waffenhandels ab.110 Leider beteiligen sich nicht alle Staaten in vollem Umfang und die Daten sind
teilweise widerspr&#252;chlich.111 
Eine ausdr&#252;ckliche Unterscheidung zwischen bemannten und unbemannten Waffensystemen erfolgte in 
den Definitionen der Waffenkategorien urspr&#252;nglich nicht. Daher w&#252;rden UWS bzw. AWS bei Erf&#252;llung der
sonstigen Kriterien unter die jeweiligen Waffenkategorien fallen. Einem Vorschlag von Regierungsexperten
folgend, wurden UCAVs 2016 als eine eigene Unterkategorie (IV b) aufgenommen (UN 2016b): &#187;Unmanned
fixed-wing or variable-geometry wing aircraft, designed, equipped or modified to engage targets by employing
guided missiles, unguided rockets, bombs, guns, cannons or other weapons of destruction&#171; (UN 2016a, S. 29). 
&#220;ber die Aufnahme von unbemannten Hubschraubern in die Liste der Waffenkategorien soll demn&#228;chst beraten
werden. Boden- und seegest&#252;tzte unbemannte Systeme (UGV, USV und UUV) werden bisher nicht gesondert
erw&#228;hnt, sind per Definition aber auch nicht ausgeschlossen. Es ist damit bisher kein internationaler
festgehaltener Konsens, sondern Auslegungssache, ob &#8211; von UCAVs abgesehen &#8211; auch andere unbemannte
Waffenplattformen Bestandteil dieses Vertrags sind oder AWS es zuk&#252;nftig sein werden.
9.1.3 Nichtverbreitung und Exportkontrolle
Tr&#228;gertechnologie-Kontrollregime
Das Tr&#228;gertechnologie-Kontrollregime (Missile Technology Control Regime &#8211; MTCR) wurde 1987 durch die
G7 ins Leben gerufen. Das prim&#228;re Ziel ist es, durch die Kontrolle der Ausfuhr von G&#252;tern und Technologien
die Proliferation von Tr&#228;gersystemen f&#252;r alle Arten von Massenvernichtungswaffen einzuschr&#228;nken. Es handelt
sich hierbei um eine Gruppe von mittlerweile 35 Staaten, die auf informeller und freiwilliger Basis gemeinsame
Exportstandards f&#252;r ballistische Raketen, Marschflugk&#246;rper und UAVs erarbeiten und umsetzen.112 Allerdings
beteiligen sich einige Schl&#252;sselakteure nicht am MTCR, so z. B. China, Israel, Iran, Nordkorea oder Pakistan.113 
Indien trat 2016 als j&#252;ngstes Mitglied dem MTCR bei. Die MTCR-Regeln sind f&#252;r die Mitgliedstaaten nicht 
unmittelbar rechtlich bindend, sondern werden typischerweise in nationales Recht umgesetzt. Bei Exporten
zwischen den MTCR-Mitgliedstaaten, innerhalb der EU und NATO sowie in bestimmten Bereichen der zivilen
Raumfahrt, werden die Richtlinien nicht angewendet.
Das MTCR (2017) unterscheidet zwei Kategorien von G&#252;tern, die unterschiedlich strengen Restriktionen
unterliegen und die im Detail im &#187;Equipment, Software and Technology Annex&#171; aufgef&#252;hrt werden: G&#252;ter der
Kategorie I sollen grunds&#228;tzlich gar nicht ausgef&#252;hrt werden. Ausnahmen von dieser Regel k&#246;nnen im
Einzelfall zul&#228;ssig sein, unter der Bedingung, dass der Importstaat Garantien abgibt, dass das Gut nur im Einklang mit 
110 https://www.unroca.org/ (1.9.2020)
111 Ein Beispiel: Deutschland meldete f&#252;r 2016 folgende Exporte in der Kategorie Kampfpanzer: 41 nach Indonesien, 33 nach Katar,
7 nach Singapur, 1 in die Schweiz. Indonesien und Katar &#252;bermittelten f&#252;r 2016 keinerlei Importdaten. Singapur und die Schweiz 
gaben Berichte ab, meldeten aber keine Importe von Kampfpanzern aus Deutschland, und Spanien meldete den Import von 108
&#187;Leopard 2&#171;, die jedoch in der Exportbilanz Deutschlands nicht aufgef&#252;hrt sind (UN 2016c, 2016d, 2016e u. 2016f).
112 http://mtcr.info/ (1.9.2020)
113 Israel folgt den MTCR-Regeln, ohne Mitglied zu sein. China stellte 2004 einen Antrag auf MTCR-Mitgliedschaft. Dieser wurde 
jedoch aufgrund von Bedenken hinsichtlich des Standards der chinesischen Exportkontrollen abschl&#228;gig beschieden (ACA 2017).
den MTCR-Regeln verwendet wird. Dies betrifft komplette Raketensysteme und UAVs (einschlie&#223;lich Cruise
Missiles und Aufkl&#228;rungsdrohnen), die eine Nutzlast von 500 kg oder mehr &#252;ber mindestens 300 km tragen
k&#246;nnen, sowie deren Subsysteme und Komponenten (beispielsweise Triebwerke oder Steuerungssysteme). Der
Export von Ger&#228;ten oder Einrichtungen zur Produktion von Kategorie-I-Komponenten soll ausnahmslos
verboten werden. F&#252;r den Export von G&#252;tern der Kategorie II besteht ein Ermessensspielraum nicht zuletzt aus dem
Grund, als es sich hierbei oft um Dual-Use-G&#252;ter handelt. Deren Export ist statthaft, solange sie nicht dazu
gedacht sind, in Form eines Systems der Kategorie I oder zur Ausbringung von Massenvernichtungswaffen
verwendet zu werden. Kategorie II umfasst Tr&#228;gersysteme mit Nutzlasten unter 500 kg (bei einer Reichweite
&#252;ber 300 km) sowie andere festgelegte Risikotechnologien.
Kritiker werfen dem MTCR vor, dass es sich um ein diskriminierendes Regime handelt, da f&#252;r einige
Empf&#228;ngerstaaten in der Vergangenheit bereits Ausnahmen gemacht wurden und dies vor allem dazu dienen
k&#246;nnte, bestimmten Staaten gewisse Technologien vorzuenthalten (Mallik 2004, S. 11).
Bestimmte Kategorien von UCAVs oder deren Bestandteile werden bereits vom MTCR erfasst, dasselbe
gilt damit auch f&#252;r bestimmte AWS. Der MTCR-Annex und die darin gelisteten G&#252;ter unterliegen einer
st&#228;ndigen Revision. Das MTCR k&#246;nnte daher auch zuk&#252;nftig um weitere Technologien erweitert werden, die eine
Relevanz f&#252;r AWS aufweisen. Ob dies eine realistische Perspektive darstellt, ist allerdings fraglich. Derzeit gibt
es im Gegenteil Bestrebungen, UCAVs aus dem MTCR herauszunehmen. Diese werden ma&#223;geblich von US-
amerikanischen Herstellern von Drohnen unterst&#252;tzt, mit dem Argument, das Abkommen benachteilige sie
gegen&#252;ber Wettbewerbern aus L&#228;ndern, die diesem nicht angeh&#246;ren (vor allem China), und schneide sie von einem
milliardenschweren Zukunftsmarkt f&#252;r zivile und milit&#228;rische Drohnen ab (Sch&#246;rnig 2017, S. 12 f.).
Haager Verhaltenskodex gegen die Proliferation ballistischer Raketen
Der Haager Verhaltenskodex gegen die Proliferation ballistischer Raketen (Hague Code of Conduct against
Ballistic Missile Proliferation &#8211; HCoC) verfolgt im Prinzip dieselbe Zielsetzung wie das MTCR. Im Unterschied
zum MTCR hat der HCoC mit derzeit 139 Staaten (Stand 2018) eine wesentlich breitere Mitgliederbasis.
Allerdings beinhaltet er lediglich Prinzipien, Verpflichtungen und Vorschl&#228;ge f&#252;r vertrauensbildende
Ma&#223;nahmen, z. B. die Ank&#252;ndigung von Raketenstarts und die Erstellung von j&#228;hrlichen nationalen Berichten &#252;ber
Raketenprogramme und -best&#228;nde. Eindeutige Verbotsnormen oder Kooperationsanreize enth&#228;lt er hingegen
nicht. Auch bezieht sich der HCoC nur auf ballistische Raketen. Cruise Missiles114 und UAVs sind nicht
Bestandteil des Abkommens.
Ein Vorschlag der Weapons of Mass Destruction Commission (WMDC 2006, S. 143) unter dem Vorsitz
des ehemaligen schwedischen Au&#223;enministers und Leiters der United Nations Monitoring, Verification and 
Inspection Commission (UNMOVIC) (Amtszeit 2000&#8211;2003) Hans Blix, Cruise Missiles und UAVs in den
HCoC einzubeziehen sowie ein multilaterales Zentrum zum transparenten Datenaustausch einzurichten, wurde
bis heute von der Staatengemeinschaft nicht aufgegriffen. Daher ist der HCoC f&#252;r AWS derzeit nicht von
besonderer Bedeutung.
Wassenaar-Abkommen
Das Wassenaar-Abkommen (Wassenaar Arrangement zu Exportkontrollen f&#252;r konventionelle R&#252;stungsg&#252;ter
und Dual-Use-G&#252;ter [Waren, Software und Technologie]) von 1996 soll der St&#228;rkung der Exportkontrolle im
Bereich von R&#252;stungsg&#252;tern und sensitiven Technologien, insbesondere auch solchen mit Dual-Use-Charakter,
dienen. Hervorgegangen ist es aus dem Coordinating Committee on Multilateral Export Controls (COCOM),
urspr&#252;nglich w&#228;hrend des Kalten Krieges etabliert, um die Lieferung sensitiver G&#252;ter an die Staaten des
Ostblocks zu verhindern.
Heute umfasst das Wassenaar-Abkommen 41 Mitgliedstaaten. Diese erstellen Listen sensitiver G&#252;ter 
(Wassenaar Arrangement 2017), deren Export an Drittstaaten reglementiert wird.115 Hierbei orientieren sich die
114 Cruise Missiles (Marschflugk&#246;rper) weisen technologisch viele Merkmale bewaffneter UAVs auf. Der wesentliche Unterschied ist,
dass sie nicht auf Wiederverwendbarkeit ausgelegt sind, da bei ihnen das Wirkmittel und das Tr&#228;gersystem eine Einheit bilden.
Eine eindeutige Klassifizierung ist allerdings oft schwierig.
115 In Deutschland ist dies vor allem durch das Gesetz &#252;ber die Kontrolle von Kriegswaffen (KrWaffG) und das Au&#223;enwirtschaftsgesetz
(AWG) in Verbindung mit der Au&#223;enwirtschaftsverordnung (AWV) geregelt. Die in der AWV aufgef&#252;hrten R&#252;stungsg&#252;ter
orientieren sich eng an der Liste des Wassenaar-Abkommens.
Mitglieder an gemeinsamen Best Practices (Wassenaar Arrangement Secretariat 2019). Ferner stehen sie einem
freiwilligen Informationsaustausch sowohl &#252;ber erfolgte als auch &#252;ber verweigerte R&#252;stungs- und
Technologieexporte.
Explizit im Wassenaar-Abkommen erw&#228;hnt sind UAVs (in Category 9: Aerospace and Propulsion) sowie
unbemannte Unterwasserfahrzeuge (UUVs, in Category 8: Marine). Dar&#252;ber hinaus ist eine Reihe f&#252;r
unbemannte Systeme unmittelbar relevanter Technologien in den Kontrolllisten aufgef&#252;hrt (TAB 2011, S. 198 f.). 
Andere unbemannte Tr&#228;gersysteme finden keine explizite Erw&#228;hnung, allerdings wird auch im Wassenaar-
Abkommen nicht zwischen bemannt und unbemannt differenziert &#8211; weshalb auch UGVs, USVs und somit jegliche 
zuk&#252;nftige AWS bei Zutreffen der entsprechenden Kriterien den Exportkontrollen des Wassenaar-Abkommens
unterworfen w&#228;ren.
Da bereits heute auch bestimmte Software bzw. Algorithmen in den Kontrolllisten aufgef&#252;hrt sind (z. B. 
einige kryptografische Verfahren oder Software, um akustische, optische und andere Sensordaten auszuwerten)
(Wassenaar Arrangement 2017), w&#228;re es perspektivisch durchaus m&#246;glich, kritische Softwarekomponenten von 
AWS in das Kontrollregime des Wassenaar-Abkommens einzubeziehen.
Vertrag &#252;ber den Waffenhandel
Der von der UN-Generalversammlung angenommene Vertrag &#252;ber den Waffenhandel (Arms Trade Treaty &#8211;
ATT) (UN 2013) soll helfen, weltweit g&#252;ltige Standards f&#252;r den Export, Import und den Transfer von
konventionellen Waffen zu schaffen und somit diesen zu regulieren. Dem Vertrag sind bisher 92 Staaten beigetreten
(UN o. J.a).116 In Artikel 2 des Vertrags werden die Waffenkategorien definiert, die der ATT einschlie&#223;t. Hier
finden sich &#8211; analog zum UN-Waffenregister &#8211; Kampfpanzer, gepanzerte Kampffahrzeuge, schwere
Artilleriewaffen, Kampfflugzeuge und Angriffshubschrauber sowie Kriegsschiffe. Der Vertrag nimmt nicht direkt auf 
unbemannte oder autonome Waffensysteme Bezug, verweist hingegen in Artikel 5 Absatz 3 auf die Definition
konventioneller Waffen im UN-Waffenregister.117 Da im Jahr 2016 UCAVs als Unterkategorie in das UN-
Waffenregister aufgenommen wurden, gilt dies wohl auch f&#252;r den ATT. UGVs, USVs und UUVs werden im
UN-Waffenregister nicht gesondert erw&#228;hnt, aber per Definition auch nicht ausgeschlossen. Es ist damit &#8211; wie 
beim UN-Waffenregister &#8211; auch im Falle des ATT Auslegungssache, ob neben UCAVs auch andere
unbemannte Waffenplattformen (und damit zuk&#252;nftig auch AWS) bereits Bestandteil dieses
Informationsabkommens sind oder ob diese in der Zukunft explizit aufgenommen werden k&#246;nnen.
Eine &#220;bersicht &#252;ber die wichtigsten internationalen Abkommen zu VSBM, Nichtverbreitung und
Exportkontrolle bietet Tabelle 9.2.
Tab. 9.2 Transparenz- und vertrauensbildende Ma&#223;nahmen, Nichtverbreitung und
Exportkontrolle
Abkommen Rahmen AWS Bestandteil des Verifi- Art des
Vertrags? kation Abkommens
Wiener Doku- multilateral ja, AWS im Allgemeinen* ja VSBM der
ment OSZE-Staaten
UN-Waffen- UN ja, autonome UCAVs nein internationale
register wahrscheinlich AWS im VSBM
Allgemeinen**
MTCR multilateral ja, autonome UCAVs nein Exportregime
Wassenaar- multilateral ja, autonome UCAVs; UAVs nein Exportregime
Abkommen werden explizit erw&#228;hnt
116 Im April 2019 erkl&#228;rte US-Pr&#228;sident Trump, dass die USA den ATT nicht weiter unterst&#252;tzen werden (Smith 2019).
117 Artikel 5 Absatz 3 des ATT im Original: &#187;National definitions of any of the categories covered under Article 2 (1) (a)&#8211;(g) shall not cover
less than the descriptions used in the United Nations Register of Conventional Arms at the time of entry into force of this Treaty.&#171;
        
 
 
 
   
 
 
  
 
  
    
  
     
   
       
   
        
    
    
       
  
     
     
     
   
  
    
   
   
  
   
  
    
   
   
  
 
  
     
   
  
     
    
                                              
     
   
          
9.2
wahrscheinlich AWS im
Allgemeinen*
ATT UN ja, autonome UCAVs
wahrscheinlich AWS im
Allgemeinen*
nein internationale
Normen zum 
Waffenhandel
* analog zum KSE
** Falls AWS der Definition einer der Hauptwaffentypen entsprechen; die dortigen
Definitionen schlie&#223;en unbemannte Systeme nicht aus (wahrscheinlich Auslegungssache).
Quelle: Alwardt et al. 2017, S. 80
Die Konvention &#252;ber bestimmte konventionelle Waffen
Das zentrale Forum f&#252;r die Debatte um eine m&#246;gliche Einhegung von AWS auf internationaler Ebene ist die
Konvention &#252;ber bestimmte konventionelle Waffen (Convention on Certain Conventional Weapons &#8211; CCW)
der UN (o. J.b).118 Dieses UN-Abkommen wurde 1980 in Genf beschlossen, trat im Dezember 1983 in Kraft
und wurde bisher von 125 Staaten unterzeichnet. Das Ziel der CCW besteht darin, (neue) konventionelle Waffen
daraufhin zu bewerten, ob ihr Einsatz &#252;berm&#228;&#223;iges Leiden verursachen oder unterschiedslos wirken kann und 
sie daher in erkl&#228;rten Kriegen oder bewaffneten Konflikten zu verbieten oder zu beschr&#228;nken sind. Neben dem
eigentlichen CCW-Rahmenvertrag sind bisher f&#252;nf Protokolle verabschiedet worden, die sich mit der
Reglementierung bestimmter konventioneller Waffentypen besch&#228;ftigen: Das Protokoll I (1980) verbietet den Einsatz
von Waffen, die durch in R&#246;ntgenuntersuchungen nicht entdeckbare Splitter wirken; Protokoll II (1980,
ge&#228;ndert 1996) regelt den Einsatz von Minen, Sprengfallen und andere Vorrichtungen; Protokoll III (1980) hat
Brandwaffen zum Inhalt; Protokoll IV (1995) verbietet blindmachende Laserwaffen, und Protokoll V (2003) 
befasst sich mit explosiven Kampfmittelr&#252;ckst&#228;nden.
Seit 2014 steht das Thema AWS auf der Tagesordnung, anf&#228;nglich im Rahmen informeller
Expertengruppen. 2016 wurde eine Group of Governmental Experts (GGE) etabliert, deren Rolle es ist, technologische und
definitorische Fragen zu kl&#228;ren und ggf. den Weg f&#252;r formale Verhandlungen &#252;ber ein Verbot oder eine
anderweitige Regulierung von AWS zu bereiten.
Neben einem ethischen und einem (v&#246;lker)rechtlichen Diskussionsstrang standen anf&#228;nglich Versuche im
Mittelpunkt, verschiedene Grade an Autonomie mithilfe technologischer Kriterien zu bestimmen und daraus 
Definitionen f&#252;r AWS abzuleiten, die es erm&#246;glichen sollten, diese von anderen automatischen Waffensystemen
abzugrenzen. Das Fehlen einer allgemein g&#252;ltigen Definition von AWS und der Umgang mit diesem Defizit
bestimmten die Debatte. Einige sahen die Notwendigkeit, sich zun&#228;chst &#252;ber gemeinsame Charakteristika von
&#187;lethal autonomous weapon systems&#171; (LAWS119) zu verst&#228;ndigen, andere wiesen auf die Schwierigkeiten
dieses Unterfangens hin oder bezweifelten teilweise dessen Machbarkeit.
In der Folge konzentrierte sich die Debatte verst&#228;rkt auf die Art und Weise und das Ausma&#223;, in dem
Menschen die Kontrolle &#252;ber AWS aus&#252;ben. Dies war der Erkenntnis geschuldet, dass zentrale Fragen der
V&#246;lkerrechtskonformit&#228;t von AWS nicht nur von technischen, sondern auch ma&#223;geblich von operationellen und
anderen Kontextfaktoren ihres Einsatzes bestimmt werden. Dar&#252;ber hinaus bestand (und besteht immer noch) bei 
den Staatenvertretern eine breite &#220;bereinstimmung, dass es keine autonomen Waffensysteme geben soll, die
ohne menschliche Beteiligung die Entscheidung &#252;ber den Einsatz von Gewaltmitteln gegen Menschen treffen
k&#246;nnen bzw. d&#252;rfen (CCW 2015, S. 4).
118 Vertragstext in derzeit aktueller Fassung von 2001.
119 LAWS ist die im Kontext der CCW verwendete Abk&#252;rzung f&#252;r autonome Waffensysteme. Das &#187;lethal&#171; sollte anf&#228;nglich zur
Unterscheidung von &#187;cyber&#171; dienen, wird aber inzwischen meist als &#187;f&#252;r Menschen (zumindest potenziell) t&#246;dlich&#171; verstanden.
9.2.1 Menschliche Kontrolle &#252;ber AWS
Nicht jede Art menschlicher Kontrolle im Sinne von &#187;human in the loop&#171; ist ausreichend, um den politischen, 
ethischen und (v&#246;lker)rechtlichen Mindestanforderungen f&#252;r verantwortliches Handeln gerecht zu werden. Das
krasseste Gegenbeispiel w&#228;re ein Mensch, dessen Aufgabe es w&#228;re, mittels Knopfdrucks einen Angriff
freizugeben, und der als einzige Information das Aufleuchten eines Lichts h&#228;tte, das signalisierte, dass &#187;das System&#171;
diesen Angriff priorisierte. Ausgehend von dieser &#220;berlegung wurde das Konzept &#187;Meaningful Human
Control&#171;120 (MHC), von der britischen NGO Article 36 (2013, S. 3 f.) gepr&#228;gt und auf folgende Weise definiert:121 
F&#252;r die Aus&#252;bung von MHC &#252;ber individuelle Angriffe m&#252;ssen mindestens folgende Voraussetzungen 
erf&#252;llt sein:
&#8250; Information: Ein menschlicher Bediener und diejenigen, die verantwortlich f&#252;r die Planung eines Angriffs
sind, m&#252;ssen &#252;ber ad&#228;quate Kontextinformationen &#252;ber das Zielgebiet verf&#252;gen, dar&#252;ber, warum ein
bestimmtes Objekt als Ziel vorgeschlagen wird, &#252;ber die Zielsetzungen der Operation sowie &#252;ber die
unmittelbaren und langfristigen Auswirkungen des Waffeneinsatzes in diesem Kontext.
&#8250; Aktive Handlung: Ein Angriff darf nur durch eine aktive Handlung eines menschlichen Bedieners initiiert
werden.
&#8250; Rechenschaftspflicht: Wer verantwortlich daf&#252;r ist, die Informationen zu bewerten und den Angriff
durchzuf&#252;hren, muss f&#252;r die Folgen des Angriffs zur Rechenschaft gezogen werden k&#246;nnen.
Der Begriff MHC wurde zwar breit adoptiert, allerdings nicht von allen Akteuren im gerade skizzierten
Bedeutungsumfang. Auch wird nicht immer transparent kommuniziert, wie der Terminus verstanden wird bzw.
werden soll, was die Diskussion mitunter erschwert. So wird beispielsweise nicht von allen der explizite Bezug auf
individuelle Angriffe geteilt. Dies ist sehr bedeutsam, da MHC &#252;ber individuelle Angriffe eine wesentlich
stringentere Kontrolle impliziert als &#252;ber AWS ganz allgemein oder lediglich &#252;ber kritische Funktionen (d. h.
insbesondere Zielauswahl und Zielbek&#228;mpfung) von AWS (UNIDIR 2014b).
Um das gemeinsame Verst&#228;ndnis &#252;ber AWS zu sch&#228;rfen und daraus ggf. Kriterien abzuleiten, um
akzeptable von unakzeptablen AWS (bzw. deren Einsatzspektrum) unterscheiden zu k&#246;nnen, kann die Diskussion um
MHC sich als n&#252;tzlich erweisen. Zu kl&#228;rende Fragen sind u. a. (Davison 2017; IKRK 2018a; UNIDIR 2014b,
S. 3 ff.):
&#8250; Wann im Produktzyklus eines AWS ist MHC prim&#228;r umzusetzen: wenn das System f&#252;r den Einsatz
aktiviert wird (wie bei den meisten konventionellen Waffen) oder w&#228;hrend des Einsatzes oder wesentlich
fr&#252;her, z. B. wenn die Waffe entwickelt bzw. designt wird?
&#8250; In welchen Phasen eines beabsichtigten Angriffs soll MHC greifen: w&#228;hrend der Gewinnung von
Aufkl&#228;rungsdaten, der Analyse des Kontextes, der Zielidentifikation, der Zielauswahl inkl. Abw&#228;gungen
bez&#252;glich des HVR (Unterscheidungsgebot, Verh&#228;ltnism&#228;&#223;igkeit, Vorsorgeprinzip) oder der letztendlichen
Entscheidung zum Angriff?
&#8250; Was soll MHC unterworfen sein: das AWS selbst, bestimmte Funktionen des AWS, jeder individuelle 
Angriff oder etwas anderes?
120 Zu Deutsch in etwa bedeutsame menschliche Kontrolle; &#187;control&#171; kann im Deutschen sowohl Kontrolle als auch Steuerung
bedeuten. Im Kontext MHC ist nach Ansicht des TAB Kontrolle im Sinne von situativem Verst&#228;ndnis plus Eingriffsm&#246;glichkeit
vorzuziehen, da Steuerung als direkte Manipulation (an einem Joystick oder &#196;hnlichem) missdeutet werden kann.
121 &#220;bersetzung durch das TAB; im Original: &#187;Requirements for meaningful human control over individual attacks include, but are
not necessarily limited to:
&#8226; Information &#8211; a human operator, and othersresponsible for attack planning, need to have adequate contextual information on
the target area of an attack, information on why any specific object has been suggested as a target for attack, information on 
mission objectives, and information on the immediate and longer-term weapon effects that will be created from an attackin that
context.
&#8226; Action &#8211; initiating the attackshould require a positive action by a human operator.
&#8226; Accountability &#8211; those responsible for assessing the information and executing the attack need to be accountable for the
outcomes of the attack.&#171;
&#8250; Welche Vorkehrungen w&#252;rden gew&#228;hrleisten, dass MHC tats&#228;chlich ausge&#252;bt wird?
&#8250; W&#252;rde eine auf MHC basierende Norm ggf. bereits existierende Waffensysteme ber&#252;hren?
&#8250; Wie k&#246;nnte MHC ausge&#252;bt werden, wenn keine Kommunikationsverbindung zu dem AWS existiert?
&#8250; Welches Ausma&#223; menschlicher Kontrolle ist erforderlich, um &#187;meaningful&#171; zu sein, und wie h&#228;ngt dieses
ab von operationellen Gegebenheiten (Art des Ziels, Umgebung etc.)?
&#8250; Wie gut muss das Verhalten von AWS vorhersagbar sein? Wie verl&#228;sslich m&#252;ssen AWS sein?
&#8250; Wie kann MHC &#252;berhaupt erfolgen, wenn die Zeitr&#228;ume, in denen rechnergest&#252;tzte Entscheidungen gef&#228;llt
werden, so kurz werden, dass Menschen dem Geschehen nicht mehr folgen, geschweige denn es
kontrollieren und eingreifen k&#246;nnen?
Zur Illustration der Komplexit&#228;t und des Facettenreichtums der Diskussionen im Rahmen der CCW, bei der
zum gegenw&#228;rtigen Zeitpunkt das grunds&#228;tzliche begriffliche und konzeptuelle gemeinsame Verst&#228;ndnis im
Zentrum steht, zeigt Tabelle 9.3 eine &#220;bersicht &#8211; erstellt vom Vorsitzenden der CCW &#8211; &#252;ber verschiedene
M&#246;glichkeiten, menschliche Kontrolle &#252;ber AWS begrifflich zu fassen (ohne Anspruch auf Vollst&#228;ndigkeit).
Tab. 9.3 Formulierungsvarianten zur menschlichen Kontrolle &#252;ber AWS
maintaining substantive human participation
(Aufrechterhaltung) (substanzieller) (menschlicher) (Beteiligung)
ensuring meaningful involmement
(Sicherung) (sinnvoller) (Mitwirkung)
exerting appropriate responsibility
(Aus&#252;bung) (angemessener) (Verantwortung)
preserving sufficient supervision
(Bewahrung) (ausreichender) (&#220;berwachung)
Minimum level of validation
(Mindestniveau an) (Validierung)
Minimum indispensable control
(unabdingbares Mindest- (Kontrolle)
ma&#223; an) judgement
(Urteil)
decision
(Entscheidung)
Quelle: Zusammenfassung des CCW-Vorsitzes, CCW GGE 2018a, S. 7
Das US-Verteidigungsministerium lehnt beispielsweise das Konzept MHC ab und stellt dagegen in den
Vordergrund, dass Kommandeure und Operatoren &#187;angemessene Niveaus menschlicher Beurteilung &#252;ber den
Einsatz von Gewaltmitteln&#171;122 aus&#252;ben sollen, was &#8211; insbesondere in der deutschen &#220;bersetzung &#8211; auf den ersten
Blick recht &#228;hnlich klingen mag.
Die USA f&#252;hren weiter aus, dass es kein einheitliches festgeschriebenes angemessenes Niveau
menschlicher Beurteilung geben kann, da dies vom Kontext abh&#228;ngt. In bestimmten F&#228;llen ist weniger menschliche
Beteiligung sogar w&#252;nschenswert, da die Nutzung autonomer Funktionen h&#246;here Pr&#228;zision und
Geschwindigkeit aufweisen w&#252;rde, als dies bei menschlicher Kontrolle m&#246;glich w&#228;re (beispielsweise bei SARMO-
Systemen) (CCW GGE 2018j, S. 2 f.).
122 Im Original: &#187;appropriate levels of human judgment over the use of force&#171; (DOD 2012).
Kritiker bef&#252;rchten wiederum, dass unter bestimmten Bedingungen auch das Ingangsetzen eines AWS
ohne spezifische weitere menschliche Kontrolle als angemessen interpretiert werden k&#246;nnte. So wird im
zitierten Dokument (DOD 2012) als eine Voraussetzung f&#252;r angemessene Kontrolle genannt, dass ein menschlicher
Bediener &#187;einzelne Ziele oder spezifische Gruppen von Zielen&#171; ausw&#228;hlt, bevor sie angegriffen werden. Mit
dieser vage gehaltenen Umschreibung kann auch ein Auftrag in der Form &#187;Zerst&#246;re alle feindlichen Fahrzeuge
im Operationsgebiet!&#171; konform gehen, ohne dass weitere menschliche Kontrolle gefordert w&#228;re
(Altmann/Gubrud 2017, S. 206 ff.).
Wie diese Diskussion sich weiterentwickelt und ob bzw. auf welche genaue Ausformulierung sich die
Staaten im Rahmen der Diskussionen in der CCW verst&#228;ndigen k&#246;nnten, ist gegenw&#228;rtig nicht abzusehen.
9.2.2 Positionen wichtiger Staaten bzw. Organisationen
Im Folgenden werden die Positionen einiger wichtiger Akteure beleuchtet, wie sie im Rahmen der CCW
vorgebracht werden. Wie verstehen sie AWS und wie grenzen sie diese von anderen Waffentypen ab, welche
Vorteile, Nachteile und Implikationen verbinden sie mit AWS, welche Form menschlicher Kontrolle wird als
ad&#228;quat angesehen und welche Folgerungen werden daraus gezogen in Bezug auf Fragen der Regulierung von 
AWS? Als Grundlage hierf&#252;r diente die Zusammenstellung des United Nations Institute for Disarmament
Research (UNIDIR 2017, S. 23 ff.), die erg&#228;nzt und aktualisiert wurde.
Deutschland
Die deutsche Regierung hat &#252;ber zwei Legislaturperioden hinweg stets erkl&#228;rt, dass sie sich &#187;aktiv f&#252;r die
&#196;chtung letaler autonomer Waffensysteme (einsetzt), die dem Menschen die Entscheidungsgewalt &#252;ber Leben und
Tod entziehen&#171; (Bundesregierung 2018f). Dies l&#228;sst einen gewissen Interpretationsspielraum offen, ob &#196;chtung
beispielsweise mit international verbindliches Verbot gleichgesetzt werden kann.
Auf der CCW (CCW GGE 2018f) wurde die Position so umrissen: Die ultimative Entscheidung &#252;ber
Leben und Tod muss weiterhin ausschlie&#223;lich Menschen vorbehalten sein. Daher wird ein Verzicht auf die
Entwicklung bzw. Beschaffung von Waffen erkl&#228;rt, die den menschlichen Faktor beim Waffeneinsatz gegen
Menschen vollst&#228;ndig ausschlie&#223;en. Konkret werden Waffensysteme abgelehnt, die daf&#252;r ausgelegt sind, t&#246;dliche 
Effekte oder andere Sch&#228;den gegen menschliche Wesen zu richten und die v&#246;llig ohne Interaktion bzw.
Kontrolle durch Menschen Wahrnehmungen generieren, Folgerungen ziehen, entscheiden, handeln, evaluieren und 
lernen. Die F&#228;higkeit zu lernen und eine Eigenwahrnehmung zu entwickeln wird als wesentliches Attribut
genannt, um eine Funktion bzw. ein System autonom zu nennen. Dabei ist man sich mit der franz&#246;sischen Position
einig, dass heute AWS (wie hier definiert) noch nicht existieren (CCW GGE 2018e).
Dass bei der Definition die Betonung auf einen vollst&#228;ndigen Ausschluss des menschlichen Faktors beim
t&#246;dlichen Waffeneinsatz gelegt wird, er&#246;ffnet allerdings ein weites Feld f&#252;r autonome Systeme, die nicht unter
diese Definition fallen w&#252;rden, da sie eine minimale menschliche Aufsicht aufweisen, und sei sie noch so
gering. Dass hier ein wesentlicher Kl&#228;rungsbedarf besteht, wird von der Bundesregierung anerkannt, indem sie 
feststellt, dass &#187;eine Einigung &#252;ber Mindeststandards wirksamer menschlicher Kontrolle zentrales Element in
der Diskussion &#252;ber Handlungsoptionen der CCW-Vertragsstaaten&#171; (Bundesregierung 2018f) sein muss. Eine
ausformulierte ressort&#252;bergreifend abgestimmte nationale Position zu diesen konzeptionellen Fragen k&#246;nnte
dazu beitragen, diese Diskussion inhaltlich voranzutreiben.
Deutschland spielt in den Debatten im Rahmen der CCW eine treibende Rolle und hat u. a. in den Jahren 
2015 und 2016 den Vorsitz der Expertentreffen dazu genutzt, Bedingungen f&#252;r einen Konsens auszuloten. Diese
Konsensorientierung ist im Kontext der CCW verst&#228;ndlich und naheliegend, da f&#252;r Beschl&#252;sse Einstimmigkeit 
erforderlich ist. Andererseits geht das Einnehmen dieser vermittelnden Rolle damit einher, dass darauf
verzichtet wurde, die beschriebene inhaltliche Position der Bundesregierung offensiver zu formulieren und in die CCW
einzubringen.
Kasten 9.1 Aktuelle deutsche Position
Das Ausw&#228;rtige Amt hat auf dem Arbeitstreffen der CCW GGE im M&#228;rz 2019 folgende Elemente einer
Definition f&#252;r Autonomie in LAWS vorgestellt (AA 2019b):
&#8250; die F&#228;higkeit, eine Umwelt wahrzunehmen (sensorisch zu erfassen und zu interpretieren);
&#8250; die Umst&#228;nde einer sich ver&#228;ndernden Situation evaluieren ohne Bezugnahme auf vordefinierte Ziele;
&#8250; abw&#228;gen und ausw&#228;hlen der geeignetsten Vorgehensweise;
&#8250; auf Basis dieser Schlussfolgerungen Aktionen initiieren;
&#8250; all dies ohne menschliche Beteiligung, nachdem das System aktiviert wurde.
Gleichzeitig wird betont, dass aus deutscher Sicht eine exakte, von allen Beteiligten geteilte Definition f&#252;r
den weiteren Fortschritt des CCW-Prozesses nicht notwendig ist. Das bisher erzielte gemeinsame
Verst&#228;ndnis w&#228;re ausreichend, um nun in Richtung auf ein Ergebnisdokument voranzuschreiten. Hierf&#252;r w&#228;ren die
2018 formulierten m&#246;glichen Leitlinien (Kasten 9.5) eine gute Basis (AA 2019a, 2019c u. 2019d).
Frankreich
Frankreich hat in einem informellen Arbeitspapier im Rahmen der CCW eine sehr enge Definition f&#252;r LAWS
vorgebracht, die u. a. verlangt, dass &#252;berhaupt keine Kommunikationsverbindung mit der menschlichen
Kommandokette vorliegt: &#187;LAWS sollten so verstanden werden, dass sie ein vollst&#228;ndiges Fehlen menschlicher
Aufsicht implizieren. Dies bedeutet, dass keinerlei Verbindung (zur Kommunikation oder Kontrolle) mit der
milit&#228;rischen Kommandokette besteht.&#171;123 Zugleich wird aber konstatiert, dass solche Systeme, die sich zu
100 % auf die interne Modellierung der Umgebung verlassen m&#252;ssten, beim ersten unvorhergesehenen Ereignis
unvorhersehbar reagieren w&#252;rden. Dies w&#252;rde sie milit&#228;risch nutzlos machen (CCW GGE 2016b). Es ist kaum 
vorstellbar, dass Staaten mit einem gewissen Verantwortungsbewusstsein derartige Waffen anstreben bzw.
entwickeln.
Gleichzeitig hat Frankreich erkannt, dass der Umgang mit der milit&#228;rischen Nutzung von KI in einer
nationalen KI-Strategie eine wesentliche Rolle spielen muss. Im Bericht von Villani (2018), der den Ausgangspunkt
f&#252;r eine franz&#246;sische KI-Strategie markiert, werden einige konkrete Vorschl&#228;ge gemacht, wie die Proliferation
von AWS eingeschr&#228;nkt werden kann. Dies reicht von nationalen Exportregeln &#252;ber eine internationale
Beobachtungsstelle, um m&#246;gliche Gefahren durch AWS fr&#252;hzeitig zu erkennen, bis hin zum Ansto&#223; einer breiten
gesellschaftlichen Debatte, die sich auch mit ethischen Fragen von KI im Milit&#228;r befassen soll (Villani 2018,
S. 125 ff.).
Niederlande
Die Regierung der Niederlande (Dutch Government 2016) hat ihre Position zum Themenkomplex AWS in einer
Stellungnahme zu dem in ihrem Auftrag angefertigten Bericht des Adviesraad Internationale Vraagstukken
(Beirat f&#252;r internationale Angelegenheiten) und des Commissie Van Advies Inzake Volkenrechtelijke
Vraagstukken (Beratender Ausschuss f&#252;r Fragen des V&#246;lkerrechts) (AIV/CAVV 2015)124 ausf&#252;hrlich dargelegt.
Sie strebt an, eine international abgestimmte Definition zu erarbeiten (insbesondere im Rahmen der CCW
GGE), und nimmt hierf&#252;r die Definition von AIV und CAVV als Ausgangspunkt: &#187;[...] eine autonome Waffe 
ist definiert als: eine Waffe, die ohne menschliches Zutun Ziele ausw&#228;hlt und bek&#228;mpft, die bestimmte vorher
123 Im Original: &#187;LAWS should be understood as implying a total absence of human supervision, meaning there is absolutely no link 
(communication or control) with the military chain of command.&#171; (CCW GGE 2016a)
124 AIV und CAVV sind unabh&#228;ngige Gremien, die die niederl&#228;ndische Regierung und das Parlament beraten.
definierte Kriterien erf&#252;llen. Dies erfolgt, nachdem ein Mensch die Entscheidung getroffen hat, die Waffe
einzusetzen, wohlwissend dass ein Angriff nicht mehr durch menschliche Intervention gestoppt werden kann,
nachdem er lanciert wurde.&#171;125 
Diese Definition wird pr&#228;zisiert, indem eine klare Trennlinie gezogen wird zwischen autonomen
Waffensystemen, bei denen Menschen eine entscheidende Rolle spielen (&#187;in the wider loop&#171;), und vollst&#228;ndig
autonomen Waffensystemen ohne jegliche menschliche Kontrolle. Der &#187;wider loop&#171; schlie&#223;t &#252;ber den &#187;narrow loop&#171;
(der die konkrete Zielauswahl und -bek&#228;mpfung umfasst) hinaus Aufgaben wie Missionsplanung, Vorauswahl
der Ziele, Wahl der Wirkmittel und Durchf&#252;hrungsplanung ein. Auch die Bewertungen der Vereinbarkeit eines
Angriffs mit dem HVR betrifft den &#187;wider loop&#171;. F&#252;r &#187;meaningful control&#171; reicht nach Ansicht der
niederl&#228;ndischen Regierung die menschliche Aufsicht &#252;ber den &#187;wider loop&#171; aus, der &#187;narrow loop&#171; k&#246;nnte ohne 
menschliche Intervention(sm&#246;glichkeit) ablaufen.
Dass vollst&#228;ndig autonome Waffensysteme in den n&#228;chsten Jahrzehnten entwickelt werden, wird als
unwahrscheinlich eingesch&#228;tzt. Dies erfordert, wenn es denn technisch &#252;berhaupt m&#246;glich ist, erhebliche
Fortschritte auf dem Gebiet der KI. Dar&#252;ber hinaus wird bezweifelt, ob Staaten derartige Waffen &#252;berhaupt
entwickeln wollten (AIV/CAVV 2015, S. 17).
Es wird bekr&#228;ftigt, dass im Zuge einer m&#246;glichen Beschaffung von AWS diese ausf&#252;hrlich unter
realistischen Bedingungen getestet w&#252;rden. Bez&#252;glich der Waffenpr&#252;fung gem&#228;&#223; Artikel 36 ZP I, ob bestimmte
autonome Waffen mit dem HVR kompatibel sind, haben die Niederlande das Advisory Committee on International
Law and the Use of Conventional Weapons (AIRCW) etabliert, das daf&#252;r zust&#228;ndig ist. Die Existenz einer
Verantwortungsl&#252;cke wird abgestritten: Solange Kommandeure und Operatoren &#187;meaningful human control&#171;
aus&#252;ben, tragen sie die volle Verantwortung. Eine entsprechende Ausbildung und das Training der Soldaten
sollen sicherstellen, dass ein verantwortungsvoller Umgang mit AWS stattfindet. Ein Moratorium f&#252;r die
Entwicklung und den Einsatz vollautonomer Waffen wird derzeit f&#252;r nicht sinnvoll bzw. nicht machbar gehalten
(Dutch Government 2016).
Dessen ungeachtet ist sich die niederl&#228;ndische Regierung &#252;ber das Potenzial von Autonomie im
Verteidigungssektor wohl bewusst: &#187;Wenn die niederl&#228;ndische Armee auf technologischem Feld fortschrittlich bleiben
will, werden autonome Waffen jetzt und in der Zukunft eine Rolle spielen. [...] Deren Einsatz muss aber immer
menschlicher Kontrolle (&#8250;meaningful human control&#8249;) unterliegen&#171; (Dutch Government 2016). Diese
Einsch&#228;tzung spiegelt auch die &#187;Strategische FuE-Agenda&#171; des niederl&#228;ndischen (Ministerie van Defensie 2016) wider,
die Big Data, k&#252;nstliche Intelligenz und &#187;man-machine teaming&#171; zum Schwerpunkt hat.
Vereinigtes K&#246;nigreich
Das britische Ministry of Defence (MOD 2017b, S. 13) arbeitet mit folgenden Definitionen autonomer bzw.
automatisierter Waffensysteme: &#187;Ein automatisiertes System [...] ist so programmiert, dass es als Antwort auf
eingehende Signale eines oder mehrerer Sensoren einem vorher definierten Satz von Regeln logisch folgt, um
daraus ein Ergebnis zu erzeugen. Wenn der Regelsatz, mit dem es operiert, bekannt ist, ist das Ergebnis
vorhersagbar.
Ein autonomes System ist f&#228;hig, Absichten und Anweisungen auf einer h&#246;heren Ebene zu verstehen.
Mittels dieses Verst&#228;ndnisses und seiner Wahrnehmung der Umwelt kann das System geeignete Handlungen
vollziehen, die einen gew&#252;nschten Zustand herbeif&#252;hren. Es ist dazu f&#228;hig, aus einer Anzahl von Alternativen eine
Vorgehensweise auszuw&#228;hlen, ohne auf menschliche Aufsicht und Kontrolle angewiesen zu sein, obwohl diese
dennoch vorhanden sein k&#246;nnen. Obwohl die allgemeine Aktivit&#228;t eines autonomen unbemannten Flugger&#228;ts
vorhersagbar sein wird, sind dies individuelle Handlungen m&#246;glicherweise nicht.&#171;126 
125 Im Original: &#187;An autonomous weapon is defined as: A weapon that, without human intervention, selects and engages targets
matching certain predefined criteria, following a human decision to deploy the weapon on the understanding that an attack, once launched,
cannot be stopped by human intervention.&#171; (AIV/CAVV 2015, S.11)
126 Im Original: &#187;An automated [...] system is one that, in response to inputs from one or more sensors, is programmed to logically
follow a predefined set of rules in order to provide an outcome. Knowing the set of rules under which it is operating means that its
output is predictable.
Als Schl&#252;sselbegriff f&#252;r die Abgrenzung autonomer von automatisierten Systemen wird hier die
Vorhersehbarkeit (&#187;predictable&#171;) verwendet. Die Aktionen automatisierter Systeme sind vollst&#228;ndig vorhersehbar, die
von autonomen Systemen auf der Ebene individueller Handlungen nicht.
Diese Definition f&#252;r AWS ist so eng gefasst (insbesondere aufgrund des Postulats von &#187;Verst&#228;ndnis von
Absichten und Anweisungen auf einer h&#246;heren Ebene&#171;), dass im Weiteren konstatiert wird, derartige Systeme
w&#252;rden derzeit nicht und m&#246;glicherweise niemals existieren (CCW 2016c, Ziffer 5). Zudem wird Systemen, die
ohne menschliche Kontrolle Waffeneins&#228;tze durchf&#252;hren k&#246;nnen, ein milit&#228;rischer Nutzen schlichtweg
komplett abgesprochen (CCW 2016c, Ziffer 3).127 Somit ist es nur konsequent, dass das Vereinigte K&#246;nigreich
erkl&#228;rte, nicht an derartigen Systemen zu forschen.
F&#252;r die Art der menschlichen Kontrolle wird der Terminus &#187;meaningful human control&#171; abgelehnt, da er
insbesondere beim Punkt Verantwortlichkeit nicht mit der derzeitigen Milit&#228;rdoktrin in Einklang zu bringen sei. 
Stattdessen wird z. B. der Begriff &#187;intelligent partnership&#171; vorgeschlagen, allerdings ohne eine pr&#228;zise
Definition anzubieten (CCW 2016c, Ziffer 6).
In einem Bericht des Ausschusses zu k&#252;nstlicher Intelligenz des britischen Oberhauses (House of Lords
Select Committee on Artificial Intelligence 2018, S. 101 ff.), in dem die wirtschaftlichen, ethischen und sozialen
Auswirkungen der Fortschritte im Bereich KI beleuchtet wurden, wurde diese Herangehensweise deutlich
kritisiert. Der Definitionsansatz (&#187;ist f&#228;hig, Absichten und Anweisungen auf einer h&#246;heren Ebene zu verstehen&#171;)
ist mit den Positionen der meisten anderen Regierungen weltweit nicht kompatibel. Dies w&#252;rde die
M&#246;glichkeiten zu konstruktiven Beitr&#228;gen in der internationalen Debatte einschr&#228;nken und Gro&#223;britannien eine Rolle
als ethisch und moralisch f&#252;hrende Nation auf diesem Gebiet erschweren. Daher empfiehlt der Ausschuss, eine 
Expertengruppe einzuberufen, die zeitnah eine international anschlussf&#228;hige Definition f&#252;r AWS vorlegen soll
(House of Lords Select Committee on Artificial Intelligence 2018, Ziffer 345 f.).
Hinter der bisherigen Definition l&#228;sst sich die Intention erkennen, dass bereits genutzte Systeme mit einem
hohen Grad operationeller Autonomie &#8211; insbesondere Abwehrsysteme gegen schnell anfliegende Flugk&#246;rper
(Raketen, Granaten) vom Typ &#187;SARMO&#171; (&#187;sense and react to military objects&#171; z. B. &#187;C-RAM&#171;, &#187;Phalanx&#171;,
&#187;NBS Mantis&#171;) oder Flugk&#246;rper mit intelligenter Zielerkennung und -identifizierung (etwa &#187;Brimstone&#171;) &#8211; auf 
keinen Fall darunter fallen sollen. Dies speist sich aus der Bef&#252;rchtung, dass im Rahmen der internationalen
Diskussion ggf. beschlossene Einschr&#228;nkungen f&#252;r AWS auch auf diese als milit&#228;risch unverzichtbar bewertete
Systeme ausgedehnt werden k&#246;nnen (Sharkey 2018).
An diesem Punkt l&#228;sst sich die Diversit&#228;t der nationalen Definitionsans&#228;tze gut demonstrieren. So
postuliert der franz&#246;sische Ansatz das vollst&#228;ndige Fehlen menschlicher Aufsicht als Kernkriterium f&#252;r AWS, was
dem britischen diametral gegen&#252;bersteht.
Hinsichtlich der Frage der Regulierung von LAWS im Rahmen der CCW vertritt die britische Regierung
die Position, dass die Sorgfalt, die verantwortliche Regierungen und Milit&#228;rs bereits heute bei der &#220;berpr&#252;fung
von Waffensystemen an den Tag legen, vollkommen ausreichend ist, um die Entwicklung s&#228;mtlicher neuer
Waffensysteme einschlie&#223;lich LAWS zu regulieren. Menschen m&#252;ssen stets die Oberaufsicht &#252;ber
Waffensysteme aus&#252;ben, und das Ziel der CCW sollte sein, ein Einverst&#228;ndnis dar&#252;ber zu erzielen, welche Elemente der
Kontrolle &#252;ber Waffensysteme Menschen vorbehalten bleiben sollen. Es wird betont, dass das Vereinigte
K&#246;nigreich vollst&#228;ndig autonome Waffensysteme weder besitzt noch entwickelt. Es wird versichert, dass jeder
Waffeneinsatz unter menschlicher Kontrolle steht und stehen wird, um absolut sicherzustellen, dass
menschliche Aufsicht, Befehlsgewalt und Verantwortlichkeit gewahrt sind (CCW GGE 2018i). Insgesamt ist die
britische Regierung der Auffassung, dass das HVR in seiner gegenw&#228;rtigen Form f&#252;r die Regulierung von LAWS
ausreichend ist. Ein Bedarf f&#252;r neue v&#246;lkerrechtliche Regeln oder Verbote wird nicht gesehen (Bowcott 2015).
Jenseits der skizzierten Debatte um Konzepte, Definitionen und Regulierung wird die herausragende Rolle,
die Autonomie in vielen Bereichen k&#252;nftig einnehmen k&#246;nnte, vom britischen Milit&#228;r keineswegs untersch&#228;tzt.
Dies kann man beispielsweise dem Strategiepapier &#187;Future Operating Environment 2035&#171; des MOD (2015)
entnehmen. AWS werden dort unter der &#220;berschrift &#187;remote and automated systems&#171; ausf&#252;hrlich behandelt.
An autonomous system is capable of understanding higher-level intent and direction. From this understanding and its perception of
its environment, such a system is able to take appropriate action to bring about a desired state. It is capable of deciding a course of
action, from a number of alternatives, without depending on human oversight and control, although these may still be present.
Although the overall activity of an autonomous unmanned aircraft will be predictable, individual actions may not be.&#171;
127 Im Original: &#187;The UK does not believe that there would be any military utility in a fully autonomous lethal weapon system.&#171;
Kasten 9.2 Qualit&#228;t menschlicher Kontrolle aus britischer Sicht
Die britische Regierung hat ihre Position mittlerweile weiterentwickelt und ausdifferenziert. Von der
fr&#252;heren Definition autonomer Systeme mit der mindestens missverst&#228;ndlichen Formulierung, diese w&#228;ren in der
Lage, &#187;Absichten und Anweisungen auf einer h&#246;heren Ebene zu verstehen&#171;, wurde merklich abger&#252;ckt (UK
Mission 2019a). Stattdessen wird die Qualit&#228;t menschlicher Kontrolle beim Waffeneinsatz in den Mittelpunkt 
gestellt. Der direkte Einbezug von Menschen bei jeglicher Aktion eines Systems ist weder praktikabel noch
unter allen Umst&#228;nden w&#252;nschenswert.
Zur Sicherstellung der menschlichen Kontrolle ist die Fokussierung auf sogenannte kritische
Funktionen (vor allem Zielauswahl und -bek&#228;mpfung) zu kurz gegriffen (UK Mission 2019b). Stattdessen muss der
gesamte Entwicklungs- und Nutzungszyklus einer Waffe in den Blick genommen werden, wie dies u. a. in
einem Arbeitspapier dargelegt wird (CCW GGE 2018i).
F&#252;r eine detaillierte Diskussion wird vorgeschlagen, diesen Zyklus in sechs Stadien zu unterteilen,
angefangen von der Konzeption politischer Grundlagen &#252;ber FuE, TEVV inklusive der Waffenpr&#252;fungen nach
Artikel 36 ZP I, Stationierung einschlie&#223;lich der entsprechenden Kommandokette (C2: &#187;command and
control&#171;), Entscheidungen hinsichtlich des Einsatzes bzw. Abbruchs des Einsatzes bis schlie&#223;lich zur Analyse
der Auswirkungen des Waffeneinsatzes. Abbildung 9.1 ordnet diesen Stadien wesentliche Aktivit&#228;ten zu.
Dies dient dazu, die unterschiedlichen Blickwinkel zu verdeutlichen, unter denen menschliche Kontrolle zur
Sicherstellung eines effektiven und gleichzeitig rechtlich und ethisch konformen Waffeneinsatzes betrachtet
werden sollte.
Die praktizierte menschliche Kontrolle auf all diesen Ebenen reicht nach Ansicht der britischen
Regierung aus, um die Entwicklung und den Einsatz jeglicher neuer Waffentechnologien, einschlie&#223;lich von AWS
zu regulieren. Dass die Aufsichtsfunktion, Autorit&#228;t, Verantwortlichkeit und Rechenschaftspflicht beim
Waffeneinsatz durch Menschen wahrgenommen werden, ist im Vereinigten K&#246;nigreich somit jederzeit
gew&#228;hrleistet (CCW GGE 2018i, S. 1)
Hinsichtlich des weiteren Vorgehens im Rahmen der CCW wird eine m&#246;gliche Zustimmung zu einer
regelm&#228;&#223;ig tagenden Expertengruppe signalisiert und es wird anerkannt, dass der deutsch-franz&#246;sische
Vorschlag oder eine Art &#187;code of conduct&#171; Ausgangspunkte f&#252;r die weitere Diskussion sein k&#246;nnen. Einer
dar&#252;ber hinausgehenden Erteilung eines Verhandlungsmandats, um neue Instrumente zur verbindlichen
Regulierung von LAWS zu implementieren, wird jedoch eine strikte Absage erteilt (UK Mission 2019c).
Abb. 9.1 Rahmen f&#252; r menschlich e Kontrolle im Entwicklu ngs- und Nutzungszyklu s
einer Waffe
n
at
io
n
al
e 
Po
lit
ik
 
fr&#252;
he
Fo
rs
ch
un
g 
un
d
En
tw
ick
lu
ng
 
de
ta
ill
ie
rt
es
 
Sy
st
em
de
si
gn
Pr
og
ra
m
m
- u
nd
 
Pr
oj
ek
tm
an
ag
em
en
t 
An
fo
rd
er
un
gs
-
pr
of
il 
0
. 
Po
lit
is
ch
e 
K
o
n
tr
o
lle
 
1.
Fu
E 
2. T
 &amp; E
 
V &amp; 
V 
&#220;be
rpr&#252;f
ungen
 
3. Einsatzund C2(commandand
control) 
4. Nutzung 
und
Abbruch 5
. A
n
alyse 
Test,
 Bew
ertun
g 
und 
Abna
hme 
Regu
lieru
ng u
nd 
Zerti
fizie
rung
 Stationierung/Einsatz amSchauplatz 
Training
Einsatzregeln
Operations-planung
Zielentscheidungen
Schlachtfeldm
anagem
ent 
Sch
ad
en
san
alyse 
g
ew
o
n
n
en
e 
Erken
n
tn
isse 
R
&#252;
ckm
eld
u
n
g
 
Quelle: nach CCW GGE 2018i, S. 5
Europ&#228;ische Union
In ihrem Statement auf der Sitzung der CCW GGE im April 2018 erkl&#228;rte die Delegation der EU, es m&#252;sse
Menschen vorbehalten sein, die Entscheidung &#252;ber die Aus&#252;bung t&#246;dlicher Gewalt zu treffen. Au&#223;erdem habe
das HVR f&#252;r alle Waffen einschlie&#223;lich LAWS in vollem Umfang zu gelten. Eine Arbeitsdefinition f&#252;r LAWS
begr&#252;&#223;te die EU-Delegation. Im &#220;brigen soll es in Anbetracht der Dual-Use-Eigenschaften der Technologien
vermieden werden, den Fortschritt in der zivilen FuE zu behindern (CCW GGE 2018b).
Im Vergleich zu diesen weich formulierten Zielen nimmt das EU-Parlament eine wesentlich deutlichere
Position ein. Bereits 2014 verabschiedete es eine Entschlie&#223;ung zum Einsatz bewaffneter Drohnen, die die Hohe
Vertreterin f&#252;r Au&#223;en- und Sicherheitspolitik, die Mitgliedstaaten und den Rat auffordert, &#187;die Entwicklung,
Produktion und Verwendung von vollkommen autonom funktionierenden Waffen, mit denen Milit&#228;rangriffe
ohne Mitwirkung des Menschen m&#246;glich sind, zu verbieten&#171; (EP 2014). Dies wurde k&#252;rzlich in einer weiteren
Entschlie&#223;ung bekr&#228;ftigt, in der explizit gefordert wurde, &#187;auf die Aufnahme internationaler Verhandlungen
&#252;ber ein rechtsverbindliches Instrument hinzuarbeiten, mit dem letale autonome Waffensysteme untersagt
werden&#171; (EP 2018).
Abseits von der milit&#228;rischen Anwendung der Robotik regte das EU-Parlament an, ein umfassendes EU-
Registrierungssystem f&#252;r fortschrittliche Roboter einzuf&#252;hren, wenn dies f&#252;r bestimmte Kategorien von
Robotern sachdienlich und notwendig ist. Eine neu einzurichtende Europ&#228;ische Agentur f&#252;r Robotik und K&#252;nstliche
Intelligenz kann hier die Federf&#252;hrung &#252;bernehmen (EP 2017, S. 9 f.). Eine analoge Initiative w&#228;re sicher auch
f&#252;r AWS denkbar.
Kasten 9.3 AWS und der Europ&#228;ische Verteidigungsfonds
Der 2017 ins Leben gerufene Europ&#228;ische Verteidigungsfonds (European Defence Fund) soll die
Wettbewerbs- und Innovationsf&#228;higkeit der technologischen und industriellen Basis der europ&#228;ischen Verteidigung
steigern. Er ist pro Jahr mit etwa 500 Mio. Euro f&#252;r Forschung sowie etwa 1 Mrd. Euro f&#252;r Entwicklung und 
Beschaffung (nach 2020) ausgestattet (EK 2017).
In der Frage, ob Mittel aus dem Fonds auch f&#252;r FuE von AWS bereitgestellt werden d&#252;rfen, zeichnet
sich aktuell eine Einigung ab. In einem zwischen den Mitgliedsl&#228;ndern abgestimmten Entwurf f&#252;r eine
entsprechende Regulierung, der allerdings noch vom EU-Parlament sowie vom Rat formal gebilligt werden
muss, ist dies explizit ausgeschlossen (General Secretariat of the Council 2019). Dort steht unter Punkt 7:
&#187;Ma&#223;nahmen zur Entwicklung von t&#246;dlichen autonomen Waffen ohne die M&#246;glichkeit der bedeutsamen
menschlichen Kontrolle &#252;ber Entscheidungen zur Auswahl und Bek&#228;mpfung bei Schl&#228;gen, die sich gegen 
Menschen richten, sollen durch den Fonds nicht f&#246;rderf&#228;hig sein, unbeschadet der M&#246;glichkeit, die
Entwicklung von Fr&#252;hwarnsystemen und Gegenma&#223;nahmen f&#252;r defensive Zwecke finanziell zu f&#246;rdern.&#171;128 
USA
Die USA st&#252;tzen sich auf ihre bereits 2012 ausgearbeitete Definition von AWS (DOD 2012, S. 13 f.): &#187;Ein 
Waffensystem, das nach seiner Aktivierung Ziele ausw&#228;hlen und bek&#228;mpfen kann ohne weitere Einwirkung
durch einen menschlichen Bediener. Dies schlie&#223;t von Menschen &#252;berwachte AWS ein, die es menschlichen
Bedienern erlauben, das System im Betrieb zu &#252;berstimmen&#171;.129 
Ausgehend von der von allen Staaten geteilten Pr&#228;misse, dass alle Waffen einschlie&#223;lich LAWS
vollst&#228;ndig konsistent mit dem HVR sein m&#252;ssen, betonen die USA, dass dabei den nationalen Waffenreviewprozessen
eine zentrale Bedeutung zukommen soll. Es wird festgestellt, dass Fortschritte in KI und maschinellem Lernen 
nicht nur Risiken und Herausforderungen mit sich bringen, sondern dass dies die Einhaltung des HVR auch 
128 Im Original: &#187;Actions for the development of lethal autonomous weapons without the possibility for meaningful human control
over the selection and engagement decisions when carrying out strikes against humans should also not be eligible for financial 
support by the Fund, without prejudice to the possibility to provide funding for actions for the development of early warning systems
and countermeasures for defensive purposes.&#171;
129 Im Original: &#187;A weapon system that, once activated, can select and engage targets without further intervention by a human operator.
This includes human-supervised autonomous weapon systems that are designed to allow human operators to override operation of
the weapon system.&#171;
erleichtern und verbessern k&#246;nnte. Daher ist es wichtig, besser zu verstehen, wie diese Technologie dazu genutzt 
werden kann, das Risiko f&#252;r Zivilisten und verb&#252;ndete Kr&#228;fte im bewaffneten Konflikt zu reduzieren.130 Daher 
w&#228;re es voreilig, in die Aushandlung eines politisch oder juristisch bindenden Dokuments einzusteigen (CCW 
GGE 2017d u 2018h).
Die USA sind der Ansicht, dass f&#252;r den Fortgang der Diskussionen im Rahmen der CCW keine allgemein
akzeptierte Definition von LAWS erforderlich ist. Dies w&#228;re nur dann notwendig, wenn Normen oder
Regularien formuliert werden sollen, was aber zum gegenw&#228;rtigen Zeitpunkt als verfr&#252;ht angesehen wird. Aktuell ist
es v&#246;llig ausreichend, Charakteristika von LAWS zu identifizieren, um die Diskussion voranzubringen. Diese
sollten allerdings nicht auf spezifischen technologischen Annahmen basieren, da diese durch die Fortschritte 
bei FuE schnell &#252;berholt sein k&#246;nnen (CCW GGE 2017c).
Der zentrale Punkt bei der Mensch-Maschine-Interaktion bei LAWS ist, dass Maschinen die Intentionen 
des Kommandeurs bzw. Operators umsetzen. Dies w&#228;re bei Einhaltung eines angemessenen Niveaus
menschlicher Beurteilung beim Waffeneinsatz (&#187;appropriate levels of human judgment over the use of force&#171;)
gew&#228;hrleistet. Der in der Debatte vorgebrachte Fokus auf menschliche Kontrolle (&#187;meaningful human control&#171;) w&#228;re
u. a. deshalb fehlgeleitet, weil autonome Funktionen eines Waffensystems die menschliche Kontrolle &#252;ber die 
Waffe effektiv erh&#246;hen k&#246;nnten, wie am Beispiel von smarten Bomben ausgef&#252;hrt wird. Au&#223;erdem w&#228;ren die 
Verantwortung und die Rechenschaftspflicht von Kommandeuren und Operatoren jederzeit gegeben,
unabh&#228;ngig vom Niveau der Autonomie der eingesetzten technischen Systeme. Wesentlich ist vor allem, wie Menschen
diese Systeme nutzen und was von den Waffen erwartet wird (CCW GGE 2018j).
Russland
Das russische Verteidigungsministerium (CCW GGE 2018d) verwendet folgende Definitionen f&#252;r autonome
und semiautonome Waffensysteme:131 
&#8250; autonomes Waffensystem: ein unbemanntes Ger&#228;t, das keine Munition ist und das milit&#228;rische und
unterst&#252;tzende Aufgaben ferngesteuert durch einen Bediener, autonom oder in Kombination der beiden Modi
durchf&#252;hren kann;
&#8250; semiautonomes Waffensystem: ein robotisches milit&#228;risches Ger&#228;t, das die Mitwirkung eines Bedieners
erfordert.
Russland stellt fest, dass die unterschiedlichen Definitionen f&#252;r LAWS, die die einzelnen Staaten verwenden,
die Diskussion ganz erheblich erschweren. Einige Staaten verstehen darunter auch bereits genutzte
semiautonome und automatisierte Systeme; andere nur solche, die mit einem hohen Grad an k&#252;nstlicher Intelligenz
ausgestattet sind und die derzeit noch nicht existieren, aber in Zukunft m&#246;glich sind. Demzufolge sollen zuerst die
grundlegenden Funktionen gekl&#228;rt und eine Arbeitsdefinition aufgestellt werden, die als Voraussetzung f&#252;r
Fortschritte in den Diskussionen gesehen wird. Dabei wird Wert darauf gelegt, dass die Definition keinesfalls
zu einer Einteilung von Waffensystemen in gute und b&#246;se f&#252;hren darf.
Des Weiteren ist man der Ansicht, dass existierende Systeme mit einem hohen Grad an Autonomie bzw.
Automatisierung nicht als LAWS klassifiziert werden sollen. Bei diesen ist sichergestellt, dass sie den Kriterien
des HVR gen&#252;gen. Aufgrund der Effizienz und Pr&#228;zision dieser Systeme reduzieren sie das Risiko f&#252;r zivile
Einrichtungen und Personen. Um dies jederzeit zu gew&#228;hrleisten, sollen die Staaten sich auf jeweils eigene
Standards beziehen. Versuche, universelle Parameter f&#252;r kritische Funktionen, d. h. Zielauswahl und -
bek&#228;mpfung, festzulegen, werden nicht als erfolgversprechend angesehen. Es ist z. B. zweifelhaft, ob Kriterien f&#252;r ein
Mindestma&#223; an Bedeutsamkeit menschlicher Kontrolle entwickelt werden k&#246;nnen (CCW GGE 2018d).
130 Dieser Punkt wird auch in der j&#252;ngst ver&#246;ffentlichten KI-Strategie der US-Regierung betont (DOD 2019, S.6): &#187;AI also has the 
potential to enhance our implementation of the Law of War. By improving the accuracy of military assessments and enhancing 
mission precision, AI can reduce the risk of civilian casualties and other collateral damage.&#171;
131 Im Original: &#187;Autonomous weapons system &#8211; an unmanned piece of technical equipment that is not a munition and is designed to
perform military and support tasks under remote control by an operator, autonomously or using the combination of these methods; 
Semi-autonomous weapon system &#8211; type of robotic military equipment requiring involvement of the operator.&#171;
China
China definiert LAWS anhand von f&#252;nf Charakteristika132 (CCW GGE 2018c):
1. T&#246;dlichkeit, d. h. eine ausreichende Tragkraft f&#252;r eine t&#246;dliche Ladung;
2. Autonomie, d. h. Abwesenheit von menschlicher Intervention und Kontrolle w&#228;hrend des gesamten
Prozesses der Ausf&#252;hrung einer Aufgabe;
3. Unm&#246;glichkeit des Abbruchs, d. h., es gibt kein Mittel, das Ger&#228;t zu stoppen, nachdem es aktiviert wurde;
4. unterschiedslose Wirkung, d. h., das Ger&#228;t f&#252;hrt die Aufgabe der T&#246;tung und Verst&#252;mmelung aus, ohne
R&#252;cksicht auf Bedingungen, Szenarien und Art der Ziele;
5. Evolution, d. h., das Ger&#228;t kann durch Interaktion mit der Umwelt lernen, seine Funktionen und
F&#228;higkeiten in einer Weise erweitern, dass sie menschliche Erwartungen &#252;bertrifft.
China &#228;u&#223;ert die Bef&#252;rchtung, dass LAWS die Schwelle f&#252;r kriegerische Auseinandersetzungen senken k&#246;nnen.
Gleichzeitig w&#252;rden die Kosten der Kriegsf&#252;hrung gesenkt (dabei bleibt in dem Statement offen, ob nur
finanzielle oder auch politische Kosten und/oder der Verlust von Soldaten gemeint ist). Es wird klar formuliert, dass
LAWS weder zur effektiven Diskriminierung zwischen Kombattanten und Zivilisten noch zu Einsch&#228;tzungen
hinsichtlich der Verh&#228;ltnism&#228;&#223;igkeit f&#228;hig sind. Auch ist schwierig zu etablieren, wer die Verantwortung bei
einem Einsatz von LAWS tr&#228;gt. Daher ruft China alle Staaten auf, Vorsicht walten zu lassen. Nationale
Waffenpr&#252;fungen (nach Artikel 36 ZP I) sind positiv zu bewerten, k&#246;nnen aber keinesfalls die grunds&#228;tzlichen
Bedenken ausr&#228;umen, die LAWS aufwerfen.
Aus diesem Grund bef&#252;rwortet China die Entwicklung eines bindenden Protokolls zu LAWS im Rahmen 
der CCW, &#228;hnlich dem &#252;ber blind machende Laserwaffen (Chinese Delegation o. J., S. 1).133 Die Bezugnahme
auf dieses Protokoll ist bemerkenswert, denn es wurde geschaffen, um ein pr&#228;ventives Verbot f&#252;r
Laserblendwaffen zu installieren. Andererseits hat China bis heute weder eine schl&#252;ssige Definition noch konkrete
Charakteristika f&#252;r LAWS vorgebracht, die unter ein solches Protokoll fallen w&#252;rden. Dar&#252;ber hinaus bezieht sich
der Vorschlag explizit nur auf den Einsatz von LAWS, aber nicht auf die Entwicklung, Produktion oder
Beschaffung. Insgesamt betrachtet ist die Position Chinas ambivalent und schwierig zu fassen und einzusch&#228;tzen
(siehe dazu auch die Analyse von Kania 2018).
Dieser Eindruck verst&#228;rkt sich, da gleichzeitig der positive Beitrag herausgestrichen wird, den KI auf die
wirtschaftliche und soziale Entwicklung in vielen L&#228;ndern leistet. Aus diesem Grund wird gefordert, dass
voreilige Handlungen in Bezug auf LAWS die zuk&#252;nftige Entwicklung und Nutzung von KI nicht hemmen d&#252;rfen
(CCW GGE 2018c). Aber auch im milit&#228;rischen Bereich treibt China die Nutzung von KI dynamisch voran. KI
wird als Schl&#252;sseltechnologie gesehen, um im globalen wirtschaftlichen und milit&#228;rischen Wettbewerb bestehen
zu k&#246;nnen. KI gilt als Instrument, um technologische Entwicklungsstufen &#252;berspringen zu k&#246;nnen und mit
Rivalen wie den USA technologisch gleichziehen oder sie sogar &#252;berholen zu k&#246;nnen (Allen 2019; Kania 2017).
Bewegung der Blockfreien Staaten
Die Bewegung der Blockfreien Staaten134 &#228;u&#223;ert Besorgnis, da LAWS viele ethische, moralische, rechtliche,
technische sowie den internationalen Frieden und die Stabilit&#228;t betreffende Fragen aufwerfen. Sie setzt sich
daf&#252;r ein, dass konkrete Schritte unternommen werden, einschlie&#223;lich Elementen eines rechtlich bindenden
Instrumentariums zur Regulierung bzw. zum Verbot von LAWS. Weichere Instrumente wie politische
Deklarationen, &#187;codes of conduct&#171; und andere freiwillige Ma&#223;nahmen sind nicht geeignet, rechtlich verbindliche
Instrumente zu ersetzen. Die Staatengemeinschaft wird aufgefordert, bis zur Umsetzung solcher Instrumente 
ein Moratorium f&#252;r die weitere Entwicklung und die Nutzung von LAWS zu erkl&#228;ren (CCW GGE 2018g).
132 Leider ist im Originaltext an vielen Stellen v&#246;llig unklar, was genau gemeint ist. &#220;ber die Gr&#252;nde daf&#252;r soll an dieser Stelle nicht
spekuliert werden.
133 Im Original &#187;China supports the development of a legally binding protocol on issues related to the use of LAWS, similar to the
Protocol on Blinding Laser Weapons, to fill the legal gap in this regard.&#171;
134 Internationale Organisation mit derzeit 120 Mitgliedstaaten, deren Ursprung ihre Neutralit&#228;t w&#228;hrend des Ost-West-Konflikts
zwischen NATO und Warschauer Pakt war; sie setzt sich f&#252;r eine Gleichberechtigung zwischen den Staaten ein.
Internationales Komitee vom Roten Kreuz
Nach Ansicht des IKRK sollte das Ziel der CCW GGE sein, sich auf Grenzen f&#252;r die Autonomie von
Waffensystemen zu einigen. Relevant sind dabei insbesondere die kritischen Funktionen der Zielauswahl und -
bek&#228;mpfung und nicht z. B. Navigation oder Flugsteuerung. Ein Minimum an menschlicher Kontrolle ist sowohl aus
ethischen als auch aus rechtlichen Erw&#228;gungen unabdingbar. Ein vollst&#228;ndig autonomes Waffensystem ohne
jegliche menschliche Kontrolle ist deshalb unzul&#228;ssig. Aber auch jenseits vollautonomer Waffensysteme
besteht ein Spektrum von Risiken, das von der internationalen Staatengemeinschaft nicht &#252;bersehen werden sollte.
Da technologische Entwicklungen, die die direkte menschliche Kontrolle &#252;ber Waffensysteme mindern, den
internationalen Diskurs &#252;ber diese Fragen abzuh&#228;ngen drohen, wird den Staaten angeraten, die Aufgabe,
Autonomie in Waffensystemen zu beschr&#228;nken, mit hoher Dringlichkeit anzugehen (IKRK 2018b).
Kasten 9.4 Aktuelle Position des IKRK
Das IKRK ist der Auffassung, dass ein Standard formuliert werden soll, der die Art und Qualit&#228;t menschlicher
Kontrolle definiert, die f&#252;r die Einhaltung des HVR und ethischer Prinzipien erforderlich ist. Auf dem Weg
zu einem solchen Standard, der der Autonomie von Waffensystemen Grenzen setzt, wird die Beantwortung
einiger Schl&#252;sselfragen als erforderlich angesehen (IKRK 2019):
&#8250; Muss die menschliche Aufsicht &#252;ber ein Waffensystem mit der M&#246;glichkeit, einzugreifen und es ggf.
zu deaktivieren, w&#228;hrend der gesamten Operation dauerhaft gew&#228;hrleistet sein? Welche Ausnahmen
sind zul&#228;ssig?
&#8250; Muss es verpflichtend sein, dass ein menschlicher Operator in der Lage ist, mit hoher Sicherheit
vorherzusagen, dass die Waffe ein spezifisches Ziel zu einem spezifischen Zeitpunkt angreifen wird und 
welche Auswirkungen dies haben wird? Welche Ausnahmen sind zul&#228;ssig?
&#8250; Welchen Standards hinsichtlich der Zuverl&#228;ssigkeit sollen Waffen, die in ihren kritischen Funktionen
&#252;ber Autonomie verf&#252;gen, gen&#252;gen? Wie soll dies &#252;berpr&#252;ft werden?
&#8250; Angenommen, dass LAWS zur Selektion und zum Angriff materieller Ziele (d. h. Dinge) akzeptabel 
w&#228;ren &#8211; welche operationellen Einschr&#228;nkungen w&#252;rden daraus folgen, insbesondere hinsichtlich ihres
Operationsraums (z. B. besiedelte oder unbesiedelte Gebiete) und der Dauer ihrer Operation? W&#228;re dies
f&#252;r mobile bzw. station&#228;re Systeme unterschiedlich?
9.2.3 Gemeinsamer Vorschlag von Deutschland und Frankreich auf der CCW GGE
Auf dem Treffen der GGE im November 2017 ergriffen Deutschland und Frankreich die Initiative und brachten
gemeinsam einen ersten konkreten Vorschlag ein, wie die internationale Gemeinschaft mit LAWS umgehen
kann (CCW GGE 2017a). Ausgehend von der Pr&#228;misse, dass LAWS gegenw&#228;rtig noch nicht existieren und die
Staaten sich noch kein umfassendes Bild &#252;ber die Charakteristika der zu regulierenden Systeme machen
konnten, wird eine umfassende Regulierung zum gegenw&#228;rtigen Zeitpunkt als verfr&#252;ht angesehen. Zudem wird der
Diskussionsstand der CCW so eingesch&#228;tzt, dass &#187;eine internationale &#196;chtung entsprechender Waffensysteme 
aufgrund der unterschiedlichen Positionierung der Vertragsstaaten der CCW aktuell nicht durchsetzbar
erscheint&#171; (Bundesregierung 2018f). Stattdessen wird eine politische Deklaration vorgeschlagen, flankiert von
transparenz- und vertrauensbildenden Ma&#223;nahmen.
Im Zentrum der politischen Deklaration (CCW GGE 2017a, S. 3) soll eine Bekr&#228;ftigung der Staaten stehen,
dass weiterhin Menschen die letztendliche Entscheidung &#252;ber den Einsatz t&#246;dlicher Gewalt treffen und
ausreichende Kontrolle &#252;ber t&#246;dliche Waffensysteme aus&#252;ben werden. Au&#223;erdem soll erkl&#228;rt werden, dass die
v&#246;lkerrechtlichen Regelungen, insbesondere das HVR, uneingeschr&#228;nkt auf die Entwicklung und Nutzung von
LAWS anwendbar sein sollen.
Auf freiwilliger Basis k&#246;nnen Ma&#223;nahmen getroffen werden, die die Transparenz und gegenseitiges
Vertrauen (CCW GGE 2017a, S. 3 f.) st&#228;rken. Konkret genannt werden erstens die Identifikation und Austausch
von Best-Practice-Beispielen f&#252;r Waffenpr&#252;fungen gem&#228;&#223; Artikel 36 ZP I (Kap. 7.1), zweitens die Erlaubnis
f&#252;r Beobachter, an Demonstrationen zuk&#252;nftiger LAWS teilzunehmen, sowie drittens der gegenseitige
Austausch von Informationen. Der Vorschlag sieht dar&#252;ber hinaus vor, dass auf Basis der politischen Deklaration
als n&#228;chster Schritt ein &#187;code of conduct&#171; (CCW GGE 2017a, S. 4) entwickelt wird, der politisch bindende
Regeln f&#252;r die Entwicklung und die Nutzung von LAWS ausformuliert.
Zus&#228;tzlich wird vorgeschlagen, im Rahmen der CCW ein Komitee technischer Experten (CCW GGE
2017a, S. 4) einzurichten, das den Auftrag hat, die Staaten regelm&#228;&#223;ig &#252;ber neue technologische Entwicklungen
mit Relevanz f&#252;r LAWS zu unterrichten, damit die Staaten Ma&#223;nahmen entwickeln und umsetzen k&#246;nnen, mit
denen die spezifischen Herausforderungen durch LAWS beantwortet werden k&#246;nnten.
Die Reaktionen auf den deutsch-franz&#246;sischen Vorschlag waren gemischt. Die meisten Stimmen
begr&#252;&#223;ten ihn als konstruktiven Schritt in die richtige Richtung. Die im Vorschlag vertretene Linie erhielt &#187;als
Mittelweg Unterst&#252;tzungszusagen zahlreicher Staaten&#171; (Bundesregierung 2018d, S. 58). Einige Delegationen und
insbesondere die Akteure aus dem NGO-Segment der CCW kritisierten den Vorschlag dagegen als nicht
ambitioniert genug. In der Tat dr&#252;ckt die von Deutschland und Frankreich angestrebte politische Deklaration im Gro&#223;en
und Ganzen lediglich den Minimalkonsens der Staaten aus, wie er in den Ergebnisprotokollen der CCW GGE
festgehalten wurde (CCW 2016a; CCW GGE 2017e u. 2018a), und die transparenz- und vertrauensbildenden
Ma&#223;nahmen sind deutlich weniger weitreichend, als es insbesondere das von den NGOs geforderte Moratorium
bzw. Komplettverbot ist.
Einige Delegationen wiesen auf die Schw&#228;che von auf Freiwilligkeit basierenden Ma&#223;nahmen hin und
sahen diese lediglich als Zwischenschritt auf dem Weg zu einem rechtlich bindenden Instrumentarium,
beispielsweise in Form eines neuen Protokolls der CCW135 (&#228;hnlich dem zum Verbot von Laserblendwaffen von 
1995). Andere unterstrichen die Notwendigkeit, das Wissen und gemeinsame Verst&#228;ndnis der Thematik zu
vertiefen, bevor eine bestimmte Handlungsoption gepr&#252;ft werden kann (CCW GGE 2018a, S. 12 f.). Diejenigen
L&#228;nder, die ein direktes Verbot bef&#252;rworten,136 hielten dagegen, dass ihrer Meinung nach LAWS dem
humanit&#228;ren V&#246;lkerrecht fundamental entgegenstehen, und daher bis zur Ausarbeitung eines Verbots ein sofortiges
Moratorium in Kraft gesetzt werden muss, weil ansonsten eine unkontrollierte Proliferation von LAWS zu
bef&#252;rchten ist (Rosen Jacobson 2017, S. 4 ff.).
Der NGO-Verbund Campaign to Stop Killer Robots ist der Ansicht, dass politische Deklarationen, &#187;codes
of conduct&#171; und andere Ma&#223;nahmen nicht geeignet sind, die vielf&#228;ltigen und ernsten ethischen, juristischen,
operationellen und technischen Herausforderungen zu meistern, die von LAWs gestellt werden. Daher wird
gefordert, dass die Staaten der CCW in formelle Verhandlungen einsteigen sollen, mit dem Ziel, ein
verbindliches Protokoll zu vereinbaren, das ein Verbot der Entwicklung, der Produktion und des Einsatzes von
vollst&#228;ndig autonomen Waffensystemen vorsieht (Campain to Stop Killer Robots 2017).
9.2.4 Ausgangslage f&#252;r die weitere Diskussion im Rahmen der CCW
Im Rahmen der informellen Expertentreffen (2014&#8210;2016) und der ersten Arbeitstreffen der GGE (2017 und 
2018) konnte eine allgemeine Zustimmung der Staatenvertreter zu einigen grundlegenden Punkten erzielt
werden, die die Basis f&#252;r die weitere Diskussion im Rahmen der CCW bilden (CCW 2016a; CCW GGE 2017e u.
2018a):
&#8250; Ein Staat tr&#228;gt die rechtliche und politische Verantwortung f&#252;r jeden Einsatz der Waffensysteme seiner
Streitkr&#228;fte und hat f&#252;r die daraus resultierende Taten eine Verantwortlichkeit unter Beachtung des
(humanit&#228;ren) V&#246;lkerrechts sicherzustellen (CCW 2016a, Punkt 2a).137 
135 Gesetz zum Protokoll II in der am 3. Mai 1996 ge&#228;nderten Fassung und zum Protokoll IV vom 13. Oktober 1995 zum UN-
Waffen&#252;bereinkommen vom 18. April 1997
136 Zum Stichtag 22. November 2018 waren dies 28 L&#228;nder: &#196;gypten, Algerien, Argentinien, Bolivien, Brasilien, Chile, China, Costa
Rica, Djibouti, Ecuador, El Salvador, Ghana, Guatemala, Heiliger Stuhl, Irak, Kolumbien, Kuba, Mexiko, Marokko, Nicaragua,
&#214;sterreich, Pakistan, Panama, Peru, Simbabwe, Staat Pal&#228;stina, Uganda, Venezuela (Campaign to Stop Killer Robots 2018). Hinzu
kommt Belgien, da das belgische Parlament am 13. Juli 2018 eine Resolution verabschiedete, die die Regierung aufforderte, dem
belgischen Milit&#228;r die Verwendung von LAWS zu untersagen (Walsh 2018).
137 Im Original: &#187;A state will bear the legal and political responsibility and establish accountability for action by any weapon system 
used by the state&#8217;s forces in accordance with applicable International Law, in particular International Humanitarian Law.&#171;
&#8250; Konzepte dar&#252;ber, was eine angemessene menschliche Einbindung in Bezug auf Entscheidungen &#252;ber die
Anwendung t&#246;dlicher Gewalt darstellt, sind essenziell f&#252;r die weitere Er&#246;rterung von LAWS (CCW 2016a,
Punkt 2b).138 
&#8250; Zivile Organisationen, die Industrie, Forscher und Wissenschaft sollen weiterhin eine wichtige Rolle in
den CCW-Verhandlungen spielen (CCW 2016a, Punkt 2c).139 
&#8250; Die Diskussion &#252;ber neue und f&#252;r das Feld der LAWS relevante Technologien soll weiterhin eine der
Priorit&#228;ten f&#252;r die CCW sein (CCW 2016a, Punkt 2d).140 
&#8250; Gleichzeitig wird darauf hingewiesen, dass in Anbetracht der Dual-Use-Eigenschaften von Technologien 
f&#252;r autonome Systeme keine der Aktivit&#228;ten der CCW GGE den Fortschritt oder den Zugang zu ziviler
Forschung, Entwicklung und Anwendung dieser Technologien behindern darf (CCW GGE 2017e).
Als wichtige Punkte, die es im weiteren Verlauf der CCW-Verhandlungen zu ber&#252;cksichtigen und zu kl&#228;ren
gilt, wurden festgehalten (CCW 2016a, Punkt 4; CCW GGE 2017e u. 2018a):
&#8250; Identifizierung von Charakteristika und Ausarbeitung einer Arbeitsdefinition von LAWS;
&#8250; Anwendung und Einhaltung der relevanten rechtlichen Grunds&#228;tze und Regeln des internationalen Rechts,
insbesondere des HVR, im Hinblick auf LAWS;
&#8250; Einhaltung von relevanten Menschenrechten;
&#8250; rechtliche und politische Verantwortung und Festlegung von Rechenschaftspflichten;
&#8250; ethische und moralische Fragen;
&#8250; Auswirkungen auf die regionale sowie internationale Sicherheit und Stabilit&#228;t;
&#8250; Einfl&#252;sse auf die Hemmschwelle zu bewaffneten Konflikten;
&#8250; Risiko eines Wettr&#252;stens;
&#8250; milit&#228;rischer Nutzen und Risiken;
&#8250; Risiko der Weiterverbreitung, inklusive unter Beteiligung nichtstaatlicher Akteure;
&#8250; Risiken von Cyberoperationen in Bezug auf LAWS.
Dieses umfassende Arbeitsprogramm ist ambitioniert, wenn man sich vor Augen f&#252;hrt, wie bed&#228;chtig die
bisherige Diskussion in den letzten Jahren vonstatten ging. Da f&#252;r Beschl&#252;sse im Rahmen der CCW
Einstimmigkeit erforderlich ist, ist absehbar, dass Debatten um konkrete Handlungsoptionen z&#228;h verlaufen und lediglich 
einen Minimalkonsens zum Ergebnis haben k&#246;nnten.
138 Im Original: &#187;Views on appropriate human involvement with regard to lethal force and the issue of delegation of its use are of 
critical importance to the further consideration of LAWS amongst the High Contracting Parties and should be the subject of further 
consideration.&#171;
139 Im Original: &#187;Civil society organizations, industry, researchers and scientific organizations should continue to play an important
role in exploring the prospective issue in accordance with the established procedural rules of the CCW.&#171;
140 Im Original: &#187;The discussion on emerging technologies in the area of LAWS is one of the priorities for the CCW and should be 
continued, while not prejudging discussions in other relevant fora.&#171;
Kasten 9.5 Die weitere Arbeit der CCW
In der Zusammenfassung des Stands der Diskussionen der CCW-GGE-Sitzungen 2018 sind die bis dahin
erzielten Gemeinsamkeiten unter der &#220;berschrift &#187;M&#246;gliche Leitlinien&#171; (CCW GGE 2018k, S. 4) aufgelistet.
Daran, wie allgemein gehalten einige der Punkte sind, l&#228;sst sich ablesen, wie weit der Weg zu einem
gemeinsamen Verst&#228;ndnis in strittigeren Fragen noch ist.
&#8250; Das HVR ist weiterhin auf alle Waffensysteme anwendbar einschlie&#223;lich der m&#246;glichen Entwicklung
und des Einsatzes von LAWS.
&#8250; Die menschliche Verantwortung f&#252;r Entscheidungen bez&#252;glich des Einsatzes von Waffensystemen
muss gewahrt bleiben, da die Rechenschaftspflicht nicht auf Maschinen &#252;bertragen werden kann. Dies
trifft f&#252;r den gesamten Lebenszyklus eines Waffensystems zu.
&#8250; Die Rechenschaftspflicht bez&#252;glich der Entwicklung, Stationierung und des Einsatzes jeglicher im
Rahmen der CCW zu betrachtender neuer Waffensysteme muss im Einklang mit geltendem V&#246;lkerrecht
gew&#228;hrleistet sein, einschlie&#223;lich durch den Betrieb dieser Waffensysteme innerhalb einer
verantwortlichen menschlichen Kommandokette.
&#8250; Im Einklang mit ihren v&#246;lkerrechtlichen Verpflichtungen sollten Staaten bei Forschung, Entwicklung,
Erwerb bzw. Einf&#252;hrung neuer Waffen oder Mittel und Methoden der Kriegsf&#252;hrung pr&#252;fen, ob deren 
Einsatz immer oder unter gewissen Umst&#228;nden v&#246;lkerrechtlich verboten sein k&#246;nnte.
&#8250; Wenn neue Waffensysteme entwickelt oder erworben werden, die auf aufkommende Technologien im
Bereich LAWS basieren, sollten deren physische Sicherheit, angemessene nichtphysische
Sicherheitsma&#223;nahmen (einschlie&#223;lich Cybersicherheit gegen &#187;hacking&#171; oder &#187;spoofing&#171;), das Risiko, dass
terroristische Gruppierungen sie beschaffen, sowie das Risiko der Proliferation beachtet werden.
&#8250; Die Bewertung von Risiken und Ma&#223;nahmen zu deren Begrenzung sollte Teil des Zyklus von Design,
Entwicklung, Test und Stationierung von aufkommenden Technologien in jeglichen Waffensystemen
sein.
&#8250; In Bezug auf die Nutzung aufkommender Technologien im Bereich LAWS sollte beachtet werden, dass
die Einhaltung des HVR sowie weiterer internationaler Verpflichtungen aufrechterhalten wird.
&#8250; Bei der Ausgestaltung m&#246;glicher politischer Ma&#223;nahmen sollten aufkommende Technologien im
Bereich LAWS nicht vermenschlicht dargestellt werden.
&#8250; Diskussionen und m&#246;gliche politische Ma&#223;nahmen im Rahmen der CCW sollten den Fortschritt bzw.
den Zugang zu friedlichen Nutzungen intelligenter autonomer Technologien nicht behindern.
&#8250; Die CCW, deren Zielsetzung es ist, einen Ausgleich zwischen milit&#228;rischen Notwendigkeiten und
humanit&#228;ren Erw&#228;gungen zu schaffen, bietet einen angemessenen Rahmen, um das Themenfeld der
aufkommenden Technologien im Bereich LAWS zu behandeln.
Das Arbeitsprogramm der CCW-GGE-Sitzung im M&#228;rz 2019 konzentrierte sich auf f&#252;nf Punkte (CCW GGE
2019):
&#8250; Exploration der m&#246;glichen Herausforderungen durch aufkommende Technologien im Bereich LAWS
in Bezug auf das HVR;
&#8250; Charakterisierung der zu ber&#252;cksichtigenden Systeme f&#252;r ein gemeinsames Verst&#228;ndnis der Konzepte 
und Eigenschaften, die in Bezug auf die Zielsetzungen der CCW relevant sind;
&#8250; weitere Betrachtung des menschlichen Elements bei der Anwendung t&#246;dlicher Waffengewalt; Aspekte
der Mensch-Maschine-Interaktion bei Entwicklung, Stationierung und beim Einsatz von
aufkommenden Technologien im Bereich LAWS;
&#8250; Pr&#252;fung m&#246;glicher milit&#228;rischer Anwendungen von im Kontext der Arbeit der GGE relevanten
Technologien;
&#8250; Er&#246;rterung von M&#246;glichkeiten des Umgangs mit humanit&#228;ren und sicherheitspolitischen
Herausforderungen, die durch aufkommende Technologien im Bereich LAWS in Bezug auf die Zielsetzungen der
CCW gestellt werden unter Ber&#252;cksichtigung vergangener, jetziger und zuk&#252;nftiger Vorschl&#228;ge, ohne 
dabei politische Ergebnisse vorwegzunehmen.
Die Resultate der Arbeitssitzung sollen wie in den vergangenen Jahren in einen Bericht m&#252;nden, der in der
zweiten Sitzung im August 2019 verabschiedet werden soll.
9.3 Regulierungsans&#228;tze der pr&#228;ventiven R&#252;stungs- und Exportkontrolle
Der zunehmende Einsatz unbemannter Waffensysteme ist zu einem Symbol f&#252;r die sich wandelnde
Kriegsf&#252;hrung des 21. Jahrhunderts geworden, und automatisierte oder zuk&#252;nftig autonome Waffensysteme k&#246;nnten
hierbei einen entscheidenden Paradigmenwechsel darstellen. AWS stellen in vielerlei Hinsicht eine
Herausforderung dar und werfen zahlreiche Fragen auf, sowohl was ihre &#220;bereinstimmung mit den Prinzipien des
humanit&#228;ren V&#246;lkerrechts angeht als auch die Auswirkungen, die ihre Verbreitung und ihr Einsatz entfalten k&#246;nnen,
gerade auch in Bezug auf potenzielle R&#252;stungsdynamiken, die internationale Sicherheit sowie regionale und
strategische Stabilit&#228;t.141 
Bei dem im Rahmen der CCW gef&#252;hrten Diskurs steht die Frage der Kompatibilit&#228;t von AWS mit dem
HVR im Mittelpunkt. Ihre potenziell destabilisierenden Wirkungen im Hinblick auf die internationale
Sicherheitspolitik und zwischenstaatliche Krisenstabilit&#228;t sind jedoch bislang unzureichend thematisiert worden.
Pr&#228;ventive R&#252;stungskontrolle dient der Identifikation und Ausarbeitung von r&#252;stungskontrollpolitischen
Regulierungsans&#228;tzen f&#252;r zuk&#252;nftige, bisher nicht stationierte Waffensysteme, mit dem konkreten Ziel, der
destabilisierenden Wirkung von potenziellen R&#252;stungswettl&#228;ufen und den Gefahren milit&#228;rischer
Eskalationsmechanismen bereits im Vorfeld zu begegnen (Neuneck/Mutz 2000; Petermann et al. 1997). Angesichts der
zunehmend beschleunigten technologischen Entwicklungen, z. B. in den Feldern Robotik und
Informationstechnologie, &#187;gibt es gegenw&#228;rtig mehr denn je einen Bedarf f&#252;r pr&#228;ventive R&#252;stungskontrolle&#171; (Dickow et al.
2015b, S. 67).
Aus Sicht des TAB ist pr&#228;ventive R&#252;stungskontrolle ein breitangelegter Prozess, der darauf abzielt, die
m&#246;glicherweise problematischen Konsequenzen technologischer Entwicklungen fr&#252;hzeitig zu erkennen, f&#252;r die
in politischer Verantwortung stehenden Entscheidungstr&#228;ger beurteilbar zu machen und durch Institutionen und
Verfahren auf nationaler und internationaler Ebene in ihren Risiken zu begrenzen (Petermann et al. 1997, S. 17).
Ob dies im konkreten Fall von AWS erfolgversprechend ist, wird von einigen vor dem Hintergrund
bisheriger Erfahrungen skeptisch gesehen. So merkt Gei&#223; (2015, S. 5) an, dass &#187;traditionell [...] das V&#246;lkerrecht bei
der Regulierung neuer Waffentechnologien regelm&#228;&#223;ig mindestens einen Krieg zu sp&#228;t&#171; kam. Und Nasu und
McLaughlin (2014, S. 53) weisen darauf hin, dass &#8211; obschon es historische Pr&#228;zedenzf&#228;lle daf&#252;r gibt, dass
Waffensysteme vor ihrer Stationierung verboten wurden &#8211; es unwahrscheinlich ist, dass dies geschieht, bevor ihre
F&#228;higkeiten vollst&#228;ndig untersucht und verstanden worden sind. Um dies einsch&#228;tzen zu k&#246;nnen, m&#252;ssten AWS 
erst einmal mindestens bis kurz vor ihrer Einsatzreife erforscht und entwickelt werden. Wenn dabei ein
erheblicher milit&#228;rischer Nutzen festgestellt wird, w&#228;re es schwierig, ein Verbot oder auch nur eine Beschr&#228;nkung 
umzusetzen, denn es g&#228;be einen inversen Zusammenhang zwischen dem strategisch milit&#228;rischen Nutzen einer
Waffe und der Bereitschaft eines Staates, Restriktionen zuzustimmen.
Scharre (2017, S. 30) f&#252;hrt weiter aus: Ein wesentlicher Faktor f&#252;r die Erfolgschancen eines Waffenverbots
ist die Abw&#228;gung zwischen der wahrgenommenen Abscheulichkeit (&#187;horribleness&#171;) der Waffe und deren mili-
141 Dieses Kapitel st&#252;tzt sich ma&#223;geblich auf Alwardt et al. (2017, 84 ff.).
t&#228;rischem Nutzen. Viele Waffen, die verboten wurden, besitzen nur geringen milit&#228;rischen Nutzen, aber
verursachen gro&#223;es Leid oder wirken in hohem Ma&#223;e destabilisierend (z. B. Blendlaser, auf einer Erdumlaufbahn
stationierte Massenvernichtungswaffen). Einschr&#228;nkungen f&#252;r Waffen zu erreichen, die zwar schrecklich sind,
aber gleichzeitig kriegsentscheidende Vorteile versprechen, ist erheblich schwieriger. Als instruktives Beispiel
hierf&#252;r kann die anhaltende Kontroverse um die &#196;chtung von Streumunition herangezogen werden. Das
Totalverbot dieser Waffen, zu dem die internationale Staatengemeinschaft im Jahr 2008 in der CCM (Convention on
Cluster Munitions/&#220;bereinkommen &#252;ber Streumunition) &#252;bereinkam, weist zwar derzeit 120 Signatarstaaten
auf (davon haben 17 noch nicht ratifiziert), allerdings befinden sich die Gro&#223;m&#228;chte China, Russland sowie die
Vereinigten Staaten142 nicht darunter (ICBL-CMC 2017).
Im Fall AWS kommt erschwerend hinzu, dass die Schl&#252;sseltechnologien f&#252;r Autonomie zu einem gro&#223;en
Teil im zivilen Umfeld entwickelt und somit potenziell breit verf&#252;gbar sein werden. Dies macht es
wahrscheinlich, dass unabh&#228;ngig vom Ausma&#223; internationaler &#196;chtung von AWS potente terroristische Gruppierungen
oder &#187;Schurkenstaaten&#171; (Scharre 2017, S. 31) diese bauen.
Zudem birgt die Verifikation von Abkommen &#252;ber AWS gro&#223;e Herausforderungen. Zwar sind formelle 
Verifikationsregime nicht zwingend notwendig f&#252;r die Einhaltung von Abkommen, ein hohes Ma&#223; an
gegenseitiger Transparenz dagegen schon. Wie diese Transparenz hergestellt werden kann, wenn der Unterschied
zwischen einer verbotenen und einer erlaubten Waffe lediglich in deren Software liegt, ist gegenw&#228;rtig v&#246;llig
unklar (Scharre 2017, S. 31 f).
&#220;berblick &#252;ber Regulierungsans&#228;tze
Es existiert ein weites Spektrum an M&#246;glichkeiten der Regulierung und Einhegung von Waffensystemen. Diese
k&#246;nnen zum einen an unterschiedlichen Tatbest&#228;nden ansetzen, &#252;blicherweise kommen hier Entwicklung, Test,
Besitz, Transfer/Handel, Stationierung und/oder Einsatz der Waffensysteme in Betracht. Eine zweite Dimension
ist die Stringenz der Regulierung, die von eher weichen Instrumenten &#8211; wie vertrauens- und
sicherheitsbildenden Ma&#223;nahmen, freiwillige Selbstverpflichtungen entweder unilateral erkl&#228;rt oder international abgestimmt &#8211;
bis hin zu harter Regulierung durch verbindliche Abkommen mit Verifikationsmechanismen reichen. Die Kunst
der R&#252;stungskontrollpolitik besteht darin, aus diesem Spektrum diejenigen Ma&#223;nahmen auszuw&#228;hlen, die auf
Basis der eigenen Wertvorstellungen und Intentionen den gr&#246;&#223;ten nationalen und kollektiven Nutzen im
Hinblick auf Sicherheit und Risikoabwehr erzeugen. Dabei kommt es nicht nur auf die W&#252;nschbarkeit der
Ma&#223;nahmen an, sondern auch entscheidend darauf, welche davon in Kooperation mit Partnern im internationalen
Rahmen umsetzbar erscheinen.
In Bezug auf die existierenden R&#252;stungskontrollvertr&#228;ge k&#246;nnen AWS, wie in Kapitel 9.1.1 ausgef&#252;hrt,
bisher allenfalls unter die sehr weit gefassten Definitionen der Hauptwaffensysteme fallen und w&#252;rden in
speziellen F&#228;llen den Beschr&#228;nkungen von INF und New Start unterliegen oder in gegenseitigem Einvernehmen
unter die Limitierungen des multilateralen KSE-Vertrags fallen, der jedoch von Russland aufgek&#252;ndigt wurde
und keine vielversprechende Zukunft mehr zu haben scheint. Dar&#252;ber hinaus werden AWS auch von einigen
internationalen Abkommen zu Exportkontrolle und Vertrauensbildung erfasst (Kap. 9.1.2 u. 9.1.3). Da weder
Drohnen noch allgemeine Robotik derzeit universellen r&#252;stungskontrollpolitischen Beschr&#228;nkungen unterliegen
(Dickow et al. 2015b, S. 71), gibt es bisher auch kein R&#252;stungskontrollabkommen, das explizit autonome
Waffensysteme benennt und in irgendeiner Form Regulierungen unterwirft.
Welches aber w&#228;ren &#252;berhaupt Instrumentarien der konventionellen R&#252;stungskontrolle oder VSBM, die in
Bezug auf AWS Anwendung finden k&#246;nnten? In der folgenden Tabelle 9.4 ist eine Auswahl m&#246;glicher Ans&#228;tze
zur Regulierung von AWS aufgelistet, jeweils charakterisiert nach Art des Abkommens und der Verbindlichkeit
der Regulierung. Die letzte Spalte enth&#228;lt eine Einsch&#228;tzung, ob Verifikationsma&#223;nahmen eine notwendige
Voraussetzung f&#252;r das Zustandekommen und den Bestand der jeweiligen Abkommen w&#228;ren. Oftmals stellen die
Erfahrungen, die im Zuge der Erprobung geeigneter Verifikationsmechanismen gemacht werden, die Weichen,
ob es am Ende bei einer VSBM bleibt oder aber ein rechtlich verbindliches R&#252;stungskontrollabkommen
geschaffen werden kann. Auf jeden Fall schafft eine verl&#228;ssliche Verifikation ein erhebliches Ma&#223; an Vertrauen
in ein Abkommen, das aber auch von beiden (oder allen) Vertragsparteien gewollt sein muss.
142 Eine Direktive der Administration von US-Pr&#228;sident G.W. Bush, die vorsah, bis Ende 2018 praktisch jegliche Streumunition aus
den aktiven Arsenalen zu entfernen und schlie&#223;lich zu vernichten, wurde am 30. November 2017 au&#223;er Kraft gesetzt (HRW 2017).
Tab. 9.4 M&#246;gliche Ans&#228;tze zur Regulierung von AWS
Art des Abkommens Verifikation
Transparenzma&#223;nahmen, z. B. freiwillige
Offenlegung der eigenen
Best&#228;nde an AWS
freiwilliger Verzicht auf den Besitz und/
oder Einsatz von AWS
(&#187;code of conduct&#171;)
Exportkontrolle oder -monitoring von AWS
oder spezifischer Teilkomponenten
(Schwierigkeit Dual-Use-Dilemma)
Beschr&#228;nkung der milit&#228;rischen Nutzung
von AWS &#252;ber die Definition
von nationalen oder internationalen
Einsatzregeln bzw. dem Ausschluss
bestimmter F&#228;higkeiten (&#187;rules of
engagement&#171;)
regionale oder internationale
Begrenzung der Arsenale
internationale &#196;chtung von AWS
(im UN-Rahmen)
Quelle: Alwardt et al. 2017, S. 86
VSBM (auch unilateral
m&#246;glich)
VSBM (auch unilateral
m&#246;glich)
Nichtverbreitung (auch
unilateral m&#246;glich)
VSBM (auch unilateral
m&#246;glich) oder internationaler
R&#252;stungskontrollvertrag
(verbindlich)
R&#252;stungskontrollvertrag
(verbindlich)
R&#252;stungskontrollvertrag
(verbindlich)
freiwillig
freiwillig
freiwillig
national:
freiwillig,
international:
ggf. notwendig
notwendig
notwendig
9.3.1 Vertrauens- und sicherheitsbildende Ma&#223;nahmen
Wie in Kapitel 9.1.2 erl&#228;utert, existiert besonders in Europa bereits eine Reihe von Ans&#228;tzen im Bereich der
VSBM, die bisher jedoch nicht dezidiert auf AWS zugeschnitten sind. Zuk&#252;nftige VSBM k&#246;nnten
Transparenzma&#223;nahmen wie die Offenlegung der eigenen Best&#228;nde an automatisierten oder autonomen
Waffensystemen und ihrer Stationierungsorte beinhalten sowie die Vorf&#252;hrung ihrer F&#228;higkeiten umfassen. In diesem
Zusammenhang ist das Wiener Dokument hervorzuheben, das zus&#228;tzlich auch Verifikationsmechanismen
beinhaltet, bisher aber auf den OSZE-Raum beschr&#228;nkt ist. Die Verabschiedung eines &#228;hnlich gearteten, aber weltweit
g&#252;ltigen und um bestimmte automatisierte sowie zuk&#252;nftige autonome Waffensysteme erweiterten Abkommens
im UN-Rahmen w&#228;re eine wirkungsvolle VSBM. Mittels VSBM k&#246;nnten auch Ma&#223;nahmen erprobt werden,
die zu einem sp&#228;teren Zeitpunkt und im Zusammenhang mit konkreten R&#252;stungskontrollma&#223;nahmen als
Elemente zuk&#252;nftiger Verifikationsma&#223;nahmen fungieren.
Weitere weiche Ma&#223;nahmen, die zwischen den Staaten freiwillig oder auch unilateral umsetzbar w&#228;ren,
sind der Austausch von Erfahrungen und Best-Practice-Beispielen beim Umgang und bei der Einsch&#228;tzung und
Regulierung von Autonomie in den verschiedenen Technologiefeldern. Dies k&#246;nnte auch z. B. die gemeinsame
Entwicklung von Standards, Methoden und Protokollen f&#252;r den Test und die Validierung autonomer
Waffensysteme umfassen (CCW GGE 2018a, S. 3).
Daneben steht es jedem Staat aber frei, selber Zeichen zu setzen und den zuk&#252;nftigen Einsatz potenter
AWS eigenen strengen Regeln zu unterwerfen (&#187;rules of engagement&#171;). Diese k&#246;nnten z. B. in der
Beschr&#228;nkung der maximalen Reichweite oder der Zuladung von AWS bestehen. M&#246;glich sind aber auch selbstauferlegte
Beschr&#228;nkungen beim Einsatzspektrum, z. B. verbindliche Festlegungen, bewaffnete Systeme ausschlie&#223;lich
f&#252;r den Eigenschutz im Rahmen der Luftnahunterst&#252;tzung einzusetzen (Sch&#246;rnig 2014).
Noch st&#228;rker w&#228;re das Signal, wenn ein Staat bzw. eine Staatengruppe von vornherein auf die Entwicklung 
und Stationierung von AWS verzichten w&#252;rde. W&#252;rde ein Staat diese freiwilligen Selbstbeschr&#228;nkungen noch
f&#252;r andere Staaten &#252;berpr&#252;fbar machen (Selbstauferlegung freiwilliger Verifikationsma&#223;nahmen), so w&#228;re dies
hochwirksam im Sinne der Sicherheits- und Vertrauensbildung und k&#246;nnte mit der Zeit Nachahmer unter
anderen Staaten finden.
Ein Zwischenschritt zu einem verbindlichen R&#252;stungskontrollabkommen w&#228;re es, aus dem hier
aufgef&#228;cherten Strau&#223; m&#246;glicher Ma&#223;nahmen ein geeignetes B&#252;ndel auszuw&#228;hlen und als internationale
Selbstverpflichtung (&#187;code of conduct&#171;) festzuschreiben, wie dies im deutsch-franz&#246;sischen Vorschlag auf der GGE
skizziert wurde (Kap. 9.2.3).
Aber auch jenseits der gro&#223;en B&#252;hne internationaler Politik spielen Selbstverpflichtungen, &#187;codes of
conduct&#171;, Ethikkodizes oder Richtlinien zur Sicherstellung guter fachlicher Praxis eine nicht zu untersch&#228;tzende
Rolle bei der Eind&#228;mmung von Risiken emergenter Technologien. Eines der ersten und vielleicht das
erfolgreichste Beispiel daf&#252;r sind die 1975 geschaffenen Richtlinien zum Umgang mit Risiken eines Zweigs der
seinerzeit gerade aufkommenden Gentechnik (genauer der rekombinanten DNA)143 der Asilomar-Konferenz 
(Spektrum 1999). Basierend auf diesem Vorbild wurde Anfang 2017 vom Future of Life Institute (FLI o. J.b) 
eine Konferenz zum Thema &#187;Beneficial AI&#171; organisiert, auf der 23 Prinzipien beschlossen wurden, die als
Material f&#252;r Diskussion und gleichzeitig als ehrgeizige Ziele dienen sollten, damit KI in Zukunft die
Lebensbedingungen von jedermann verbessern helfen kann.144 Unter Punkt 18 hei&#223;t es dort: &#187;An arms race in lethal
autonomous weapons should be avoided&#171;.
Ein anderer Ansatz, der in analoger Weise auf den Bereich KI/Robotik &#252;bertragbar w&#228;re, wurde beim
Thema Chemiewaffen in den Haager Ethikleitlinien&#171; der OPCW (o. J.) umgesetzt.145 Dort werden Normen und
Kerns&#228;tze formuliert, die universelle G&#252;ltigkeit haben sollen. Sie richten sich an alle &#187;Praktiker im Bereich
Chemie&#171; und rufen zu einer &#187;verantwortungsbewussten Nutzung der Chemie&#171; und zur &#187;Unterst&#252;tzung des
Chemiewaffen&#252;bereinkommens&#171; auf (OPCW 2015).
Auf dem Feld der KI/Robotik ist hier die &#187;Ethically Aligned Design&#171; der IEEE Global Initiative for Ethical
Considerations in Artificial Intelligence and Autonomous Systems hervorzuheben. Das Institute of Electrical
and Electronics Engineers (IEEE 2016) ist eine der gr&#246;&#223;ten professionellen Vereinigungen von
Wissenschaftlern und Ingenieuren weltweit. Die Initiative hat zum Ziel, einen globalen Diskussionsprozess anzusto&#223;en, der
in Empfehlungen f&#252;r eine ethische Governance von KI/autonomen Systemen/Robotik m&#252;nden soll. Das
Europ&#228;ische Parlament hat bereits konkrete Vorschl&#228;ge f&#252;r ethische Verhaltenskodizes im Bereich der Robotik f&#252;r
Forscher, Ingenieure sowie f&#252;r Aussch&#252;sse f&#252;r ethische Fragen in der Forschung in Form einer &#187;Charta f&#252;r
Robotik&#171; vorgelegt (EP 2017, S. 25 ff.). Bem&#252;hungen dieser Art um einen verantwortungsvollen Umgang mit
diesen Technologien verdienen jede Unterst&#252;tzung.
9.3.2 Exportkontrolle
Die Exportkontrolle ist ein probates Mittel zur Eind&#228;mmung der Proliferation von Waffentechnologien. Im
Bereich konventioneller Waffen dient dies meist der Wahrung nationaler sicherheitspolitischer Interessen.
Aufgrund ihres Eingriffs in internationale Handelsbeziehungen sind Exportkontrollen allerdings immer auch ein
zweischneidiges Schwert. Die immanente Diskriminierung von L&#228;ndern, denen der Zugriff auf bestimmte
Technologien verwehrt wird, kann &#246;konomischen Schaden verursachen, internationale Spannungen verst&#228;rken bzw.
Gr&#228;ben vertiefen, Vergeltungsma&#223;nahmen ausl&#246;sen sowie nicht zuletzt Anreize liefern, sich die entsprechenden
G&#252;ter auf anderen Wegen an den Exportkontrollen vorbei zu beschaffen.
Um die Weiterverbreitung von AWS einzuschr&#228;nken, w&#228;re ein speziell auf sie ausgerichtetes
internationales Exportkontrollregime denkbar, z. B. auf Grundlage des ATT (Kap. 9.1.3) (Lewis et al. 2016, S. 78 ff.). 
Tats&#228;chlich stehen bereits einige Technologien, Komponenten und Systeme mit Relevanz f&#252;r AWS auf den
einschl&#228;gigen Exportkontrolllisten des Wassenaar-Abkommens oder nationalen Listen f&#252;r
Exportbeschr&#228;nkungen (z. B. die Ausfuhrlisten der deutschen AWV oder die Kontrolllisten nach den &#187;International Traffic in Arms
Regulations&#171; und den &#187;Export Administration Regulations&#171; der USA; Stimson 2015, S. 11). Dazu z&#228;hlen z. B. 
143 Ein &#220;berblick zu deren Genese und Bedeutung findet sich z.B. beim Bayerischen Landesamt f&#252;r Gesundheit und
Lebensmittelsicherheit (Baiker 2014).
144 Die Liste der Unterzeichner umfasst aktuell 1.677 Forscher in den Feldern KI und Robotik sowie 3.662 weitere Personen, darunter 
Prominente wie der Physiker Stephen Hawking, Elon Musk (u. a. Tesla), Jaan Tallinn (Skype-Gr&#252;nder), Erik Brynjolfsson (MIT),
Ray Kurzweil (Google) und die Experten f&#252;r AWS Ronald C. Arkin (Georgia Tech), Peter Asaro, Noel Sharkey; Stand Oktober
2020.
145 Diese gehen auf eine Initiative Deutschlands auf der Konferenz der Vertragsstaaten im Dezember 2014 zur&#252;ck.
bewaffnete und bestimmte unbewaffnete UAVs, Flugkontrollsysteme, Managementsysteme mit
Schwarmf&#228;higkeiten oder bestimmte Navigationssysteme. Eine umfassende Exportkontrolle zur effektiven Eind&#228;mmung
der Verbreitung von AWS w&#252;rde jedoch auf einige schwer zu &#252;berwindende Hindernisse treffen. Zu nennen
sind hier vor allem die Dual-Use-Problematik, die gro&#223;e Bedeutung von Software und daraus folgend die
Schwierigkeiten bei der Verifikation von getroffenen Ma&#223;nahmen (AIV/CAVV 2015, S. 47).
Der ausgepr&#228;gte Dual-Use-Charakter der AWS zugrundeliegenden Technologien erschwert die Definition
und Abgrenzung derjenigen G&#252;ter, deren Proliferation beschr&#228;nkt werden soll, erheblich. Hinzu kommt, dass
derzeit FuE in f&#252;r AWS zentralen Technologiefeldern ma&#223;geblich im zivilen Sektor vorangetrieben werden.
Dies alles beg&#252;nstigt ihre weitere Verbreitung ungemein. So ist es beispielsweise in der Forschung zu KI Usus,
dass neben den Ver&#246;ffentlichungen in Fachzeitschriften auch die Quellcodes der verwendeten Software frei zur
Verf&#252;gung gestellt werden. Dass in erster Linie Algorithmen und Software bestimmend f&#252;r AWS sein werden
und nicht die Hardware, stellt die Eind&#228;mmung ihrer Verbreitung vor gro&#223;e Herausforderungen, denn Software
kann viel leichter auf Datentr&#228;gern an Grenzkontrollen vorbei oder direkt elektronisch ins Ausland verbracht
werden als materielle G&#252;ter.
Um M&#246;glichkeiten auszuloten, trotz der beschriebenen Schwierigkeiten bei der Eind&#228;mmung der
Proliferation Fortschritte zu erzielen, w&#228;re die Entwicklung und der Austausch zu Best-Practice-Beispielen beim
Umgang mit Exporten relevanter G&#252;ter, insbesondere Software, anzuraten. Hierf&#252;r w&#228;re die Einbindung der
relevanten Akteure in Wissenschaft und Wirtschaft sinnvoll (CCW GGE 2018c). Eine Reihe konkreter Vorschl&#228;ge
hierf&#252;r wird beispielsweise von Bromley/Maletta (2018, S. 34 ff.) entwickelt. Ein m&#246;glicher Schritt w&#228;re ein
auf die Nichtverbreitung von AWS ausgerichtetes globales Exportmonitoring. Organisiert werden k&#246;nnte dies
auf unterschiedlichste Weise, etwa formell innerhalb des UN-Waffenregisters oder auf informeller Basis
&#228;hnlich wie bei der Australia Group, die den Handel mit biologischen und chemischen Agenzien &#252;berwacht, um
das Risiko der Proliferation entsprechender Waffen zu minimieren.146 Dies k&#246;nnte ggf. R&#252;ckschl&#252;sse auf die
Intentionen einzelner Staaten in Hinblick auf die Entwicklung oder Beschaffung von AWS erlauben. Es k&#246;nnte
der Erfassung wesentlicher technologischer Komponenten von AWS dienen und damit zugleich auch eine
wirkungsvolle Grundlage f&#252;r internationale oder nationale Exportkontrollma&#223;nahmen darstellen.
Da nicht nur im zwischenstaatlichen Bereich, sondern auch in substaatlichen Konflikten bis hin zu
terroristischen oder kriminellen Hintergr&#252;nden AWS unterschiedlichster Provenienz (bis hin zu improvisierten
Ger&#228;tschaften) eine Gefahr darstellen k&#246;nnen, ist auch zu &#252;berlegen, wie hier Zugangsbarrieren errichtet werden
k&#246;nnten. Brundage (2018, S. 41) beschreibt hierf&#252;r eine Reihe von Ansatzpunkten, u. a. den Erlass von
Verkaufsbeschr&#228;nkungen von potenziell gef&#228;hrlichen Kleinstdrohnen (&#228;hnlich der Waffengesetzgebung), die
Registrierung und Kennzeichnung von Drohnen (z. B. mit einer Art Nummernschild), einen verpflichtenden
F&#252;hrerschein f&#252;r bestimmte Typen von Drohnen sowie die Ausweisung und Durchsetzung von &#187;no fly zones&#171;.
9.3.3 Verbindliche Regulierung bzw. Verbot von AWS
Die Forderung, AWS weltweit zu &#228;chten, wird insbesondere von NGOs und zurzeit 28 Staaten erhoben bzw.
unterst&#252;tzt.147 Deren zentrales Argument ist, dass Maschinen niemals &#252;ber Leben und Tod von Menschen
entscheiden sollen. Dies widerspricht dem HVR und verletzt die Menschenw&#252;rde. Durch die daraus resultierende
Entmenschlichung des Krieges sinkt die Hemmschwelle f&#252;r Staaten, Konflikte mit Gewaltmitteln l&#246;sen zu
wollen. Bis ein weltweites Verbot ausgehandelt und umgesetzt ist, wird angeregt, ein Moratorium f&#252;r das Testen,
die Herstellung, das Erwerben und den Einsatz der Systeme einzurichten (Heyns 2013).
Im Rahmen der CCW gibt es lediglich bei einer Gruppe kleinerer L&#228;nder (mit der m&#246;glichen Ausnahme
von China) Unterst&#252;tzung f&#252;r diese Position. Die allermeisten Staaten mit hochentwickelter R&#252;stungsindustrie 
und insbesondere die derzeit f&#252;hrenden Staaten auf dem Feld automatisierter UWS wie die USA und Israel
scheinen wenig Interesse an einem generellen Verbot zu haben (Dickow et al. 2015b, S. 71). Sie f&#252;hren ins
Feld, dass es bislang an einer allgemein akzeptierten Definition f&#252;r AWS fehle, die eine notwendige Grundlage
f&#252;r die Abgrenzung von erlaubten und verbotenen Systemen sei. Au&#223;erdem w&#252;rden AWS nicht per se dem
146 Die Australia Group ist ein informeller Zusammenschluss von 42 Staaten sowie der Europ&#228;ischen Kommission (http://australi-
agroup.net/en/index.html; 9.10.2020).
147 Stand November 2018 nach Z&#228;hlung der &#187;Campaign to Stop Killer Robots&#171;; hinzu kommt noch Belgien.
HVR widersprechen, sondern ggf. nur unter bestimmten Einsatzbedingungen. Ganz generell seien der
Entwicklungsstand der Technologie und das Wissen dar&#252;ber noch nicht weit genug fortgeschritten, um daraus
weitreichende Schlussfolgerungen &#252;ber Beschr&#228;nkungen und/oder Verbote ableiten zu k&#246;nnen.
F&#252;hrt man sich die geringen Fortschritte vor Augen, die in den letzten Jahren insbesondere im Rahmen der
CCW beim Thema AWS gemacht wurden, ist es offensichtlich, dass der Weg zu einer verbindlichen
internationalen Regulierung von AWS noch weit zu sein scheint. Verbindliche Regulierungen sind auch unterhalb der
Ebene eines Totalverbots denkbar. Diese k&#246;nnten einerseits dazu dienen, besonders problematische
Technologien bzw. Nutzungsarten von AWS einzuhegen. Andererseits k&#246;nnten sie wichtige Zwischenschritte darstellen,
um die Partizipation von Schl&#252;sselakteuren f&#252;r R&#252;stungskontrolle zu gewinnen und um die inhaltliche und
Vertrauensbasis f&#252;r eine umfassendere Regulierung zu legen.
Elemente einer verbindlichen Regulierung
Im Folgenden sollen einige Aspekte skizziert werden, die Bestandteile einer Regulierung werden k&#246;nnten,
damit ein m&#246;glicher zuk&#252;nftiger Einsatz von AWS mit den v&#246;lkerrechtlichen Vorgaben konform gehen k&#246;nnte. 
Hier wird der von Lewis (2015, S. 1322 ff.) entwickelten Struktur gefolgt:
&#8250; Technologische bzw. an den F&#228;higkeiten der AWS orientierte Charakteristika: Dies w&#228;ren
Mindestanforderungen an das Kamera- bzw. Bildverarbeitungssystem (Grundvoraussetzung f&#252;r Unterscheidung von
Kombattanten und Zivilisten), an das System f&#252;r kontextuelle Entscheidungen sowie der Umgang mit
(selbst)lernenden Systemen, um nur einige zu nennen;
&#8250; Charakteristika des Operationsraums: z. B. Dichte der Besiedlung, Abstandsregeln zu bestimmten
Einrichtungen (Schulen, Kirchen etc.), Differenzierungen zwischen den Medien (land-, luft-, see-,
unterwassergest&#252;tzt), Spezifizierung der zeitlichen Ausdehnung der Operation;
&#8250; Charakteristika der gegnerischen Kr&#228;fte: z. B. sind Soldaten regul&#228;rer Streitkr&#228;fte anhand der Uniformen
leichter zu erkennen als nichtuniformierte K&#228;mpfer;
&#8250; Mindestanforderungen an menschliche Aufsicht bzw. Kontrolle: z. B. die M&#246;glichkeit, Angriffe noch im
fortgeschrittenen Stadium abbrechen zu k&#246;nnen;
&#8250; andere Kriterien: z. B. Art der Wirkmittel (t&#246;dlich/nichtt&#246;dlich), offensive oder defensive Ausrichtung der
Systeme, mobile bzw. station&#228;re Systeme, Umgang mit Wetter- und anderen Umweltbedingungen;
&#8250; Regelungen zur Verantwortung und Haftbarkeit.
Des Weiteren k&#246;nnten auch Regulierungen mit regionalem Fokus, im Sinne der konventionellen
R&#252;stungskontrolle des KSE-Vertrags, wichtige stabilit&#228;tsf&#246;rdernde und r&#252;stungshemmende Ma&#223;nahmen zwischen Staaten
darstellen. Eine internationale &#220;bereinkunft zur Einschr&#228;nkung des Einsatzes von AWS, wie auch zum Teil im
Rahmen der CCW diskutiert, k&#246;nnte neben der Wahrung des HVR auch in zwischenstaatlichen
Konfliktsituationen zus&#228;tzliches Vertrauen schaffen und Eskalationsspiralen hemmen sowie den Tendenzen einer
zunehmenden Entgrenzung des Krieges entgegenwirken.
Das Problem der Verifikation
Die Charakteristika von AWS und insbesondere die geschilderte Dual-Use-Problematik schr&#228;nken die
M&#246;glichkeiten zur Verifikation der zahlenm&#228;&#223;igen AWS-Best&#228;nde eines Staates, die &#220;berpr&#252;fung des tats&#228;chlichen
Automatisierungs- oder Autonomiegrades eines unbemannten Waffensystems sowie die Kontrolle von AWS-
Eins&#228;tzen auf die Einhaltung v&#246;lkerrechtlicher Prinzipien stark ein. Die Etablierung verl&#228;sslicher
Verifikationsmechanismen, die f&#252;r den Erfolg zuk&#252;nftiger R&#252;stungskontrollvertr&#228;ge im Bereich der AWS wesentlich sind,
gestaltet sich sehr schwierig. Die Verifikation beispielsweise numerischer Obergrenzen von St&#252;ckzahlen w&#252;rde 
derzeit am Fehlen einer Definition von AWS sowie ggf. an Dual-Use-bedingten Schwierigkeiten bei der
Abgrenzung von zivilen und milit&#228;rischen Systemen scheitern. Die Frage, in welchem Ausma&#223; ein Waffensystem
tats&#228;chlich autonom handelt, hat wiederum weniger mit der Hardware und dem physischen Erscheinungsbild 
zu tun, sondern wird durch die spezifische Programmierung bzw. die Software bestimmt.
        
 
 
    
    
    
   
     
  
   
     
 
 
   
 
 
    
   
  
 
  
     
      
    
    
  
   
    
  
  
    
   
 
    
  
    
     
     
     
    
 
        
  
   
 
      
   
   
 
                                              
  
  
   
9.4
Auf welche Weise Softwarecodes direkt am System verifiziert werden k&#246;nnten, ist v&#246;llig unklar. Erstens
ist es schwer vorstellbar, dass staatliche Akteure es gestatten, dass die Steuersoftware von Waffensystemen
Dritten zur Inspektion offengelegt wird, und zweitens muss sichergestellt werden, dass diese nicht sofort nach
der Inspektion durch ein schnell und einfach durchf&#252;hrbares Update wieder ver&#228;ndert wird. &#196;hnlich schwierig
w&#228;re es, Eins&#228;tze von AWS auf die Einhaltung des HVR hin zu kontrollieren. Voraussetzung hierf&#252;r w&#228;re eine 
technische L&#246;sung, die ein nicht manipulierbares Monitoring der Eins&#228;tze und aller relevanten
Entscheidungsprozesse sicherstellt sowie die unabh&#228;ngige Analyse und Pr&#252;fung dieser Daten gew&#228;hrleistet. Ein erster
Vorschlag hierzu wurde von Mitgliedern des International Committee for Robot Arms Control (ICRAC)148 bereits
erarbeitet (Gubrud/Altmann 2013). Kurz umrissen besagt dieser, dass in einer Blackbox Einsatzdaten
verschl&#252;sselt und f&#228;lschungssicher aufgezeichnet werden (beispielsweise mittels einer Blockchain), die bei begr&#252;ndeten
Zweifeln der Rechtm&#228;&#223;igkeit eines Angriffs ex post analysiert werden k&#246;nnen, ohne dabei milit&#228;risch sensible
oder geheime Informationen preisgeben zu m&#252;ssen. Als Einzelma&#223;nahme betrachtet reicht diese
Verifikationsma&#223;nahme jedoch nicht f&#252;r den Bereich konventioneller R&#252;stungskontrolle aus, da hier im Vorfeld und &#252;ber
die gesamte Dauer eines R&#252;stungskontrollvertrags Vertrauen in Bezug auf die Entwicklung, Beschaffung oder
Vorhaltung von Waffensystemen herzustellen ist und nicht allein f&#252;r die Art sowie Umst&#228;nde des
Waffeneinsatzes. Sie k&#246;nnte jedoch ein konstruktives Element darstellen, eingebettet in ein allgemeines Regime von
Transparenz, vertrauensbildenden Ma&#223;nahmen, Inspektionen, technischen Sicherungsma&#223;nahmen und
forensischer Untersuchung verd&#228;chtiger Ereignisse.149 
Handlungsm&#246;glichkeiten
Die Bem&#252;hungen um eine Regulierung von AWS speisen sich aus zwei zentralen Argumentationsstr&#228;ngen. Der
eine befasst sich mit der Frage, ob der Einsatz von AWS mit den Prinzipien des humanit&#228;ren V&#246;lkerrechts
vereinbar w&#228;re. Die internationale Debatte dazu hat innerhalb der CCW eine ad&#228;quate Plattform gefunden. Der
andere dreht sich um die Auswirkungen der Entwicklung, Stationierung oder des Einsatzes von AWS f&#252;r die 
internationale Sicherheit. Die zentrale Frage hier ist, wie mit m&#246;glichen sicherheitspolitischen Implikationen,
z. B. R&#252;stungsdynamiken, zwischenstaatlichen Spannungen oder strategischen Instabilit&#228;ten, umgegangen
werden kann. Der Austausch zu dieser Frage befindet sich noch ganz am Anfang und hat bisher noch kein
geeignetes internationales Forum gefunden. Allerdings existiert im Bereich pr&#228;ventiver R&#252;stungskontrolle ein breites
Spektrum von Ansatzm&#246;glichkeiten, wie angesichts weltweit zunehmender geopolitischer Spannungen, schnell
voranschreitender technologischer Entwicklungen sowie verst&#228;rkter milit&#228;rischer R&#252;stung ein wichtiger Beitrag
zur Krisenstabilit&#228;t und internationalen Sicherheit geleistet werden kann.
Die Erfolgsaussichten einer Eind&#228;mmung der Verbreitung bzw. Nutzung von AWS in bewaffneten
Konflikten mit den Mitteln pr&#228;ventiver R&#252;stungskontrolle sind schwer einzusch&#228;tzen. Die Ausgangsbedingungen
sind nicht ideal, denn die Dynamik der zugrundeliegenden technologischen Trends ist beeindruckend
(Digitalisierung, Algorithmen, KI). Diese Dynamik umfasst alle Lebensbereiche und hat in vielen Bereichen
transformativen Charakter. Dies trifft auch f&#252;r den milit&#228;rischen Bereich zu. Der milit&#228;rische Nutzen eines verst&#228;rkten
Einsatzes von Autonomie ist klar erkennbar. Daher wird dies von allen technologisch fortgeschrittenen Staaten
angestrebt. Beispielsweise ist die st&#228;rkere Nutzung von Autonomie ein Kernelement der &#187;revolution of military
affairs&#171;, die derzeit in den USA in vollem Gange ist (DOD 2018b, 2019). Je gr&#246;&#223;er der erwartete strategische
Nutzen ausf&#228;llt, desto weniger werden Schl&#252;sselstaaten bereit sein, Restriktionen oder Verboten zuzustimmen
(Nasu/McLaughlin 2014, S. 52). Diese Haltung spiegelt sich derzeit auch erkennbar in den im Rahmen der
CCW von einigen Staaten vertretenen Positionen wider.
Im Folgenden werden m&#246;gliche Bausteine f&#252;r die Politikgestaltung skizziert. Diese leiten sich von der
angesprochenen Dichotomie der Argumentationsstr&#228;nge ab. Der erste bezieht sich auf die v&#246;lkerrechtliche und
ethische Dimension und die M&#246;glichkeiten innerhalb der CCW, w&#228;hrend der darauffolgende schwerpunktm&#228;&#223;ig
auf die sicherheitspolitische Dimension fokussiert. Die Bausteine sind keineswegs als sich gegenseitig
ausschlie&#223;ende Handlungsoptionen zu verstehen, sondern k&#246;nnen sich im Gegenteil gegenseitig erg&#228;nzen und
befruchten.
148 ICRAC ist ein Zusammenschluss von Wissenschaftlern und Aktivisten, die sich f&#252;r ein Verbot von AWS einsetzen
(https://www.icrac.net/about-icrac/; 9.10.2020).
149 Altmann (2019), pers&#246;nliche Mitteilung
9.4.1 Die M&#246;glichkeiten innerhalb der CCW aussch&#246;pfen
Zun&#228;chst einmal ist es ratsam, dass Deutschland sein bisheriges Engagement im Rahmen der CCW aktiv
fortf&#252;hrt, um eine &#196;chtung von solchen Waffensystemen zu erreichen, die dem Menschen die Kontrolle &#252;ber den
Waffeneinsatz entziehen. Da f&#252;r verbindliche Vereinbarungen Einstimmigkeit der CCW-Vertragsstaaten
erforderlich ist, m&#252;sste ein breiter Konsens aller relevanter Staaten erzielt werden, was bedeuten k&#246;nnte, dass
zun&#228;chst nur kleine gemeinsame Schritte als Minimalkonsens vereinbart werden k&#246;nnten (z. B. in Form von
VSBM). Aufgrund des bisherigen Dissenses in der CCW und grundlegend unterschiedlicher Auffassungen
einzelner Staaten sind die Erfolgsaussichten dieses Prozesses eher in einem l&#228;ngerfristigen Kontext zu betrachten,
zumal auf ein komplexes Geflecht von Fragen wie eine ad&#228;quate Verifikation, die fortschreitende Proliferation
oder die R&#252;stungsexportkontrolle Antworten gefunden werden m&#252;ssen.
Eine Fortf&#252;hrung des Diskurses um AWS im Rahmen der CCW ist auch losgel&#246;st von den Aussichten, ein
wie auch immer geartetes Ergebnis zu erzielen, als positiv zu werten. Denn dies tr&#228;gt wesentlich dazu bei, die 
Thematik auf der Agenda zu halten sowie die internationale (Fach-)&#214;ffentlichkeit zu sensibilisieren. Auf diese
Weise k&#246;nnen die Bedingungen geschaffen werden, um den Themenkomplex weiter zu durchdringen und sich 
im Laufe der Zeit inhaltlich anzun&#228;hern.
Dieser konsensorientierten Strategie entspricht die gemeinsame Initiative Deutschlands und Frankreichs,
die zun&#228;chst einmal eine politische Erkl&#228;rung und das Bekenntnis zum HVR-konformen Einsatz von AWS und 
den sich daraus ergebenden Verpflichtungen aus Artikel 36 ZP I anstrebt. Bei diesem auf eher langfristige
Erfolge angelegten Vorgehen besteht aber die Gefahr, dass mit verstreichender Zeit die Entwicklung, Verbreitung
oder vielleicht sogar der Einsatz von AWS stark vorangeschritten sein k&#246;nnte. Dies k&#246;nnte Tatsachen schaffen,
die eine Einigung im Rahmen der CCW erschweren w&#252;rden.
9.4.2 Engagement &#252;ber die CCW hinaus verbreitern
Insbesondere wenn die Fortschritte im Rahmen der CCW als zu langsam und/oder nicht hinreichend bewertet
werden, b&#246;te es sich an, dass Deutschland sich &#252;ber die CCW hinaus auf dem Feld der pr&#228;ventiven
R&#252;stungskontrolle international engagiert. Diese Strategie k&#246;nnte sich auf eine ad hoc zu bildende Koalition von Staaten,
NGOs und internationalen Organisationen st&#252;tzen, die ein gemeinsames Ziel verfolgen und m&#246;glichst z&#252;gig auf
eine Ausformulierung und Verabschiedung entsprechender Regeln zur Einhegung der Risiken von AWS
abzielen, in der Hoffnung, dass dies eine Sogwirkung entfaltet und sukzessive immer mehr Staaten sich dieser
Vorgehensweise anschlie&#223;en.
Das Vorgehen k&#246;nnte sich am Ottawa-Prozess orientieren, der im Ergebnis zum v&#246;lkerrechtlichen Verbot
von Antipersonenminen f&#252;hrte (f&#252;r einen kurzen &#220;berblick siehe z. B. Wikipedia 2004). Trotz des
unbestreitbaren humanit&#228;ren Erfolgs dieses Abkommens muss einschr&#228;nkend darauf hingewiesen werden, dass die
Gro&#223;m&#228;chte China, Russland und die USA150 ihm bis heute nicht beigetreten sind, obwohl der milit&#228;rische Nutzen
von Antipersonenminen (insbesondere f&#252;r milit&#228;risch starke Staaten) eher begrenzt ist. Ein Abkommen zu
AWS, dem die r&#252;stungstechnisch f&#252;hrenden Staaten nicht beitreten w&#252;rden, w&#228;re aber sicherlich kaum hilfreich.
Die Wahrscheinlichkeit f&#252;r eine breite Partizipation m&#246;glichst aller relevanten Staaten k&#246;nnte ggf. durch die
Wahl weniger ambitionierter Ma&#223;nahmen und Verbotstatbest&#228;nde erh&#246;ht werden.
Noch weitreichender w&#228;re es, unilateral voranzugehen und einen Verzicht auf den Einsatz, die
Beschaffung und/oder die Entwicklung von AWS verbindlich und nachpr&#252;fbar zu erkl&#228;ren. Dies w&#252;rde die von der
Bundesregierung bereits formulierte Position (CCW GGE 2018f) konsequent fortf&#252;hren und erheblich sch&#228;rfen.
Diese Strategie k&#246;nnte sich auf die in Bezug auf die Achtung der Menschenw&#252;rde vorgebrachten ethischen
Argumente st&#252;tzen (Kap. 8.2) und der herausgehobenen Rolle der Menschenw&#252;rde im Katalog der Grundrechte
Rechnung tragen. Die diffizile Aufgabe besteht darin, nachvollziehbar und glaubw&#252;rdig eine rote Linie zu
definieren, um erlaubte bzw. gew&#252;nschte Anwendungen der k&#252;nstlichen Intelligenz und Autonomie von nicht
statthaften zu unterscheiden. Die Glaubw&#252;rdigkeit solchen Handelns w&#252;rde gest&#228;rkt, wenn sie mit anderen
Politikbereichen in Einklang gebracht werden k&#246;nnte. Zu nennen sind hier insbesondere die Forschungsf&#246;rderung
sowie die Exportpolitik. Auch in diesem Zusammenhang ist ein Leitmotiv, auf die Vorbildfunktion und eine
eintretende Sogwirkung zu setzen, mit dem Ziel, dass weitere Staaten diesem Vorgehen folgen. Andererseits
150 Die seit 2014 verfolgte Politik, dass sich die USA au&#223;erhalb der koreanischen Halbinsel konform mit den zentralen Anforderungen 
der Ottawa-Konvention verhalten, wurde Ende Januar 2020 von der Trump-Administration widerrufen (Ali 2020; Scholz 2020).
besteht das Risiko, dass Schl&#252;sselakteure bzw. Gro&#223;m&#228;chte wie bei den Antipersonenminen sich der implizit
oder explizit ge&#228;u&#223;erten Einladung, hier mitzuziehen, verweigern und somit der gew&#252;nschte Effekt eines
unilateralen Handelns verpufft.
Internationalen Dialog st&#228;rken
Eine M&#246;glichkeit w&#228;re es, einen internationalen Dialogprozess zu initiieren, der die beschriebenen, bislang
vernachl&#228;ssigten Themen (u. a. R&#252;stungsdynamiken, zwischenstaatliche Spannungen bzw. strategische
Instabilit&#228;ten) aufgreift. Hier ist z. B. eine Rahmenkonvention denkbar, die eine inhaltliche und organisatorische Struktur
(etwa turnusm&#228;&#223;ige Treffen und ein kleines Sekretariat) vorgibt. Bei Bedarf und wenn die entsprechende
Zustimmung besteht, kann dieser Nukleus die Voraussetzungen schaffen, um konkretere und verbindlichere
&#220;bereink&#252;nfte zu treffen (Marchant et al. 2011, S. 313 f.). Als erfolgreiches Beispiel f&#252;r diesen Ansatz kann die
Genese des Wiener &#220;bereinkommens zum Schutz der Ozonschicht dienen, das als relativ weiche Vereinbarung
begann und im Laufe der Zeit vor allem durch das Montreal-Protokoll und dessen verschiedene Zus&#228;tze
Verbindlichkeit gewann.
Eine weitere M&#246;glichkeit bieten bilaterale und multilaterale regierungsseitige Dialogforen, die f&#252;r die
Entwicklung eines gemeinsamen Problemverst&#228;ndnisses und die Vertiefung von Kooperationen ein probates Mittel 
sind. Im Gegensatz zu Initiativen von privater Seite (z. B. von Verb&#228;nden) sind sie demokratisch legitimiert,
gleichzeitig aber weniger aufwendig zu organisieren als formale transnationale Verhandlungen. Dennoch
k&#246;nnen sie einen Rahmen vorgeben, dem harmonisierte nationale Vorgehensweisen oder sogar internationale
Regulierungen entspringen k&#246;nnen (Marchant et al. 2011, S. 311). Ein Beispiel f&#252;r ein solches Dialogforum ist die
Australia Group, die sich die Eind&#228;mmung der Proliferation chemischer und biologischer Waffen zum Ziel
gesetzt hat. Auch auf Ebene der OECD (o. J.) gibt es Beispiele, u. a. die Working Party on Biotechnology,
Nanotechnology and Converging Technologies (BNCT), die sich f&#252;r eine verantwortbare Entwicklung dieser
Technologien einsetzt.
Revitalisierung konventioneller R&#252;stungskontrolle
Ein weiteres wesentliches Handlungsfeld ist eine Revitalisierung der konventionellen R&#252;stungskontrolle. Diese
w&#228;re angesichts der heute vorherrschenden globalen sicherheitspolitischen Lage auch losgel&#246;st vom Thema
AWS als Beitrag zur Entspannung willkommen, allerdings sind die Bedingungen hierf&#252;r aktuell alles andere 
als einfach. Seri&#246;se Bem&#252;hungen in diesem Bereich k&#246;nnten z. B. die M&#246;glichkeit zur Schaffung eines
umfassenden konventionellen R&#252;stungskontrollregimes er&#246;ffnen, das auch als ein Ersatz f&#252;r den erodierenden KSE-
Vertrag dienen k&#246;nnte, vielleicht sogar um zus&#228;tzliche Staaten erweitert. In diesem Rahmen er&#246;ffnet sich die
Gelegenheit, unbemannte Waffensysteme zu inkludieren und etwaige AWS bzw. deren Einsatz weitgehend zu
regulieren.
Die Ausarbeitung und Umsetzung von R&#252;stungskontrollvereinbarungen in Anerkennung der
destabilisierenden Wirkung von AWS ist ein langer und steiniger Weg. Vertragliche numerische Obergrenzen (f&#252;r
bestimmte geografische R&#228;ume oder absolut) bzw. F&#228;higkeitsbeschr&#228;nkungen k&#246;nnten wesentliche
Stabilit&#228;tsgaranten auch im Hinblick auf eine zuk&#252;nftig zunehmend automatisierte Kriegsf&#252;hrung sein. Von enormer
Bedeutung f&#252;r den Erfolg von Beschr&#228;nkungen ist die Konzeption verl&#228;sslicher Verifikationsmechanismen.
Schritte in diese Richtung sind freiwillige Transparenz und VSBMs, die Vertrauen st&#228;rken und
Vorbildfunktion haben k&#246;nnten. Eine konkrete M&#246;glichkeit w&#228;re die Einrichtung einer internationalen
Beobachtungsstelle, die die Entwicklungen und die Verbreitung von AWS &#252;berwacht, analog derer f&#252;r Kernwaffen sowie
biologischer und chemischer Waffen (Villani 2018, S. 127). Die Ausarbeitung von Exportregeln k&#246;nnte
zus&#228;tzlich helfen, die Weiterverbreitung von AWS einzud&#228;mmen. Dabei muss jedoch daf&#252;r Sorge getragen werden,
dass diese Bestimmungen den zivilen Nutzen der infrage stehenden Technologien nicht &#252;ber Geb&#252;hr schm&#228;lern.
In Anerkennung der destabilisierenden Wirkung von AWS k&#246;nnten Staaten aber auch freiwillig auf diese
Art von Waffensystemen verzichten oder deren Automatisierungsgrad von vornherein begrenzen. Diese
Handlungsoption wurde zuvor bereits aus ethischen bzw. humanit&#228;ren Gr&#252;nden abgeleitet, hier allerdings stehen
prim&#228;r sicherheits- und stabilit&#228;tspolitische &#220;berlegungen im Vordergrund. Wie eine solche Begrenzung
aussehen k&#246;nnte, ist eine offene Frage, denn auch weit unterhalb der paradigmatischen Schwelle von &#187;human out
of the loop&#171; k&#246;nnten autonome bzw. hochautomatisierte Waffensysteme destabilisierend wirken.
An all diesen Schritten k&#246;nnte Deutschland sich aktiv beteiligen und auf diese Weise einen nicht zu
untersch&#228;tzenden Beitrag leisten, m&#246;gliche R&#252;stungsdynamiken und die daraus resultierenden Auswirkungen auf die
regionale wie globale Stabilit&#228;t im Vorfeld einzuhegen.
Begleitende nationale Schritte und Ma&#223;nahmen
Im Folgenden werden m&#246;gliche national umzusetzende Schritte umrissen, mit denen das Thema AWS verst&#228;rkt 
auf die Agenda der relevanten Entscheidungstr&#228;ger in Bundestag, Ministerien, Beh&#246;rden und der F&#252;hrung der
Bundeswehr gehoben w&#252;rde, die Basis an orientierungs- und entscheidungsrelevantem Wissen verbreitert
w&#252;rde sowie Ma&#223;nahmen angeschoben werden k&#246;nnten, die den internationalen Entscheidungsfindungsprozess
begleiten und befruchten k&#246;nnten:
&#8250; Intensivierung der Anstrengungen um eine ausf&#252;hrliche, breitangelegte (fach)&#246;ffentliche Debatte &#252;ber die 
milit&#228;rischen, v&#246;lkerrechtlichen und sicherheitspolitischen Implikationen des Einsatzes von UWS und
zuk&#252;nftigen AWS: Hierzu kann die Durchf&#252;hrung von &#246;ffentlichen Diskussionsveranstaltungen, Studien und 
Workshops geh&#246;ren, bei der auch die verschiedenen Stakeholder eingebunden werden m&#252;ssen: neben der
Regierung und den NGOs auch die Medien, Wirtschaft und Wissenschaft &#8211; insbesondere im
Sammelbereich von KI.
&#8250; Im Zusammenhang mit der Formulierung der nationalen Strategie f&#252;r KI (Bundesregierung 2018c) und 
den damit einhergehenden Kommissionen und Diskussionsforen sollten die Anwendungen der KI im
Milit&#228;rsektor sowie die Dual-Use-Aspekte nicht ausgeblendet werden. Auch f&#252;r die Enquete-Kommission
&#187;K&#252;nstliche Intelligenz &#8211; gesellschaftliche Verantwortung und wirtschaftliche, soziale und &#246;kologische
Potenziale&#171; (Deutscher Bundestag 2018b) w&#228;re dies ein hochrelevantes Untersuchungsfeld. Hier k&#246;nnte 
ein Blick nach Frankreich Anregungen liefern. Dort wurde ein ma&#223;geblicher Impuls durch den Bericht von
(Villani 2018, S. 125 f.) gesetzt.
Verst&#228;rkte Bem&#252;hungen zur Untersuchung einer international zunehmend automatisierten Kriegsf&#252;hrung im
Allgemeinen sowie des Einsatzes automatisierter UWS und zuk&#252;nftiger AWS im Besonderen: Ein spezieller
Fokus sollte dabei auf der Analyse sicherheitspolitischer Risiken f&#252;r die Bundesrepublik Deutschland und den 
M&#246;glichkeiten liegen, diesen bestm&#246;glich zu begegnen. Diese Aktivit&#228;ten k&#246;nnten im Rahmen einer
Kommission erfolgen, an der relevante Ministerien (Ausw&#228;rtiges Amt, Bundesministerium des Innern, f&#252;r Bau und
Heimat, Bundesministerium der Verteidigung etc.), Beh&#246;rden, Forschungsinstitutionen sowie NGOs und
Parlamentarier beteiligt sind.
&#8250; Die aktuellen Erkenntnisse und strategischen &#220;berlegungen mit Blick auf die milit&#228;rische Nutzung von
autonomen Waffensystemen in der Bundeswehr k&#246;nnten in einem nationalen Leitliniendokument
niedergelegt werden (Amoroso et al. 2018, S. 13), das als Referenzpunkt f&#252;r die verschiedenen Debattenstr&#228;nge 
dienen k&#246;nnte.
&#8250; Initiierung einer Debatte zu Fragen des Umgangs bei Exporten von sensiblen Technologien in Kooperation
mit Stakeholdern aus Wissenschaft, Wirtschaft und Gesellschaft: Darauf aufbauend k&#246;nnten Regeln zur
Nichtverbreitung und Exportkontrolle von UWS sowie AWS entwickelt werden. Hierbei muss
insbesondere die Dual-Use-Problematik ber&#252;cksichtigt und zugleich der Bedeutung und Legitimit&#228;t autonomer
Systeme im Bereich ziviler Anwendungen Rechnung getragen werden. Hierf&#252;r k&#246;nnte z. B. eine
entsprechende Arbeitsgruppe unter der &#196;gide des Bundesamtes f&#252;r Wirtschaft und Ausfuhrkontrolle (BAFA)
unter Beteiligung von Vertretern aus Wirtschaft, Wissenschaft und NGOs eingesetzt werden.
&#8250; In der Forschungspolitik und Forschungsf&#246;rderung w&#228;re bei KI und angrenzenden Feldern eine besondere
Sensibilisierung f&#252;r die Dual-Use-Problematik angebracht. Das umfasst auch die Unterst&#252;tzung der
Bem&#252;hungen um die Formulierung und Etablierung ethischer Leitbilder in der Forschung und
forschungsnahen Anwendung.
9.5 Fazit
AWS stellen in vielerlei Hinsicht eine Herausforderung f&#252;r die R&#252;stungskontrolle dar und werfen zahlreiche
Fragen auf, sowohl was ihre &#220;bereinstimmung mit den Prinzipien des humanit&#228;ren V&#246;lkerrechts angeht als
auch die Auswirkungen, die ihre Verbreitung und ihr Einsatz entfalten k&#246;nnten, gerade auch in Bezug auf
potenzielle R&#252;stungsdynamiken, die internationale Sicherheit sowie die regionale und strategische Stabilit&#228;t. Nicht
zuletzt aufgrund der gro&#223;en Dynamik der technologischen Entwicklung in den Bereichen Robotik und
k&#252;nstlicher Intelligenz ist es von herausragender Bedeutung, AWS-bezogene Mechanismen der pr&#228;ventiven
R&#252;stungskontrolle zu etablieren.
Derzeit existiert ein Fenster von M&#246;glichkeiten, um mit einem international abgestimmten, zielgerichteten
Vorgehen die m&#246;glichen Gefahren einzuhegen, die AWS mit sich bringen k&#246;nnten. Dieses Fenster schlie&#223;t sich
sukzessive mit fortschreitender technologischer Entwicklung und der kontinuierlichen Integration autonomer
Funktionen in Waffensysteme aller Art. Damit werden Strukturen gefestigt und Fakten geschaffen, die
regulierende Eingriffe erschweren oder sogar verhindern. Dieses Fenster von M&#246;glichkeiten zu nutzen ist keine
einfache Aufgabe, denn die Schwierigkeiten, die sich bei der R&#252;stungskontrolle und mit Verifikationsma&#223;nahmen
im Hinblick auf AWS stellen, sind gro&#223;. Angesichts der sicherheitspolitischen Implikationen, mit denen die
internationale Gemeinschaft durch autonome Waffensysteme zuk&#252;nftig konfrontiert werden k&#246;nnten, erscheint
es dringend geboten, diese Herausforderungen unverz&#252;glich anzugehen und L&#246;sungen zu entwickeln.
Diesbez&#252;gliche politische und diplomatische Initiativen erfordern einen langen Atem und einen breiten Diskurs unter
Einbezug von Wissenschaft und Zivilgesellschaft.
10 Literatur
10.1 In Auftrag gegebene Gutachten
Altmann, J.; Gubrud, M. (2017): Technologien f&#252;r autonome Waffensysteme &#8211; Stand und Perspektiven. K&#246;ln
Alwardt, C.; Hilgert, L.-M.; Neuneck, G.; Polle, J. (2017): Sicherheitspolitische Implikationen und
M&#246;glichkeiten der R&#252;stungskontrolle autonomer Waffensysteme. Gutachten im Auftrag des Deutschen
Bundestages. Institut f&#252;r Friedensforschung und Sicherheitspolitik, Hamburg
Koch, B.; Rinke, B. (2017): Ethische Fragestellungen im Kontext autonomer Waffensysteme. Institut f&#252;r
Theologie und Frieden, Hamburg
10.2 Weitere Literatur
AA (Ausw&#228;rtiges Amt) (2019a): Statement by Germany &#8211; On Agenda Item 5&#169; Review of the potential
military applications of related technologies in the context of the Group&#8217;s work. Group of Governmental
Experts on Emerging Technologies in the Area of Lethal Autonomous Weapons Systems of the Convention
on Prohibitions or Restrictions on the Use of Certain Conventional Weapons which may be deemed to be 
excessively injurious or to have indiscriminate Effects. Genf,
https://www.unog.ch/80256EDD006B8954/(httpAssets)/95996F2E27AC3DE7C12583D2003D858F/
$file/20190325+Statement1+Germany+GGE+LAWS.pdf (5.8.2020)
AA (2019b): Statement by Germany &#8211; On Agenda Item 5(d) Characterization of the systems under
consideration in order to promote a common understanding on concepts and characteristics relevant to the
objective and purpose of the Convention. Group of Governmental Experts on Emerging Technologies in the
Area of Lethal Autonomous Weapons Systems of the Convention on Prohibitions or Restrictions on the
Use of Certain Conventional Weapons which may be deemed to be excessively injurious or to have
indiscriminate Effects. Genf, https://www.
unog.ch/80256EDD006B8954/(http
Assets)/D6C11A0A68D81F4AC12583CB0036650B/$file/20190325+Statement2+Germany+GGE+
LAWS.pdf (5.8.2020)
AA (2019c): Statement by Germany &#8211; On Agenda Item 5(b) Further consideration of the human element in 
the use of lethal force; aspects of human machine interaction in the development, deployment and use of
emerging technologies in the area of lethal autonomous weapons systems. Group of Governmental
Experts on Emerging Technologies in the Area of Lethal Autonomous Weapons Systems of the Convention
on Prohibitions or Restrictions on the Use of Certain Conventional Weapons which may be deemed to be 
excessively injurious or to have indiscriminate Effects. Genf,
https://www.unog.ch/80256EDD006B8954/(http
Assets)/2B8E772610C0F552C12583CB003A4192/$file/20190326+Statement3+Germany+GGE+
LAWS.pdf (5.8.2020)
AA (2019d): Statement by Germany &#8211; On Agenda Item 5(e) Possible options for addressing the humanitarian 
and international security challenges posed by emerging technologies in the area of lethal autonomous
weapons systems in the context of the objectives and purposes of the Convention without prejudicing 
policy outcomes and taking into account past, present and future proposals. Group of Governmental
Experts on Emerging Technologies in the Area of Lethal Autonomous Weapons Systems of the Convention 
on Prohibitions or Restrictions on the Use of Certain Conventional Weapons which may be deemed to be 
excessively injurious or to have indiscriminate Effects. Genf,
https://www.unog.ch/80256EDD006B8954/(httpAssets)/E69C96284D07CAE8C12583CB003D9873
/$file/20190327+Statement4+Germany+GGE+LAWS.pdf (5.8.2020)
ACA (Arms Control Association) (2017): The Missile Technology Control Regime at a Glance.
https://www.armscontrol.org/factsheets/mtcr (9.10.2020)
Ahronheim, A. (2017): RAMBOW: Israel unveils latest unmanned ground vehicle. The Jerusalem Post,
18.9.2017
Airforce Technology (2017): Kratos to launch XQ-222 Valkyrie, UTAP-22 Mako at Paris Air Show 2017.
http://www.airforce-technology.com/news/newskratos-to-
launch-xq-222-valkyrie-utap-22-mako-at-paris-air-show-2017-5845491 (9.10.2020)
Airforce Technology (o. J.): X-45 J-UCAV (Joint Unmanned Combat Air System). https://www.airforce-tech-
nology.com/projects/x-45-ucav/ (9.10.2020)
AIV (Advisory Council on International Affairs); CAVV (Advisory Committee on Issues of Public
International Law) (2015): Autonomous Weapon Systems. The Need for Meaningful Human Control. AIV 
Nr. 97, https://www.advisorycouncil
internationalaffairs.nl/binaries/advisorycouncilinternationalaffairs/documents/publications/2015/10/02/
autonomous-weapon-systems/Autonomous_Weapon_Systems_AIV-CAVV-Advisory-report-
97_201510.pdf (5.8.2020)
Ali, I: (2020): Trump eases restrictions on land mine use by U.S. military. Reuters, 31.1.2020,
https://www.reuters.com/article/us-usa-war-landmines-idUSKBN1ZU2GA (9.10.2020)
Allen, G.C. (2019): Understanding China&#8217;s AI Strategy: Clues to Chinese Strategic Thinking on Artificial
Intelligence and National Security. Washington D.C., https://s3.amazonaws.com/files.cnas.org/docu-
ments/CNAS-Understanding-Chi
nas-AI-Strategy-Gregory-C.-Allen-FINAL-2.15.19.pdf?mtime=20190215104041 (5.8.2020)
ALT (Army Acquisition, Logistics and Technology) (201: Weapon Systems 2011. Arlington,
https://fas.org/man/dod-101/sys/land/wsh2011/wsh2011.pdf (9.10.2020)
Altmann, J. (2017): Zur ethischen Beurteilung automatisierter und autonomer Waffensysteme. In: Werkner,
I.-J.; Ebeling, K. (Hg.): Handbuch Friedensethik, S. 793&#8211;804
Altmann, J.; Sauer, F. (2017): Autonomous Weapon Systems and Strategic Stability. In: Survival 59(5),
S. 117&#8211;142
Alwardt, C.; Kr&#252;ger, M. (2016): Autonomy of Weapon Systems. Food for Thought Paper. Hamburg,
https://ifsh.de/file-IFAR/pdf_english/IFAR_FFT_1_final.pdf (5.8.2020)
AMF (Aspen (Ministers Forum) (2020): Statement from the Aspen Ministers Forum. https://www.aspeninsti-
tute.org/of-interest/statement-from-the-aspen-
ministersforum/ (9.10.2020)
Amoroso, D.; Sauer, F.; Sharkey, N.; Suchman, L.; Tamburrini, G. (2018): Autonomy in Weapon Systems.
The Military Application of Artificial Intelligence as a Litmus Test for Germany&#8217;s New Foreign and
Security Policy. Publication Series on Promoting Democracy 49, Berlin
Anderson, M.; Anderson, S. L. (2007): Machine ethics: Creating an ethical intelligent agent. In: AI Magazine 
28(4), S. 15&#8211;26
Apan (o. J.a): Optoelectronics and Photonics. https://community.apan.org/wg/afosr/w/researchareas/7686/op-
toelectronics-and-photonics/ (9.10.2020)
Apan (o. J.b): Computational Cognition and Machine Intelligence. https://community.apan.org/wg/afosr/w/re-
searchareas/7679/computational-cognition-and-machine-intelligence/ (9.10.2020)
Arkin, R.C. (2008): Governing lethal behavior: Embedding ethics in a hybrid deliberative/reactive robot
architec-ture part I: Motivation and philosophy. In: Association for Computing Machinery, Inc., 3rd 
ACM/IEEE International Conference on Human-Robot Interaction (HRI), Amsterdam, 2008, S. 121&#8211;128
Arkin, R.C. (2009): Governing lethal behavior in autonomous robots. Boca Raton
Arkin, R.C. (2010): The Case for Ethical Autonomy in Unmanned Systems. In: Journal of Military Ethics
9(4), S. 332&#8211;341
Arkin, R.C. (2013): Lethal Autonomous Systems and the Plight of the Non-combatant. In: AISB Quarterly
(137), S. 4&#8211;12
Arkin, R.C. (2015): Counterpoint. The case for banning killer robots. In: Commun. ACM 58(12), S. 46&#8211;47
Army Recognition (2016): Nerekhta robotic system to be added to Russian special forces&#8217; inventory.
21.12.2016, https://www.armyrecognition.com/december_
2016_global_defense_security_news_industry/nerekhta_robotic_system_to_be_added_to_russian_spe-
cial_forces_inventory.html (9.10.2020)
Army Technology (o. J.a): Squad Mission Support System (SMSS). https://www.army-technology.com/pro-
jects/squad-mission-support-system-smss/ (9.10.2020)
Army Technology (o. J.b): iRobot 510 PackBot Multi-Mission Robot. https://www.army-technology.com/pro-
jects/irobot-510-packbot-multi-mission-robot/ (9.10.2020)
Army Technology (o. J.c): AvantGuard Unmanned Ground Combat Vehicle. https://www.army-technol-
ogy.com/projects/avantguardunmannedgr/ (9.10.2020)
Army Technology (o. J.d): Uran-9 Unmanned Ground Combat Vehicle. https://www.army-technol-
ogy.com/projects/uran-9-unmanned-ground-combat-vehicle/ (9.10.2020)
Arquilla, J.; Ronfeldt, D. (2000): Swarming &amp; The Future of Conflict. RAND&#8217;s National Defense Research
Institute, Santa Monica, https://www.rand.org/content/dam/rand/pubs/documented_briefings/2005/
RAND_DB311.pdf (5.8.2020)
Article 36 (2013): Killer robots: UK Government policy on fully autonomous weapons. http://www.arti-
cle36.org/wp-content/uploads/2013/04/Policy_Paper1.pdf (9.10.2020)
Asaro, P. (2008): How Just Could a Robot War Be? In: Briggle, A.; Waelbers, K.; Brey, P. (Hg.): Current
issues in computing and philosophy. Amsterdam, S. 50&#8211;64
Asaro, P. (2012): On banning autonomous weapon systems. Human rights, automation, and the
dehumanization of lethal decision-making. In: Int. rev. Red Cross 94(886), S. 687&#8211;709
Atherton, K.D. (2018): Navy office awards $30 million contract for drone swarms. C4ISRnet, 27.6.2018,
https://www.c4isrnet.com/unmanned/2018/06/27/office-of-naval-research-awards-raytheon-30-million-
to-develop-locust-swarm/ (9.10.2020)
Austin, H. (2015): North Dakota becomes first US state to legalise use of armed drones by police. The
Independent, 9.9.2015, http://www.independent.co.uk/news/world/americas/north-dakota-becomes-first-us-
state-to-legalise-use-of-armed-drones-by-police-10492397.html (5.8.2020)
AUVAC (Autonomous Undersea Vehicle Applications Center) (o. J.): SeaOtter MKII. AUV System Spec
Sheet. http://auvac.org/2-2/ (9.10.2020)
Baiker, A. (2014): Geschichte der Gentechnik und Gentechnik-Gesetzgebung. Bayerisches Landesamt f&#252;r
Gesundheit und Lebensmittelsicherheit, https://www.
lgl.bayern.de/rubrikenuebergreifende_themen/gentechnik/gentechnik_geschichte.htm (9.10.2020)
Ballesteros, C. (2018): Russia Has Underwater Nuclear Drones, Leaked Pentagon Documents Reveal.
Newsweek, 14.1.2018
Banko, M.; Brill, E. (2001): Scaling to very very large corpora for natural language disambiguation. In:
Webber, B. (Hg.): Proceedings of the 39th Annual Meeting on Association for Computational Linguistics &#8211;
ACL &#8217;01. the 39th Annual Meeting. Toulouse, 6.&#8211;11.7.2001. In: Morristown: Association for
Computational Linguistics. Stroudsburg, S. 26&#8211;33
Barbaschow, A. (2018): University boycott ends after KAIST confirms no &#187;killer robot&#171; development. 
ZDNet, 10.4.18 https://www.zdnet.com/article/university-boycott-ends-after-kaist-confirms-no-killer-
robot-development/ (7.8.2020)
BBC News (2019): INF nuclear treaty: US pulls out of Cold War-era pact with Russia. 21.2.2020,
https://www.bbc.com/news/world-us-canada-49198565 (5.8.2020)
Beavers, A.F. (2011): Moral Machines and the Threat of Ethical Nihilism. In: Lin, P.; Abney, K.; Bekey, G.
(Hg.): Robot ethics: the ethical and social implications of robotics. In: The MIT press, London, S. 333&#8211; 
344
Bendel, O. (2018): &#220;berlegungen zur Disziplin der Maschinenethik. In: Aus Politik und Zeitgeschichte (6-8),
S. 34&#8211;38
Bendett, S. (2017): Can Russian UAVs Close the Gap with America or Israel? The National Interest,
7.7.2017, https://nationalinterest.org/blog/the-buzz/can-russian-
uavs-close-the-gap-americans-or-israelis-21473 (5.8.2020)
Berger, C.; Rumpe, B. (2008): Autonomes Fahren &#8211; Erkenntnisse aus der DARPA Urban Challenge
(Autonomous Driving &#8211; Insights from the DARPA Urban Challenge). In: it &#8211; Information Technology 50(4),
S. 258&#8211;264
Betzler, M.; Scherrer, N. (2016): Verantwortung und Kontrolle. In: Heidbrink, L.; Langbehn, C.; Sombetzki,
J. (Hg.): Handbuch Verantwortung. Wiesbaden, S. 337&#8211;352
Birnbacher, D. (2001): Instrumentalisierung und Menschenw&#252;rde. Philosophische Anmerkungen zur Debatte
um Embryonen- und Stammzellforschung. In: Kaiser, G. (Hg.): Jahrbuch der Heinrich-Heine-Universit&#228;t
D&#252;sseldorf 2001. D&#252;sseldorf, S. 243&#8211;257
Birnbacher, D. (2016): Autonomous weapon systems and human dignity. In: Bhuta, N.; Beck, S.; Gei&#223;, R.;
Liu, H.-Y.; Kre&#223;, C. (Hg.): Autonomous weapons systems. Law, ethics, policy. Cambridge, S. 105&#8211;121
Bitzinger, R.A.; Leah, C.M. (2016): Nuclear-Armed Drones? They May be Closer Than You Think. The
National Interest, 13.10.2016, https://nationalinterest.org/blog/
the-buzz/nuclear-armed-drones-they-may-be-closer-you-think-18034 (5.8.2020)
Blain, L. (2010): South Korea&#8217;s autonomous robot gun turrets: deadly from kilometers away. New Atlas,
7.12.2010, https://newatlas.com/korea-dodamm-super-aegis-autonomos-robot-gun-turret/17198/
(5.8.2020)
BMVg (Bundesministerium der Verteidigung) (1992): Humanit&#228;res V&#246;lkerrecht in bewaffneten Konflikten.
Handbuch. http://www.humanitaeres-voelkerrecht.de/HbZDv15.2.pdf (5.8.2020)
BMVg (2004): Grundz&#252;ge der Konzeption der Bundeswehr, Berlin
BMVg (2016): Pr&#252;fung neuer Waffen, Mittel und Methoden der Kriegsf&#252;hrung. Zentrale Dienstvorschrift 
Nr. A-2146/1
BMVg (2018a): Europ&#228;ische R&#252;stung st&#228;rken. 19.6.18, https://www.bmvg.de/de/aktuelles/europaeische-rues-
tung-staerken-25498 (5.8.2020)
BMVg (2018b): Heron TP. https://www.bmvg.de/de/themen/dossiers/heron-tp (9.10.2020)
BMVg (2018c): MANTIS &#8211; Schutz auf h&#246;chstem Niveau. 26.1.2018, https://www.bmvg.de/de/aktuelles/man-
tis---schutz-auf-hoechstem-niveau-21658 (14.2.2018)
Bodner, M. (2015): Russian Modernization Puts Focus on Land Force Protection. Defense News, 11.10.2015,
https://www.defensenews.com/land/2015/10/11/russian-
modernization-puts-focus-on-land-force-protection/ (5.8.2020)
Bornstein, J. (2015): DoD Autonomy Roadmap Autonomy Community of Interest. In: Defense Technical
Information Center (Hg.): Maps and Gaps in DoD CoIs. CoI = Community of Interest. NDIA 16thAnnual 
Science &amp; Engineering Technology Conference/Defense Tech Exposition. Springfield VA,
24.-26.3.2015, https://ndia
storage.blob.core.usgovcloudapi.net/ndia/2015/SET/WedBornstein.pdf (5.8.2020)
Bostrom, N. (2014): Superintelligenz: Szenarien einer kommenden Revolution. Berlin
Boulanin, V. (2016a): Mapping the development of autonomy in weapon systems. A primer on autonomy,
https://www.sipri.org/sites/default/files/Mapping-develop
ment-autonomy-in-weapon-systems.pdf (5.8.2020)
Boulanin, V. (2016b): Mapping the innovation ecosystem driving the advance of autonomy in weapon
systems, https://www.sipri.org/sites/default/files/Mapping-in
novation-ecosystem-driving-autonomy-in-weapon-systems.pdf (5.8.2020)
Boulanin, V.; Verbruggen, M. (2017): Mapping the Development of Autonomy in Weapon Systems,
https://www.sipri.org/sites/default/files/2017-11/siprireport_
mapping_the_development_of_autonomy_in_weapon_systems_1117_0.pdf (5.8.2020)
Bowcott, O. (2015): UK opposes international ban on developing &#187;killer robots&#171;. Activists urge bar on
weapons that launch attacks without human intervention as UN discusses future of autonomous weapons. The
Guardian, 13.4.2015
Bromley, M.; Maletta, G. (2018): The challenge of software and technology transfers to non-proliferation
efforts. Implementing and complying with export controls, https://www.sipri.org/sites/default/files/2018-
04/sipri1804_itt_software_bromley_et_al.pdf (5.8.2020)
Brooke-Holland, L. (2015): Overview of military drones used by the UK armed forces. BRIEFING PAPER
Number 06493, https://researchbriefings.files.parliament.uk/documents/SN06493/SN06493.pdf
(5.8.2020)
Brown, D. (2018): Russia&#8217;s Uran-9 robot tank reportedly performed horribly in Syria. Business Insider,
9.7.2018, https://www.businessinsider.com/russias-uran-9-robot-tank-performed-horribly-in-syria-2018-
7?r=DE&amp;IR=T (5.8.2020)
Brundage, M.; Avin, S.; Clark, J.; Toner, H.; Eckersley, P.; Garfinkel, B.; Dafoe, A.; Scharre, P.; Zeitzoff, T.; 
Filar, B.; Anderson, H. et al. (2018): The Malicious Use of Artificial Intelligence: Forecasting,
Prevention, and Mitigation, https://img1.wsimg.com/blobby/go/3d82daa4-97fe-4096-9c6b-
376b92c619de/downloads/1c6q2kc4v_50335.pdf (5.8.2020)
Brutzman, D.P.; Davis, D. T.; Lucas, G.R.; McGhee, R.B. (2013): Run-time Ethics Checking for Autonomous
Unmanned Vehicles: Developing a Practical Approach, Paper and Slideset. Proceedings of the 18th
International Symposium on Unmanned Untethered Submersible Technology (UUST), Portsmouth
Buchanan, B.; Miller, T. (2017): Machine Learning for Policymakers. What It Is and Why It Matters.
https://www.belfercenter.org/sites/default/files/files/publication/MachineLearningforPolicymakers.pdf
(5.8.2020)
Bumbacher, B. (2016): Kamikaze-Drohnen im Einsatz. In: Neue Z&#252;rcher Zeitung, 8.4.2016,
https://www.nzz.ch/international/naher-osten-und-nordafrika/koflikt-um-nagorni-karabach-kamikaze-
drohnen-im-einsatz-ld.12404 (5.8.2020)
Bundesregierung (2009): Einf&#252;hrung und Bedeutung unbemannter milit&#228;rischer Fahrzeuge und Luftfahrzeuge.
Antwort der Bundesregierung auf die Kleine Anfrage der Abgeordneten Alexander Bonde, Winfried
Nachtwei, Omid Nouripour, weiterer Abgeordneter und der Fraktion B&#220;NDNIS 90/DIE GR&#220;NEN &#8211;
Drucksache 16/12193 &#8211;. Deutscher Bundestag, Drucksache 16/12481, Berlin
Bundesregierung (2016): Wei&#223;buch 2016: Zur Sicherheitspolitik und zur Zukunft der Bundeswehr. Berlin,
https://www.bundesregierung.de/resource/blob/975292/736102/64781348c12e4a80948ab1bdf25cf057/
weissbuch-zur-sicherheitspolitik-2016-download-data.pdf?download=1 (5.8.2020)
Bundesregierung (2018a): Das Jagdflugzeug &#187;Eurofighter&#171; als Jagdbomber. Antwort der Bundesregierung auf 
die Kleine Anfrage der Abgeordneten Sevim Da&#287;delen, Heike H&#228;nsel, Andrej Hunko, weiterer
Abgeordneter und der Fraktion DIE LINKE. &#8211; Drucksache 19/3939 &#8211;. Deutscher Bundestag, Drucksache 
19/4396, Berlin
Bundesregierung (2018b): Drohnen-Schw&#228;rme in Waffensystemen der Bundeswehr. Antwort der
Bundesregierung auf die Kleine Anfrage der Abgeordneten Andrej Hunko, Heike H&#228;nsel, Michel Brandt, weiterer
Abgeordneter und der Fraktion DIE LINKE. &#8211; Drucksache 19/4715 &#8211;. Deutscher Bundestag, Drucksache 
19/5433, Berlin
Bundesregierung (2018c): Eckpunkte der Bundesregierung f&#252;r eine Strategie K&#252;nstliche Intelligenz. Stand: 
18. Juli 2018, https://www.bmbf.de/files/180718 %20Eckpunkte_KI-Strategie%20final%20Layout.pdf
(5.8.2020)
Bundesregierung (2018d): Jahresabr&#252;stungsbericht 2017. Bericht der Bundesregierung zum Stand der
Bem&#252;hungen um R&#252;stungskontrolle, Abr&#252;stung und Nichtverbreitung sowie &#252;ber die Entwicklung der
Streitkr&#228;ftepotenziale. Deutscher Bundestag, Drucksache 18/1380, Berlin
Bundesregierung (2018e): Neue Planungen zu bewaffneten und bewaffnungsf&#228;higen Drohnen. Antwort der
Bundesregierung auf die Kleine Anfrage der Abgeordneten Andrej Hunko, Michel Brandt, Christine 
Buchholz, weiterer Abgeordneter und der Fraktion DIE LINKE. &#8211; Drucksache 19/567 &#8211;. Deutscher
Bundestag, Drucksache 19/1082, Berlin
Bundesregierung (2018f): Regulierung von Autonomen Waffensystemen. Antwort der Bundesregierung auf
die Kleine Anfrage der Abgeordneten Katja Keul, Agnieszka Brugger, Dr. Tobias Lindner, weiterer
Abgeordneter und der Fraktion B&#220;NDNIS 90/DIE GR&#220;NEN &#8211; Drucksache 19/2816 &#8211;. Deutscher
Bundestag, Drucksache 19/3219, Berlin
B&#252;nte, O. (2018): Milit&#228;r-Projekt Maven: Hunderte Wissenschaftler unterst&#252;tzen protestierende Google-
Mitarbeiter. heise online, 17.5.2018, https://www.heise.de/newsticker/meldung/Militaer-Projekt-Maven-
Hunderte-Wissenschaftler-unterstuetzen-protestierende-Google-Mitarbeiter-4050834.html (5.8.2020)
Campaign to Stop Killer Robots (2017): Unambitious process on killer robots to continue. 24.11.2017,
https://www.stopkillerrobots.org/2017/11/ccwun-3/ (9.10.2020)
Campaign to Stop Killer Robots (2018): Country Views on Killer Robots. https://www.stopkillerrobots.org/
wp-content/uploads/2018/11/KRC_CountryViews22Nov2018.pdf (5.8.2020)
Campbell, M. (2018): Mastering board games. In: Science 362(6419), S. 1118
Campbell, M.; Hoane, A.J.; Hsu, F.-h. (2002): Deep Blue. In: Artificial Intelligence 134(1-2), S. 57&#8211;83
Cartwright, J. (2010): Rise of the robots and the future of war, The Observer, 21.11.2010,
https://www.theguardian.com/technology/2010/nov/21/military-robots-autonomous-machines (5.8.2020)
Cavanaugh, D. (2016): Robot Guns Guard the Borders of Some Countries, and More Might Follow Their
Lead. Offiziere.ch, 12.4.2016, https://www.offiziere.ch/?p=27012 (5.8.2020)
CCW (2015): Report of the 2015 Informal Meeting of Experts on Lethal Autonomous Weapons Systems
(LAWS). Nr. CCW/MSP/2015/3. Genf, https://documents-dds-ny.un.org/doc/UN-
DOC/GEN/G15/111/60/pdf/G1511160.pdf?OpenElement (5.8.2020)
CCW (2016a): Recommendations to the 2016 Review Conference. Submitted by the Chairperson of the
Informal Meeting of Experts. Advanced Version. Genf,
https://www.unog.ch/80256EDD006B8954/(httpAssets)/6BB8A498B0A12A03C1257FDB00382863/$fil 
e/Recommendations_LAWS_2016_AdvancedVersion+(4+paras)+.pdf (5.8.2020)
CCW (2016b): Report of the 2016 Informal Meeting of Experts on Lethal Autonomous Weapons Systems
(LAWS). Advanced Version. Genf, https://www.unog.ch/
80256EDD006B8954/(httpAssets)/DDC13B243BA863E6C1257FDB00380A88/$file/ReportLAWS_
2016_AdvancedVersion.pdf (5.8.2020)
CCW (2016c): United Kingdom of Great Britain and Northern Ireland Statement to the Informal Meeting of
Experts on Lethal Autonomous Weapons Systems. Genf,
https://www.unog.ch/80256EDD006B8954/(httpAs-
sets)/44E4700A0A8CED0EC1257F940053FE3B/$file/2016_LAWS+MX_
Towardaworkingdefinition_Statements_United+Kindgom.pdf (5.8.2020)
CCW GGE (2016a): Non Paper. Characterization of a LAWS. Genf, https://www.unog.ch/
80256EDD006B8954/(httpAssets)/5FD844883B46FEACC1257F8F00401FF6/$file/2016_
LAWSMX_CountryPaper_France+CharacterizationofaLAWS.pdf (9.10.2020)
CCW GGE (2016b): Non Paper. Mapping of Technological Developments. Genf,
https://www.unog.ch/80256EDD006B8954/(httpAs-
sets)/B9E3E8041CE4D326C1257F8F005A31E2/$file/2016_LAWSMX_CountryPaper_France+
MappingofTechnicalDevelopments+EN.pdf (5.8.2020)
CCW GGE (2017a): For consideration by the Group of Governmental Experts on Lethal Autonomous
Weapons Systems (LAWS). Submitted by France and Germany. Item 6 of the revised provisional agenda
Examination of various dimensions of emerging technologies in the area of lethal autonomous weapons
systems, in the context of the objectives and purposes of the Convention. CCW/GGE.1/2017/WP.4. Genf,
https://documents-dds-ny.un.org/doc/UNDOC/GEN/G17/335/40/PDF/G1733540.pdf?OpenElement
(5.8.2020)
CCW GGE (2017b): Autonomy in Weapon Systems. Submitted by the United States of America. Item 6 of
the revised provisional agenda Examination of various dimensions of emerging technologies in the area
of lethal autonomous weapons systems, in the context of the objectives and purposes of the Convention.
Nr. CCW/GGE.1/2017/WP.6. Genf, https://www.unog.ch/80256EDD006B8954/(httpAssets)/
99487114803FA99EC12581D40065E90A/$file/2017_GGEonLAWS_WP6_USA.pdf (5.8.2020)
CCW GGE (2017c): Characteristics of Lethal Autonomous Weapons Systems. Submitted by the United States
of America. Item 6 of the revised provisional agenda Examination of various dimensions of emerging 
technologies in the area of lethal autonomous weapons systems, in the context of the objectives and
purposes of the Convention. Nr. CCW/GGE.1/2017/WP.7. Genf, https://www.unog.ch/80256
EDD006B8954/(httpAssets)/A4466587B0DABE6CC12581D400660157/$file/2017_GGEon-
LAWS_WP7_USA.pdf (5.8.2020)
CCW GGE (2017d): Opening statement USA. CCW Group of Governmental Experts on LAWS. Genf,
https://www.unog.ch/80256EDD006B8954/(httpAssets)/6E9C
8002759032A8C12582490031466C/$file/2017_GGE+LAWS_Statement_USA.pdf (5.8.2020)
CCW GGE (2017e): Report of the 2017 Group of Governmental Experts on Lethal Autonomous Weapons
Systems (LAWS). Item 7 of the agenda Adoption of the report. Nr. CCW/GGE.1/2017/3. Genf,
https://documents-dds-ny.un.org/doc/UNDOC/GEN/G17/367/06/PDF/G1736706.pdf?OpenElement
(5.8.2020)
CCW GGE (2018a): Chair&#8217;s summary of the discussion on Agenda item 6(a) 9 and 10 April 2018 Agenda
item 6 (b) 11 April 2018 and 12 April 2018 Agenda item 6(c) 12April 2018 Agenda item 6(d) 13 April
2018. Genf, https://www.unog.
ch/80256EDD006B8954/(httpAssets)/DF486EE2B556C8A6C125827A00488B9E/$file/
Summary+of+the+discussions+during+GGE+on+LAWS+April+2018.pdf (5.8.2020)
CCW GGE (2018b): EU Statement. Lethal Autonomous Weapons Systems (LAWS) Group of Governmental
Experts Convention on Certain Conventional Weapons, 9&#8211;13 April 2018. Genf,
https://www.unog.ch/80256EDD006B8954/(httpAssets)/00E636906F2DB883C12582720056F109
/$file/2018_LAWSGeneralExchange_EU.pdf (5.8.2020)
CCW GGE (2018c): Position Paper. Submitted by China. Nr. CCW/GGE.1/2018/WP.7. Genf,
https://www.unog.ch/80256EDD006B8954/(httpAssets)/E42AE83BDB3525D0C125826C0040B262
/$file/CCW_GGE.1_2018_WP.7.pdf (5.8.2020)
CCW GGE (2018d): Russia&#8217;s Approaches to the Elaboration of a Working Definition and Basic Functions of
Lethal Autonomous Weapons Systems in the Context of the Purposes and Objectives of the Convention.
Submitted by the Russian Federation. Nr. CCW/GGE.1/2018/WP.6. Genf, https://www.unog.ch/
80256EDD006B8954/(http
Assets)/67F8B7C54687DC55C125825F004B2E72/$file/CCW_GGE.1_2018_WP.6.pdf (5.8.2020)
CCW GGE (2018e): Statement by France and Germany. under Agenda Item &#187;General Exchange of Views&#171;.
Genf, https://www.unog.ch/80256EDD006B8954/(httpAssets)/895931D082ECE219C12582720056F12F
/$file/2018_LAWSGeneralExchange_Germany-France.pdf (5.8.2020)
CCW GGE (2018f): Statement delivered by Germany on Working Definition of LAWS/&#187;Definition of
Systems under Consideration&#171;. Genf, https://www.unog.ch/80256EDD006B8954/(httpAs-
sets)/2440CD1922B86091C12582720057898F/$file/2018_LAWS6a_Germany.pdf (5.8.2020)
CCW GGE (2018g): General principles on Lethal Autonomous Weapons Systems. Submitted by the
Bolivarian Republic of Venezuela on behalf of the Non-Aligned Movement (NAM) and Other States Parties to
the Convention on Certain Conventional Weapons (CCW). Item 6 of the provisional agenda.
Nr. CCW/GGE.1/2018/WP.1. Genf, https://www.unog.ch/80256EDD006B8954/(httpAs-
sets)/E9BBB3F7ACBE8790C125825F004AA329/$file/CCW_GGE_1_2018_WP.1.pdf (5.8.2020)
CCW GGE (2018h): Humanitarian benefits of emerging technologies in the area of lethal autonomous
weapon systems. Item 6 of the provisional agenda. Nr. CCW/GGE.1/2018/WP.4. Genf,
https://www.unog.ch/80256EDD006B8954/(httpAssets)/7C177AE5BC10B588C125825F004B06BE/$fil 
e/CCW_GGE.1_2018_WP.4.pdf (5.8.2020)
CCW GGE (2018i): Human Machine Touchpoints: The United Kingdom&#8217;s perspective on human control over
weapon development and targeting cycles. Item 6 of the provisional agenda.
Nr. CCW/GGE.2/2018/WP.1. Genf, https://www.unog.ch/80256EDD006B8954/(httpAssets)/
050CF806D90934F5C12582E5002EB800/$file/2018_GGE+LAWS_August_Working+Paper_UK.pdf
(5.8.2020)
CCW GGE (2018j): Human-Machine Interaction in the Development, Deployment and Use of Emerging 
Technologies in the Area of Lethal Autonomous Weapons Systems. Submitted by the United States of
America. Nr. CCW/GGE.2/2018/WP.4. Genf, https://www.unog.ch/80256EDD006B8954/(httpAs-
sets)/D1A2BA4B7B71D29FC12582
F6004386EF$file/2018_GGE+LAWS_August_Working+Paper_US.pdf (5.8.2020)
CCW GGE (2018k): Report of the 2018 session of the Group of Governmental Experts on Emerging
Technologies in the Area of Lethal Autonomous Weapons Systems. Nr. CCW/GGE.1/2018/3. Genf, https://un-
docs.org/pdf?symbol=en/CCW/GGE.1/2018/3 (5.8.2020)
CCW GGE (2019): Agenda. Submitted by the Chairperson. Item 2 of the agenda. Adoption of the agenda.
Nr. CCW/GGE.1/2019/1/Rev.1. Genf, https://www.
unog.ch/80256EDD006B8954/(http
Assets)/884DF349536F790BC12583C8004C3AEB/$file/CCW_GGE1_2019_1_Rev.1_Agenda_final.pdf
(5.8.2020)
CDU/CSU; SPD (2018): Ein neuer Aufbruch f&#252;r Europa Eine neue Dynamik f&#252;r Deutschland Ein neuer
Zusammenhalt f&#252;r unser Land. Koalitionsvertrag zwischen CDU, CSU und SPD. Entwurf Stand: 7.2.2018,
http://www.tagesspiegel.
de/downloads/20936562/4/koav-gesamttext-stand-070218-1145h.pdf (5.8.2020)
Chen, S. (2013): After drones, China turns to unmanned vessels to boost its marine power. South China
Morning Post, 5.12.2013, http://www.scmp.com/news/china/article/1373490/after-drones-china-turns-un-
manned-vessels-boost-its-marine-power (5.8.2020)
Chief of Naval Operations (2016): Autonomous Undersea Vehicle Requirement for 2025. Undersea Warfare 
Directorate, Washington D.C., https://news.usni.org/wp-content/uploads/2016/03/18Feb16-Report-to-
Congress-Autonomous-Undersea-Vehicle-Requirement-for-2025.pdf (5.8.2020)
Chinese Delegation (o. J.): The poistion paper submitted by the Chinese delegation to CCW 5th Review
Conference. https://www.unog.ch/80256EDD006B8954/(http
Assets)/DD1551E60648CEBBC125808A005954FA/$file/China%27s+Position+Paper.pdf (5.8.2020)
Clearfield, C.; Tilcsik, A. (2018): Meltdown. Why our systems fail and what we can do about it. New York
Cole, C. (2016): European use of military drones expanding. Drone Wars, 19.7.2016, https://drone-
wars.net/2016/07/19/european-use-of-military-drones-expanding/ (9.10.2020)
Danzig, R. (2018): Technology Roulette. Managing Loss of Control as Many Militaries Pursue Technological 
Superiority. Washington D.C., https://s3.amazonaws.com/
files.cnas.org/documents/CNASReport-Technology-Roulette-Final.pdf (5.8.2020)
DARPA (Defense Advanced Research Projects Agency) (o. J.a): Target Recognition and Adaption in
Contested Environments (TRACE). https://www.darpa.mil/program/trace (9.10.2020)
DARPA (o. J.b): Fast Lightweight Autonomy (FLA) (Archived). https://www.darpa.mil/program/fast-light-
weight-autonomy (9.10.2020)
DARPA (o. J.c): Collaborative Operations in Denied Environment (CODE) (Archived).
https://www.darpa.mil/program/collaborative-operations-in-denied-environment (9.10.2020)
DARPA (o. J.d): Anti-Submarine Warfare (ASW) Continuous Trail Unmanned Vessel (ACTUV) (Archived).
https://www.darpa.mil/program/anti-submarine-warfare-continuous-trail-unmanned-vessel (9.10.2020)
Dassault Aviation (2016): Another world first for the nEUROn. 4.6.2016, https://www.dassault-avia-
tion.com/en/group/press/press-kits/another-world-first-neuron/ (5.8.2020)
Dastin, J. (2018): Amazon scraps secret AI recruiting tool that showed bias against women. Reuters,
10.10.2018, https://www.reuters.com/article/us-amazon-com-jobs-automation-insight/amazon-scraps-
secret-ai-recruiting-tool-that-showed-bias-against-women-idUSKCN1MK08G (5.8.2020)
Davison, N. (2017): A legal perspective: Autonomous weapon systems under international humanitarian law.
In: UNODA (Hg.): Perspectives on lethal autonomous weapon systems. New York, S. 5&#8211;18
DBWV (Deutscher Bundeswehrverband) (2016): Hochmodern und effektiv. Luftwaffe sch&#252;tzt Feldlager mit 
MANTIS. https://www.dbwv.de/landesverbaende-kamerad
schaften/lv-nord/aktuelle-themen/beitrag/news/hochmodern-und-effektiv/ (5.8.2020)
Deagel.com (o. J.): Okhotnik-B. https://www.deagel.com/Combat%20Aircraft/Okhotnik-B/a003499
(9.10.2020)
Dean, S.E. (2017): Unbemannte Unterwassersysteme Trends und technologische Entwicklung. In: Marine
Forum 10, S. 36&#8211;39
Defense Industry Daily (2019): BAE-Lockheed Team supplied USAF with LRASM. https://www.defensein-
dustrydaily.com/bae-lockheed-team-supplied-usaf-with-lrasm-british-typhoons-started-baltic-air-
policing-mission-china-has-a-third-aircraft-carrier-042192/ (5.8.2020)
Defense Update (2006): Harpy air defense suppression system. https://defense-up-
date.com/20060403_harpy.html (9.10.2020)
Deutscher Bundestag (2016): Schriftliche Fragen mit den in der Woche vom 11. Juli 2016 eingegangenen 
Antworten der Bundesregierung. Deutscher Bundestag, Drucksache 18/9191, Berlin
Deutscher Bundestag (2018a): Stenografischer Bericht. 57. Sitzung. M&#252;ndliche Frage 75, Andrej Hunko (DIE
LINKE). Technische Untersuchungen hinsichtlich des Ausweichens milit&#228;rischer Drohnen vor anderen
Luftfahrzeugen. Plenarprotokoll Nr. 19/57, Berlin
Deutscher Bundestag (2018b): Einsetzung einer Enquete-Kommission &#187;K&#252;nstliche Intelligenz &#8211;
Gesellschaftliche Verantwortung und wirtschaftliche, soziale und &#246;kologische Potenziale&#171;. Antrag der Fraktionen
CDU/CSU, SPD, FDP und DIE LINKE. Drucksache 19/2978, Berlin
Dickow, M. (2015): Robotik &#8211; ein Game-Changer f&#252;r Milit&#228;r und Sicherheitspolitik? SWP-Studie 
Nr. 2015/S 14. Berlin, https://www.swp-berlin.org/fileadmin/contents/prod-
ucts/studien/2015_S14_dkw.pdf (5.8.2020)
Dickow, M.; Dahlmann, A.; Alwardt, C.; Sauer, F.; Sch&#246;rnig, N. (2015a): First Steps towards a
Multidimensional Autonomy Risk Assessment (MARA) in Weapons Systems. SWP Working Papers Nr. FG
Sicherheitspolitik WP No 05, Berlin, https://www.swp-berlin.org/fileadmin/contents/products/arbeitspa-
piere/FG03_WP05_2015_MARA.pdf (5.8.2020)
Dickow, M.; Hansel, M.; Mutschler, M.M. (2015b): Pr&#228;ventive R&#252;stungskontrolle &#8211; M&#246;glichkeiten und
Grenzen mit Blick auf die Digitalisierung und Automatisierung des Krieges. In: S+F 33(2), S. 67&#8211;73
DOD (U.S. Department of Defense) (2009): FY2009-2034 Unmanned Systems Integrated Roadmap.
http://www.dtic.mil/get-tr-doc/pdf?Location=U2&amp;doc=GetTRDoc.pdf&amp;AD=ADA522247 (5.8.2020)
DOD (2012): Directive Number 3000.09. SUBJECT: Autonomy in Weapon Systems.
https://www.hsdl.org/?view&amp;did=726163 (5.8.2020)
DOD (2013): Unmanned Systems Integrated Roadmap FY2013-2038. http://archive.defense.gov/pubs/dod-
usrm-2013.pdf (5.8.2020)
DOD (2015a): Annual report to Congress. Military and Security Developments Involving the People&#8217;s
Republic of China 2015. Office of the Secretary of Defense, https://dod.defense.gov/Portals/1/Docu-
ments/pubs/2015_China_Military_Power_Report.PDF (1.9.2020)
DOD (2015b): Technology Investment Strategy 2015-2018. Autonomy Community of Interest (COI) Test and
Evaluation, Verification and Validation (TEVV) Working Group. Office of the Assistant Sectretary of
Defense For Research &amp; Engineering, https://apps.dtic.mil/dtic/tr/fulltext/u2/1010194.pdf (5.8.2020)
DOD (2017a): Annual Report to Congress. Military and Security Developments. Involving the People&#8217;s
Republic of China 2017. Office of the Secretary of Defense, https://dod.defense.gov/Portals/1/Docu-
ments/pubs/2017_China_Military_Power_Report.PDF (1.9.2020)
DOD (2017b): Unmanned Systems Integrated Roadmap. 2017-2042. https://www.defensedaily.com/wp-con-
tent/uploads/post_attachment/206477.pdf (5.8.2020)
DOD (2017c): Department of Defense Announces Successful Micro-Drone Demonstration. https://dod.de-
fense.gov/News/News-Releases/News-Release-View/Article/%201044811/department-of-defense-
announces-successful-micro-drone-demonstration/ (7.8.2020)
DOD (2018a): Nuclear Posture Review. Office of the Secretary of Defense. https://media.de-
fense.gov/2018/Feb/02/2001872886/-1/-1/1/2018-NUCLEAR-POSTURE-REVIEW-FINAL-RE-
PORT.PDF (5.8.2020)
DOD (2018b): Summary of the 2018 National Defense Strategy of the United States of America. Sharpening
the American Military&#8217;s Competitive Edge. Washington D.C., https://dod.defense.gov/Portals/1/Docu-
ments/pubs/2018-National-Defense-Strategy-Summary.pdf (5.8.2020)
DOD (2019): Summary of the 2018 Department of Defense Artificial Intelligence Strategy. Harnessing AI to
Advance Our Security and Prosperity. Washington D.C., https://media.de-
fense.gov/2019/Feb/12/2002088963/-1/-1/1/SUMMARY-OF-DOD-AI-STRATEGY.PDF (5.8.2020)
Dombe, A.R. (2016): The UUV Market is on Fire. Israel Defense, 3.3.2016, https://www.israelde-
fense.co.il/en/content/uuv-market-fire (5.8.2020)
DON (U.S. Department of the Navy) (2018): Strategic Roadmap for Unmanned Systems. Short Version.
https://www.secnav.navy.mil/rda/Documents/DON-Strate
gic-Roadmap-for-Unmanned-Systems.docx (30.4.2019)
Doran, D.; Schulz, S.; Besold, T.R. (2017): What Does Explainable AI Really Mean? A New
Conceptualization of Perspectives. http://arxiv.org/pdf/1710.00794v1 (5.8.2020)
Drew, J. (2017): Drone strike: Long-range attack UAVs being developed from aerial targets. In: Aviation
week &amp; space technology 179(4), S. 42&#8211;43
DSB (Defense Science Board) (2012): The Role of Autonomy in DoD Systems.
https://fas.org/irp/agency/dod/dsb/autonomy.pdf (5.8.2020)
DSB (2016): Summer Study on Autonomy. https://fas.org/irp/agency/dod/dsb/autonomy-ss.pdf (5.8.2020)
Dutch Government (2016): Autonomous weapon systems: the need for meaningful human control.
Government response to AIV/CAVV advisory report no. 97, https://aiv-advice.nl/8gr#government-responses
(9.3.2018)
Eckstein, M. (2016): Navy: Future Undersea Warfare Will Have Longer Reach, Operate With Network of
Unmanned Vehicles. https://news.usni.org/2016/03/24/navy-future-undersea-warfare-will-have-longer-
reach-operate-with-network-of-unmanned-vehicles (5.8.2020)
EDA (European Defence Agency) (2015): Remotely Piloted Aircraft Systems. https://www.eda.eu-
ropa.eu/docs/default-source/eda-factsheets/2015-01-30-factsheet_rpas_high (5.8.2020)
EDA (European Defence Agency) (2019): Remotely Piloted Aircraft Systems &#8211; RPAS. https://www.eda.eu-
ropa.eu/what-we-do/activities/activities-search/remotely-piloted-aircraft-systems---rpas (9.10.2020)
Egozi, A. (2017): Israeli air force preparing new UAV roadmap. FlightGlobal, 14.6.2017,
https://www.flightglobal.com/news/articles/israeli-air-force-preparing-new-uav-roadmap-438228/
(5.8.2020)
EK (Europ&#228;ische Kommission) (2017): Der Europ&#228;ische Verteidigungsfonds: 5,5 Mrd. EUR pro Jahr, um
Europas Verteidigungsf&#228;higkeiten zu st&#228;rken. 7.6.2017, http://europa.eu/rapid/press-release_IP-17-
1508_de.htm (5.8.2020)
Elbit Systems Ltd. (o. J.): Unmanned Surface Vehicle. http://elbitsystems.com/product/unmanned-surface-
vehicle/ (9.10.2020)
Ellman, J.; Samp, L.; Coll, G. (2017): Assessing the Third Offset Strategy. https://csis-prod.s3.amazo-
naws.com/s3fs-public/publication/170302_Ellman_ThirdOffsetStrategySummary_Web.pdf (5.8.2020)
EP (2017): Bericht mit Empfehlungen an die Kommission zu zivilrechtlichen Regelungen im Bereich Robotik
(2015/2103(INL). Stra&#223;burg, https://www.europarl.europa.eu/doceo/document/A-8-2017-0005_DE.pdf
(5.8.2020)
EP (2018): Entschlie&#223;ung des Europ&#228;ischen Parlaments vom 12. September 2018 zu autonomen
Waffensystemen. Nr. 2018/2752(RSP), Stra&#223;burg, https://www.europarl.europa.eu/doceo/document/TA-8-2018-
0341_DE.pdf (5.8.2020)
EP (Europ&#228;isches Parlament) (2014): Entschlie&#223;ung des Europ&#228;ischen Parlaments vom 27. Februar 2014 zum
Einsatz von bewaffneten Drohnen. Nr. P7_TA(2014)0172, Br&#252;ssel, https://op.europa.eu/o/opportal-ser-
vice/download-handler?identifier=a4a4eab5-8c46-11e7-b5c6-01aa75ed71a1&amp;format=pdfa1a&amp;
language=de&amp;productionSystem=cellar&amp;part= (5.8.2020)
Ernest, N.; Carroll, D.; Schumacher, C.; Clark, M.; Cohen, K.; Lee, G. (2016): Genetic Fuzzy based Artificial
Intelligence for Unmanned Combat Aerial Vehicle Control in Simulated Air Combat Missions. In: J Def
Manag 06(01)
Eshel, T. (2015): Russian Military to Test Combat Robots in 2016. https://defense-update.com/20151231_rus-
sian-combat-robots.html (5.8.2020)
Ethik-Kommission (Ethik-Kommission Automatisiertes und vernetztes Fahren) (2017): Ethik-Kommission
Automatisiertes und vernetztes Fahren: Bericht Juni 2017. Bundesministerium f&#252;r Verkehr und digitale
Infrastruktur, https://www.
bmvi.de/SharedDocs/DE/Publikationen/DG/bericht-der-ethik-kommission.pdf?__blob=publicationFile
(5.8.2020)
Ewers, E.C.; Fish, L.; Horowitz, M.C.; Sander, A.; Scharre, P. (2017): Drone Proliferation. Policy Choices for
the Trump Administration. Center for a New American Security, http://www.css.ethz.ch/con-
tent/dam/ethz/special-interest/gess/cis/
centerfor-securities-studies/resources/docs/CNASReport-DroneProliferation-Final.pdf (5.8.2020)
Eykholt, K.; Evtimov, I.; Fernandes, E.; Li, B.; Rahmati, A.; Xiao, C.; Prakash, A.; Kohno, T.; Song, D.
(2018): Robust Physical-World Attacks on Deep Learning Models. http://arxiv.org/pdf/1707.08945v5 
(5.8.2020)
Farmer, B. (2015): Taranis stealth drone may see final test flights later this year. The Telegraph, 13.9.2015,
https://www.telegraph.co.uk/news/uknews/defence/11859967/Taranis-stealth-drone-may-see-final-test-
flights-later-this-year.html (5.8.2020)
Fefegha, A. (2018): Racial Bias and Gender Bias Examples in AI systems. The Comuzi Journal, 2.9.2018,
https://medium.com/thoughts-and-reflections/racial-bias-and-gender-bias-examples-in-ai-systems-
7211e4c166a1 (5.8.2020)
Fischer, J.M.; Ravizza, M. (1998): Responsibility and control. A theory of moral responsibility. Cambridge
FLI (Future of Life Institute) (o. J.a): Autonome Waffen: ein offener Brief von KI- &amp; Robotik-Forschern.
https://futureoflife.org/open-letter-on-autonomous-weapons-german/#top (9.10.2020)
FLI (o. J.b): Die KI-Leits&#228;tze von Asilomar. https://futureoflife.org/ai-principles-german/ (9.10.2020)
Frank, M. (2009): Das ius post bellum und die Theorie des gerechten Krieges. In: PVS 50(4), S. 732&#8211;753
Franke, U.E.; Leveringhaus, A. (2015): Milit&#228;rische Robotik. In: J&#228;ger, T. (Hg.): Handbuch
Sicherheitsgefahren. Wiesbaden, S. 297&#8211;311
GA-ASI (General Atomics Aeronautical Systems, Inc.) (o. J.): Gray Eagle. http://www.ga-asi.com/gray-eagle
(9.10.2020)
Gady, F.-S. (2016): New US Defense Budget: $18 Billion for Third Offset Strategy. The Diplomat, 10.2.2016,
https://thediplomat.com/2016/02/new-us-defense-budget-18-billion-for-third-offset-strategy/ (5.8.2020)
GAO (Governmental Accountability Office) (2018): Artificial Intelligence. Emerging Opportunities,
Challenges, and Implications. HIGHLIGHTS OF A FORUM Convened by the Comptroller General.
Nr. GAO-18-142SP, https://www.gao.gov/assets/700/690910.pdf (5.8.2020)
Gautier, J.; Reiner, D.; Pintat, X. (2016): D&#201;FENSE: &#201;QUIPEMENT DES FORCES. N&#176;142. S&#233;nat Session
Ordinaire de 2016&#8211;2017, http://www.senat.fr/rap/a16-142-8/a16-142-81.pdf (1.9.2020)
Gei&#223;, R. (2015): Die v&#246;lkerrechtliche Dimension autonomer Waffensysteme. Friedrich Ebert Stiftung,
http://library.fes.de/pdf-files/id/ipa/11444-20150619.pdf (5.8.2020)
General Secretariat of the Council (2019): Proposal for a REGULATION OF THE EUROPEAN PARLIA-
MENT AND OF THE COUNCIL establishing the European Defence Fund, (First reading) &#8211; Progress
report. 6733/1/19 REV 1, Council of the European Union, https://eur-lex.europa.eu/legal-con-
tent/EN/TXT/PDF/?uri=
CONSIL:ST_6733_2019_REV_1&amp;from=EN (5.8.2020)
Global Security (o. J.a): Low Cost Autonomous Attack System (LOCAAS). https://www.globalsecu-
rity.org/military/systems/munitions/locaas.htm (9.10.2020)
Global Security (o. J.b): Samsung Techwin SGR-A1 Sentry Guard Robot. https://www.globalsecurity.org/mil-
itary/world/rok/sgr-a1.htm (9.10.2020)
Global Security (o. J.c): Sukhoi S-70 Okhotnik-B. https://www.globalsecurity.org/mili
tary/world/russia/su-70.htm (9.10.2020)
Goodfellow, I.; Papernot, N.; Huang, S.; Duan, R.; Abbeel, P.; Clark, J. (2017): Attacking Machine Learning
with Adversarial Examples. https://openai.com/blog/ad
versarial-example-research/ (5.8.2020)
Goodfellow, I.J.; Shlens, J.; Szegedy, C. (2015): Explaining and Harnessing Adversarial Examples.
http://arxiv.org/pdf/1412.6572v3 (5.8.2020)
GOV.UK (2015): SBRI: the Small Business Research Initiative. https://www.gov.uk/government/collec-
tions/sbri-the-small-business-research-initiative (9.10.2020)
Gramer, R.; Seligman, L. (2019): The INF Treaty Is Dead. Is New START Next? Foreign Policy, 1.2.2019,
https://foreignpolicy.com/2019/02/01/the-inf-treaty-is-dead-is-new-start-next-russia-arms/ (9.10.2020)
Grunwald, A. (2013): Einleitung und &#220;berblick. In: Grunwald, A. (Hg.): Handbuch Technikethik. Stuttgart,
S. 1&#8211;11
Gubrud, M. (2013): US killer robot policy: Full speed ahead. Bulletin of the Atomic Scientists, 20.9.2013,
https://thebulletin.org/2013/09/us-killer-robot-policy-full-speed-ahead/ (5.8.2020)
Gubrud, M. (2015): Semi-autonomous and on their own: Killer robots in Plato&#8217;s Cave. Bulletin of the Atomic 
Scientists, 12.4.2015, https://thebulletin.org/2015/04/semi-autonomous-and-on-their-own-killer-robots-
in-platos-cave/ (5.8.2020)
Gubrud, M.; Altmann, J. (2013): Compliance Measures for an Autonomous Weapons Convention. ICRAC
Working Paper #2, https://www.icrac.net/wp-content/uploads/2018/04/Gubrud-Altmann_Compliance-
Measures-AWC_ICRAC-WP2.pdf (5.8.2020)
Gunning, D. (2017): Explainable Artificial Intelligence (XAI). DARPA/I2O, https://www.darpa.mil/attach-
ments/XAIProgramUpdate.pdf (5.8.2020)
Gutmann, T. (2010): Struktur und Funktion der Menschenw&#252;rde als Rechtsbegriff. Preprints of the Centre for
Advanced Study in Bioethics. M&#252;nster, https://www.uni-muenster.de/imperia/md/content/kfg-normenbe-
gruendung/intern/publikationen/gutmann/07_gutmann_-_menschenw__rde_als_rechtsbegriff.pdf 
(5.8.2020)
Hagel, C. (2014): Reagan National Defense Forum Keynote. Speech, 15.11.2014, https://dod.de-
fense.gov/News/Speeches/Speech-View/Article/606635/ (5.8.2020)
Haider, A. (2014): Remotely Piloted Aircraft Systems in Contested Environments. A Vulnerability Analysis.
Kalkar, http://www.japcc.org/wp-content/uploads/2015/03/JAPCC-RPAS-Operations-in-Contested-En-
vironments.pdf (5.8.2020)
Hambling, D. (2014a): Armed Russian robocops to defend missile bases. New Scientist, 23.4.2014,
https://www.newscientist.com/article/mg22229664-400-armed-russian-robocops-to-defend-missile-ba-
ses/ (1.9.2020)
Hambling, D. (2014b): Russia Wants Autonomous Fighting Robots, and Lots of Them. Popular Mechanics,
12.5.2014, https://www.popularmechanics.com/military/a10511/russia-wants-autonomous-fighting-ro-
bots-and-lots-of-them-16787165/ (5.8.2020)
Hawley, J.K. (2017): Patriot Wars. Automation and the Patriot Air and Missile Defense System. Center for a 
New American Security, Ethical Autonomy Series, https://s3.amazonaws.com/files.cnas.org/docu-
ments/CNAS-Report-EthicalAutonomy5-PatriotWars-FINAL.pdf?mtime=20170106135013 (5.8.2020)
He, K.; Zhang, X.; Ren, S.; Sun, J. (2015): Delving Deep into Rectifiers: Surpassing Human-Level
Performance on ImageNet Classification. http://arxiv.org/pdf/1502.01852v1 (5.8.2020)
Heinrich, A. (2018): Verpasste Chancen. Die russische Krim-Annexion und Ukraine-Krise haben die
europ&#228;ische Sicherheitsordnung ersch&#252;ttert. K&#246;nnen Russland und der Westen neues Vertrauen aufbauen? In: 
Das Parlament 15&#8211;16, S. 10
Hellstr&#246;m, T. (2013): On the moral responsibility of military robots. In: Ethics Inf Technol 15(2), S. 99&#8211;107
Heyns, C. (2013): Report of the Special Rapporteur on extrajudicial, summary or arbitrary executions,
Christof Heyns. General Assembly, United Nations, Nr. A/HRC/23/47, http://www.ohchr.org/Docu-
ments/HRBodies/HRCouncil/Re
gularSession/Session23/A-HRC-23-47_en.pdf (5.8.2020)
Heyns, C. (2016): Autonomous weapon systems: living a dignified life and dying a dignified death. In: Bhuta,
N.; Beck, S.; Gei&#223;, R.; Liu, H.-Y.; Kre&#223;, C. (Hg.): Autonomous weapons systems. Law, ethics, policy.
Cambridge, S. 3&#8211;20
Hilgendorf, E. (2012): K&#246;nnen Roboter schuldhaft handeln? In: Beck, S. (Hg.): Jenseits von Mensch und
Maschine: Ethische und rechtliche Fragen zum Umgang mit Robotern, k&#252;nstlicher Intelligenz und Cyborgs.
Baden-Baden, S. 119&#8211;132
Hilgendorf, E. (Hg.) (2014): Robotik im Kontext von Recht und Moral. Robotik und Recht 3, Baden-Baden
Hoffmann, L. (2015): Germany To Lead Development of European UAV. Defense News, 11.12.2015,
https://www.defensenews.com/air/2015/12/11/germany-to-lead-development-of-european-uav/
(5.8.2020)
Hoppe, T.; Werkner, I.-J. (2017): Der gerechte Frieden: Positionen in der katholischen und evangelischen
Kirche in Deutschland. In: Werkner, I.-J.; Ebeling, K. (Hg.): Handbuch Friedensethik. Wiesbaden, S. 341&#8211; 
358
Horowitz, M.C.; Kreps, S.E.; Fuhrmann, M. (2016): Separating Fact from Fiction in the Debate over Drone 
Proliferation. In: International Security 41(2), S. 7&#8211;42
House of Lords Select Committee on Artificial Intelligence (2018): AI in the UK: ready, willing and able?
Report of Session 2017&#8211;19. House of Lords, HL Paper Nr. 100, London, https://publications.parlia-
ment.uk/pa/ld201719/ldselect/ldai/100/100.pdf (5.8.2020)
HRW (Human Rights Watch) (2012): Losing humanity. The case against killer robots (Docherty, B.). New
York
HRW (2014): Shaking the Foundations. The Human Rights Implications of Killer Robots. New York,
https://www.hrw.org/sites/default/files/reports/arms0514_ForUpload_0.pdf (5.8.2020)
HRW (2017): US Embraces Cluster Munitions. 1.12.2017, https://www.hrw.org/news/2017/12/01/us-em-
braces-cluster-munitions (9.10.2020)
Hu, J.C. (2018): Waymo&#8217;s driverless cars have logged 10 million miles on public roads. Quartz, 10.10.2018,
https://qz.com/1419747/waymos-self-driving-cars-have-log
ged-10-million-miles/ (5.8.2020)
Hutson, M. (2018): AI takes on video games in quest for common sense. In: Science 361(6403), S. 632&#8211;633
IAI (o. J.): HARPY. Autonomous Weapon for All Weather. https://www.iai.co.il/p/harpy (9.10.2020)
IBM (o. J.): So ist die Welt mit IBM Watson. https://www.ibm.com/thought-leadership/smart/de-
de/watson/ai-stories/ (9.10.2020)
ICBL-CMC (International Campaign to Ban Landmines-Cluster Munition Coalition) (2017): Cluster Munition
Monitor 2017. http://www.the-monitor.org/media/2582190/Cluster-Munition-Monitor-2017_web4.pdf
(5.8.2020)
ICC (International Criminal Court) (o. J.): The States Parties to the Rome Statute. https://asp.icc-
cpi.int/en_menus/asp/states%20parties/Pages/the%20states%20parties%20to%20the%20rome%20stat-
ute.aspx (9.10.2020)
IEEE (Institute of Electrical and Electronics Engineers) (2016): Ethically Alligned Design. A Vision for
Prioritizing Human Wellbeing with Artificial Intelligence and Autonomous Systems. Version 1 &#8211; For Public
Discussion. http://standards.
ieee.org/develop/indconn/ec/ead_v1.pdf (5.8.2020)
IKRK (Internationales Kommittee vom Roten Kreuz) (1987): Protocol Additional to the Geneva Conventions
of 12 August 1949, and relating to the Protection of Victims of International Armed Conflicts (Protocol 
I), 8 June 1977. Commentary of 1987 Responsibility. https://ihl-databases.icrc.org/applic/ihl/ihl.nsf/Arti-
cle.xsp?
action=openDocument&amp;documentId=F461FC196C18A52DC12563CD0051E2AC (5.8.2020)
IKRK (2006): A Guide to the Legal Review of New Weapons, Means and Methods of Warfare. Measures to
implement Article 36 of Additional Protocol I of 1977. Genf, https://www.icrc.org/eng/as-
sets/files/other/icrc_002_0902.pdf (5.8.2020)
IKRK (2018a): Ethics and autonomous weapon systems: An ethical basis for human control? Group of
Governmental Experts of the High Contracting Parties to the CCW. Nr. CCW/GGE.1/2018/WP.5. Genf,
https://www.unog.ch/80256EDD
006B8954/(http
Assets)/42010361723DC854C1258264005C3A7D/$file/CCW_GGE.1_2018_WP.5+ICRC+final.pdf
(5.8.2020)
IKRK (2018b): Statement of the International Committee of the Red Cross (ICRC). Genf,
https://www.icrc.org/en/document/towards-limits-autonomous-weapons (5.8.2020)
IKRK (2019): Statement of the International Committee of the Red Cross (ICRC) under agenda item 5(e)
Possible options for addressing the humanitarian and international security challenges posed by emerging
technologies in the area of lethal autonomous weapon systems in the context of the objectives and
purposes of the Convention without prejudicing policy outcomes and taking into account past, present and
future proposals. Convention on Certain Conventional Weapons (CCW) Group of Governmental Experts
on Lethal Autonomous Weapons Systems. Genf, https://www.unog.ch/80256EDD006B8954/(httpAs-
sets)/59013C15
951CD355C12583CC002FDAFC/$file/CCW+GGE+LAWS+ICRC+statement+agenda+item
+5e+27+03+2019.pdf (5.8.2020)
IKRK (o. J.): Geneva Conventions of 1949 and Additional Protocols, and their Commentaries. https://ihl-data-
bases.icrc.org/applic/ihl/ihl.nsf/vwTreaties1949.xsp (9.10.2020)
Indian Defense Blog (2017): Unmanned ground vehicle developement in India. 10.12.2017, https://indiandef-
blog.wordpress.com/2017/12/10/unmanned-ground-vehicle-developement-in-india/ (1.10.2019)
IPRAW (International Panel on the Regulation of Autonomous Weapons) (2017a): Focus on Technology and
Application of Autonomous Weapons. &#187;Focus on&#171; Report No. 1, Berlin, https://www.ipraw.org/wp-con-
tent/uploads/2017/08/2017-08-17_iPRAW_Focus-On-Report-1.pdf (5.8.2020)
IPRAW (2017b): Focus on Computational Methods in the Context of LAWS. &#187;Focus on&#171; Report No. 2,
Berlin, https://www.ipraw.org/wp-content/uploads/2017/11/2017-11-10_iPRAW_Focus-On-Report-2.pdf
(5.8.2020)
IPRAW (2018a): Focus on Ethical Implications for a Regulation of LAWS. &#187;Focus on&#171; Report No. 4, Berlin,
https://www.ipraw.org/wp-content/uploads/2018/08/2018-08-17_iPRAW_Focus-On-Report-4.pdf
(5.8.2020)
IPRAW (2018b): Concluding Report: Recommendations to the GGE. Berlin, https://www.ipraw.org/wp-con-
tent/uploads/2018/12/2018-12-14_iPRAW_Concluding-Report.pdf (5.8.2020)
Jaroslawl (2017): &#1054;&#1090;&#1082;&#1088;&#1099;&#1090;&#1099;&#1081; &#1091;&#1088;&#1086;&#1082; &#187;&#1056;&#1086;&#1089;&#1089;&#1080;&#1103;, &#1091;&#1089;&#1090;&#1088;&#1077;&#1084;&#1083;&#1105;&#1085;&#1085;&#1072;&#1103; &#1074; &#1073;&#1091;&#1076;&#1091;&#1097;&#1077;&#1077;&#171;. &#1040;&#1076;&#1084;&#1080;&#1085;&#1080;&#1089;&#1090;&#1088;&#1072;&#1094;&#1080;&#1103; &#1055;&#1088;&#1077;&#1079;&#1080;&#1076;&#1077;&#1085;&#1090;&#1072;
&#1056;&#1086;&#1089;&#1089;&#1080;&#1080;, 1.9.2017, http://kremlin.ru/events/president/news/55493 (9.10.2020)
Jenkins, R.; Purves, D. (2016): Robots and Respect: A Response to Robert Sparrow. In: Ethics int. aff. 30(03),
S. 391&#8211;400
Johnson, D.D.P. (2004): Overconfidence and war. The havoc and glory of positive illusions. Cambridge
Johnson, D.G. (2015): Technology with No Human Responsibility? In: J Bus Ethics 127(4), S. 707&#8211;715
J&#246;nsson, H. (2007): Risk and vulnerability analysis of complex systems. A basis for proactive emergency
management. Report 1038, Lund
Jun, J.-h. (2018): Hanwha, KAIST to develop AI weapons. Controversy remains on whether autonomous arms
are really necessary. In: Korean Times, 25.2.2018, https://www.ko-
reatimes.co.kr/www/tech/2018/02/133_244641.html (1.9.2020)
K.S. (2019): FCAS-Konzeptstudien starten. FLUG REVUE, 6.2.2019, https://www.flugrevue.de/militaer/das-
sault-aviation-und-airbus-fcas-konzeptstudien-starten/ (9.10.2020)
Kania, E. (2017): Battlefield Singularity. Artificial Intelligence, Military Revolution, and China&#8217;s Future
Military Power. Center for a New American Security, Washington D.C., https://s3.amazo-
naws.com/files.cnas.org/documents/Battlefield-Sin
gularity-November-2017.pdf (7.8.2020)
Kania, E. (2018): China&#8217;s Strategic Ambiguity and Shifting Approach to Lethal Autonomous Weapons
Systems. Lawfare Institute, 17.4.2018, https://www.lawfare
blog.com/chinas-strategic-ambiguity-and-shifting-approach-lethal-autonomous-weapons-systems
(7.3.2019)
Kasparov, G. (2018): Chess, a Drosophila of reasoning. In: Science 362(6419), S. 1087
Kettner, M. (Hg.) (2004): Biomedizin und Menschenw&#252;rde. Frankfurt a.M.
Kirkpatrick, J.; Pascanu, R.; Rabinowitz, N.; Veness, J.; Desjardins, G.; Rusu, A.A.; Milan, K.; Quan, J.; 
Ramalho, T.; Grabska-Barwinska, A.; Hassabis, D.; Clopath, C. et al. (2017): Overcoming catastrophic 
forgetting in neural networks. London, http://arxiv.org/pdf/1612.00796v2 (7.8.2020)
Koch, B. (2017): Bewaffnete Drohnen. Bernhard Koch zur Frage, was ihren milit&#228;rischen Einsatz so
fragw&#252;rdig macht. In: Information Philosophie (3), S. 8&#8211;15
Koch, B.; Rinke, B. (2017): Ethische Fragestellungen im Kontext autonomer Waffensysteme. Institut f&#252;r
Theologie und Frieden, Hamburg
Kratos (Kratos Defense &amp; Security Solutions, Inc.) (o. J.): Tactical UAVs.
https://www.kratosdefense.com/systems-and-platforms/unmanned-systems/aerial/tactical-uavs
(9.10.2020)
Krischke, W. (2018): Sprachwissenschaft: Altbew&#228;hrtes frischgemacht. Digital Humanities (1/6). Frankfurter 
Allgemeine, 9.5.2018, https://www.faz.net/aktuell/karriere-hochschule/digital-humanities-eine-bilanz-1-
6-sprachwissenschaft-15579104.html (9.10.2020)
L3Harris ASV (2018): First images of the Thales Mine Warfare USV. https://www.asvglobal.com/brest-first-
images-of-the-thales-mine-warfare-usv/ (9.10.2020)
Launchbury, J. (o. J.): A DARPA Perspective on Artificial Intelligence. DARPA/I2O,
https://www.darpa.mil/attachments/AIFull.pdf (1.9.2020)
Lei, Z. (2016): Nation&#8217;s next generation of missiles to be highly flexible. China Daily, 19.8.2016,
http://www.chinadaily.com.cn/china/2016-08/19/content_26530461.htm (7.8.2020)
Leveringhaus, A. (2016): Ethics and autonomous weapons. Palgrave pivot. London
Lewis, D.A.; Blum, G.; Modirzadeh, N.K. (2016): War-Algorithm Accountability.
http://arxiv.org/pdf/1609.04667v1 (7.8.2020)
Lewis, J. (2015): The Case for Regulating Fully Autonomous Weapons. comment. In: The Yale Law Journal 
124, S. 1309&#8211;1325
Lin, J.; Singer, P.W. (2014): China&#8217;s New Military Robots Pack More Robots Inside (Starcraft-Style).
https://www.popsci.com/blog-network/eastern-arsenal/chinas-new-military-robots-pack-more-robots-
inside-starcraft-style (7.8.2020)
Lin, J.; Singer, P.W. (2017): Meet China&#8217;s Sharp Sword, a stealth drone that can likely carry 2 tons of bombs.
Popular Science, 18.1.2017, https://www.popsci.com/china-sharp-sword-lijian-stealth-drone (7.8.2020)
Linn, A. (2018): Microsoft creates AI that can read a document and answer questions about it as well as a
person. Microsoft, 15.1.2018, https://blogs.microsoft.com/ai/microsoft-creates-ai-can-read-document-an-
swer-questions-well-person/ (9.10.2020)
Lischka, K.; Klingel, A.; Bertelsmann Stiftung (2017): Wenn Maschinen Menschen bewerten. Internationale 
Fallbeispiele f&#252;r Prozesse algorithmischer Entscheidungsfindung &#8211; Arbeitspapier &#8211;, Bertelsmann
Stiftung, http://web.archive.org/web/20180123143255if_/https://www.bertelsmann-stiftung.de/fileadmin/fi-
les/BSt/Publikationen/GrauePublikationen/ADM_Fallstudien.pdf (7.8.2020)
Lockheed Martin Corporation (o. J.): LRASM. Long Range Anti-Ship Missile. https://www.lockheedmar-
tin.com/en-us/products/long-range-anti-ship-missile.html (9.10.2020)
Mallik, A. (2004): Technology and Security in the 21st Century: A Demand-side Perpective. SIPRI Research
Report No. 20, Oxford
Marchant, G.E.; Allenby, B.; Arkin, R.; Barrett, E.T.; Borenstein, J.; Gaudet, L.M.; Kittrie, O.; Lin, P.; Lucas,
G. R.; O&#8217;Meara, R.; Silberman, J. (2011): International Governance of Autonomous Military Robots. In: 
The Columbia Science and Technology Law Review (XII), S. 272&#8211;315
Marcus, G. (2018): Deep Learning: A critical appraisal. New York University, https://arxiv.org/ftp/arxiv/pa-
pers/1801/1801.00631.pdf (7.8.2020)
Markoff, J. (2015): A Learning Advance in Artificial Intelligence Rivals Human Abilities. New York Times,
10.12.2015, https://www.nytimes.com/2015/12/11/science/an-advance-in-artificial-intelligence-rivals-
human-vision-abilities.html (1.9.2020)
Markowitz, M.; Gresham, J.D. (2012): Dual-Mode Brimstone Missile Proves Itself in Combat. Defense Media 
Network, 26.4.2012, https://www.defensemedianetwork.com/stories/dual-mode-brimstone-missile-
proves-itself-in-combat/ (7.8.2020)
Masirske, H.-A. (2009): Auch Roboter der Bundeswehr sollen schie&#223;en. Heise online, 12.2.2009,
https://www.heise.de/tp/features/Auch-Roboter-der-Bundeswehr-sollen-schiessen-3505402.html
(7.8.2020)
Max (2016): Luftwaffe f&#252;hrt Brimstone f&#252;r Eurofighter ein, 15.11.2016, Hartpunkt.de, https://www.hart-
punkt.de/luftwaffe-fuehrt-brimstone-fuer-eurofighter-ein/ (9.10.2020)
McCaney, K. (2015): DARPA wants to plumb the depths of an underwater Internet. Defense Systems,
22.4.2015, https://defensesystems.com/articles/2015/04/22/darpa-underwater-internet-communica-
tions.aspx (7.8.2020)
McCaney, K. (2016): Boeing&#8217;s new autonomous UUV can run for months at a time. Defense Systems,
14.3.2016, https://defensesystems.com/articles/2016/03/14/boeing-echo-voyager-uuv.aspx (7.8.2020)
McClelland, J. (2003): The review of weapons in accordance with Article 36 of Additional Protocol I. In: Int.
rev. Red Cross 85(850), S. 397&#8211;415
MDBA (2018): Brimstone. Precision Surface Attack Weapon. Data Sheet. https://www.mbda-sys-
tems.com/?action=force-download-attachment&amp;attachment_id=16010 (7.8.2020)
Meier, C.J. (2018): IBMs virtueller Arzt macht Fehler. Neue Z&#252;rcher Zeitung, 9.8.2018,
https://www.nzz.ch/wissenschaft/ibms-virtueller-arzt-watson-for-oncology-macht-fehler-ld.1410111 
(1.9.2020)
Meier, O. (2019): R&#252;stungskontrolle jenseits des INF-Vertrags: Ans&#228;tze zur Kontrolle von
Mittelstreckenraketen nach dem Ende des Abkommens. Stiftung Wissenschaft und Politik, SWP-Aktuell 20, Berlin
Melzer, N. (2009): Interpretive Guidance on the Notion of Direct Participation in Hostilities Under
International Humanitarian Law. https://www.icrc.org/eng/assets/files/other/icrc-002-0990.pdf (7.8.2020)
Meteor Aerospace Ltd. (o. J.): RAMBOW Unmanned Ground Vehicle (UGV). https://e07a06c2-2562-4ebb-
8514-35b96f794766.filesusr.com/ugd/9c6c4c_f56e383eb6ed47f281f7f4b3bcc46bf5.pdf (9.10.2020)
Michaels, J. (2013): Rand Paul Filibustering Brennan Nomination to Lead CIA. USA TODAY, 7.3.2013,
https://www.usatoday.com/story/news/politics/2013/03/06/brennan-nomination-nears-senate-
vote/1967709/ (7.8.2020)
Military Factory (o. J.a): Unmanned Combat Aerial Vehicles (UCAVs). https://www.militaryfactory.com/air-
craft/unmanned-combat-air-vehicle-ucav.asp (9.10.2020)
Military Factory (o. J.b): Kalashnikov BAS-01G BM Soratnik Unmanned Combat Ground Vehicle (UCGV).
https://www.militaryfactory.com/armor/detail.asp?armor_id=1035 (9.10.2020)
Mindell, D. A. (2015a): Our robots, ourselves. Robotics and the myths of autonomy. New York
Mindell, D.A. (2015b): Driverless Cars and the Myths of Autonomy. Huffington Post, 14.10.2015, Update
6.12.2017, https://www.huffpost.com/entry/driverless-cars-and-the-myths-of-autonomy_b_8287230 
(1.9.2020)
Minist&#232;re de la D&#233;fense (2013): The French White Paper on defence and national security. http://www.de-
fense.gouv.fr/english/content/download/206186/2393586/file/White%20paper%20on%20
defense%20 %202013.pdf (1.9.2020)
Minist&#232;re de la D&#233;fense; DGA (D&#233;l&#233;gu&#233; g&#233;n&#233;ral pour l&#8217;armement) (2013): Document de presentation de l&#8217;
orientation de la S&amp;T P&#233;riode 2014-2019. Bagneux
Ministerie van Defensie (2016): Strategische Kennis &amp; Innovatie Agenda 2016-2020. V&#243;&#243;rblijven in een 
onveiliger Wereld. Ministerie van Defensie. Den Hag, https://www.defensie.nl/binaries/defensie/docu-
menten/rapporten/2016/11/01/strategische-kennis--en-innovatieagenda-2016/SKIA+2016-2020.pdf
(21.8.2018)
Misselhorn, C. (2018): Maschinenethik und &#187;Artificial Morality&#171;: K&#246;nnen und sollen Maschinen moralisch
handeln? In: Aus Politik und Zeitgeschichte (6&#8211;8), S. 29&#8211;33
Missile Technology Control Regime (2017): Equipment, Software and Technology Annex.
Nr. MTCR/TEM/2017/Annex, http://mtcr.info/wordpress/wp-content/uploads/2017/10/MTCR-TEM-
Technical_Annex_2017-10-19-corr.pdf (7.8.2020)
Mnih, V.; Kavukcuoglu, K.; Silver, D.; Rusu, A.A.; Veness, J.; Bellemare, M.G.; Graves, A.; Riedmiller, M.; 
Fidjeland, A.K.; Ostrovski, G.; Petersen, S. et al. (2015): Human-level control through deep
reinforcement learning. In: Nature 518, S. 529&#8211;533
MOD (Ministry of Defence) (2011): RAF conducts precision strikes over Libya. https://www.gov.uk/govern-
ment/news/raf-conducts-precision-strikes-over-libya (1.9.2020)
MOD (2013): UK Air and Space Doctrine. Joint Doctrine Publication Nr. JDP 0-30. http://www.defencesy-
nergia.co.uk/wp-content/uploads/2015/05/jdp_0_30_uk_air_and_space_doctrine.pdf (7.8.2020)
MOD (2015): Future Operating Environment 2035. Strategic Trends Programme: https://assets.publishing.ser-
vice.gov.uk/government/uploads/system/uploads/at
tachment_data/file/646821/20151203-FOE_35_final_v29_web.pdf (7.8.2020)
MOD (2017a): Autonomy on the front line: supplying Armed Forces on the battlefield. Ministry of Defence,
Defence Science and Technology Laboratory, Defence and Security Accelerator, Baldwin, H.,
https://www.gov.uk/government/news/autonomy-on-the-front-line-supplying-armed-forces-on-the-bat-
tlefield (7.8.2020)
MOD (2017b): Unmanned Aircraft Systems. Joint Doctrine Publication Nr. 0-30.2. https://www.gov.uk/gov-
ernment/uploads/system/uploads/attachment_data/file/640299/20170706_JDP_0-30.2_final_CM_
web.pdf (7.8.2020)
MOD (2018): Human-Machine Teaming. Joint Concept Note 1/18, https://www.gov.uk/government/up-
loads/system/uploads/attachment_data/file/709359/20180517-concepts_uk_human_machine_team-
ing_jcn_1_18.pdf (18.9.2018)
Morav&#269;&#237;k, M.; Schmid, M.; Burch, N.; Lis&#253;, V.; Morrill, D.; Bard, N.; Davis, T.; Waugh, K.; Johanson, M.;
Bowling, M. (2017): DeepStack: Expert-level artificial intelligence in heads-up no-limit poker. In:
Science 356(6337), S. 508&#8211;513
M&#252;ller, V.C. (2016): Autonomous Killer Robots Are Probably Good News. In: Di Nucci, E.; Santoni de Sio,
F. (Hg.): Drones and responsibility. Legal, philosophical, and sociotechnical perspectives on the use of
remotely controlled weapons. London/New York, S. 67&#8211;81
Munich Security Conference (2019): Munich Security Report 2019. The Great Puzzle: Who Will Pick Up the 
Pieces? M&#252;nchen, http://www.eventanizer.com/MSR/MSC2019/ (7.8.2020)
Najberg, A. (2018): Alibaba AI Model Tops Humans in Reading Comprehension. Alizila, 15.1.2018,
https://www.alizila.com/alibaba-ai-model-tops-humans-in-reading-comprehension/ (9.10.2020)
Nasu, H.; McLaughlin, R. (2014): New Technologies and the Law of Armed Conflict. Den Haag
Naval Drones (2016): Unmanned Carrier Launched Surveillance and Strike (UCLASS) Program.
http://www.navaldrones.com/UCLASS.html (9.10.2020)
Naval Technology (o. J.a): Fire Scout VTUAV. https://www.naval-technology.com/projects/fire-scout-vtuav/
(9.10.2020)
Naval Technology (o. J.b): Long Range Anti-Ship Missile (LRASM). https://www.naval-technology.com/pro-
jects/long-range-anti-ship-missile/ (9.10.2020)
NAVALTODAY (2017): US Navy establishes first unmanned undersea vehicle squadron. 27.9.2017,
https://www.navaltoday.com/2017/09/27/us-navy-establishes-first-unmanned-undersea-vehicle-squad-
ron/ (9.10.2020)
Navy Recognition (2018): Russia Started Sea Trials of Klavesin-2 UUV in Crimea. Navy Recognition,
18.5.2018, http://www.navyrecognition.com/index.php/focus-analysis/naval-technology/6234-russia-
started-sea-trials-of-klavesin-2-uuv-in-crimea.html (9.10.2020)
Neuh&#228;user, C. (2014): Roboter und moralische Verantwortung. In: Hilgendorf, E. (Hg.): Robotik im Kontext
von Recht und Moral. Baden-Baden, S. 269&#8211;286
Neuneck, G. (2014): High-Tech im Krieg der Zukunft: Neue Technologien als Herausforderung f&#252;r die Innere 
F&#252;hrung. In: Hartmann, U.; von Rosen, C. (Hg.): Drohnen, Roboter und Cyborgs: der Soldat im
Angesicht neuer Milit&#228;rtechnologien. Berlin, S. 60&#8211;73
Neuneck, G. (2017): Wie weiter nach New START? Aktuelle Probleme der R&#252;stungsbegrenzung. In:
FriedensForum (5), S. 32&#8211;34
Neuneck, G.; Mutz, R. (Hg.) (2000): Vorbeugende R&#252;stungskontrolle. Ziele und Aufgaben unter besonderer
Ber&#252;cksichtigung verfahrensm&#228;&#223;iger und institutioneller Umsetzung im Rahmen internationaler
R&#252;stungsregime. Demokratie, Sicherheit, Frieden 130, Baden-Baden
New America (o. J.a): The Future of Drone Warfare: The Rise of Maritime Drones. https://www.ne-
wamerica.org/in-depth/world-of-drones/7-future-drone-warfare-rise-maritime-drones/ (9.10.2020)
New America (o. J.b): Who has what: countries with armed drones. https://www.newamerica.org/interna-
tional-security/reports/world-drones/who-has-what-countries-with-armed-drones/ (9.10.2020)
New America (o. J.c): Who has what: countries that have conducted drone strikes. https://www.newamer-
ica.org/in-depth/world-of-drones/2-who-has-what-countries-drones-used-combat/ (9.10.2020)
Nida-R&#252;melin, J.; Schulenburg, J.; Rath, B.; (2012): Risikoethik. Berlin
NIST (National Institute of Standards and Technology) (2007): Autonomy Levels for Unmanned Systems
(ALFUS) Framework. Volume II: Framework Models. Version 1.0. Special Publication Nr. 1011-II-1.0, 
http://ws680.nist.gov/publication/get_pdf.cfm?pub_id=823618 (7.8.2020)
NIST (2008): Autonomy Levels for Unmanned Systems (ALFUS) Framework. Volume I: Terminology.
Version 2.0. Special Publication Nr. 1011-I-2.0, https://www.nist.gov/system/files/documents/el/isd/ks/
NISTSP_1011-I-2-0.pdf (7.8.2020)
Noorman, M.; Johnson, D.G. (2014): Negotiating autonomy and responsibility in military robots. In: Ethics
Inf Technol 16(1), S. 51&#8211;62
Northrop Grumman Systems Corporation (2015): X-47B UCAS. Unmanned Combat Air System. San Diego,
https://www.northropgrumman.com/wp-content/uploads/UCAS-D_Data_Sheet.pdf (9.10.2020)
Northrop Grumman (o. J.): Electronic Warfare. https://www.northropgrumman.com/what-we-do/sea/elec-
tronic-warfare/ (9.10.2020)
OCCAR (Organisation for Joint Armament Co-operation) (o. J.) MALE RPAS &#8211; Medium Altitude Long
Endurance Remotely Piloted Aircraft System. Bonn
OECD (o. J.): Emerging technologies. www.oecd.org/sti/emerging-tech/ (9.10.2020)
ONR (Office of Naval Research) (o. J.a): Science of Autonomy. https://www.onr.navy.mil/en/Science-Tech-
nology/Departments/Code-35/All-Programs/aerospace-sci
ence-research-351/science-of-autonomy (9.10.2020)
ONR (o. J.b): Computational Methods for Decision Making &#8211; Large Scale Distributed Decision-making.
https://www.onr.navy.mil/en/Science-Technology/Departments/Code-31/All-Programs/311-Mathemat-
ics-Computers-Research/computational-methods-large-scale-distributed-decision-making (9.10.2020)
Opall-Rome, B. (2016): Israel Navy Readies for Third-Generation USV. Defense News, 27.7.2016,
https://www.defensenews.com/naval/2016/07/27/israel-navy-readies-for-third-generation-usv/ (7.8.2020)
OPCW (Organisation for the Prohibition of Chemical Weapons) (2015): The Hague Ethical Guidelines. Den
Haag, https://www.opcw.org/fileadmin/OPCW/Science_Technology/Hague_Ethical_Guidelines_Bro-
chure.pdf (7.8.2020)
Osborn, K. (2016): Navy awards MQ-25 Stingray tanker deal. Defense Systems, 24.10.2016, https://defens-
esystems.com/articles/2016/10/24/stingray.aspx (7.8.2020)
OSZE (Organisation for Security and Co-operation in Europe) (2011): Wiener Dokument 2011 &#252;ber
vertrauens- und sicherheitsbildende Ma&#223;nahmen. https://www.osce.org/de/fsc/86599?download=true (7.8.2020)
Pamuk, H.; Shepardson, D. (2019): Trump administration unveils order to prioritize and promote AI. Reuters,
11.2.2019, https://www.reuters.com/article/us-usa-trump-artificialintelligence-idUSKCN1Q00A3
(7.8.2020)
Parkin, S. (2015): Killer robost: The soldiers that never sleeps. BBC Future, 16.7.2015,
http://www.bbc.com/future/story/20150715-killer-robots-the-soldiers-that-never-sleep (7.8.2020)
Pauen, M. (2011): Autonomie. In: Kolmer, P.; Wildfeuer, A. (Hg.): Neues Handbuch philosophischer
Grundbegriffe. Band 1, Freiburg/M&#252;nchen, S. 254&#8211;264
Peitz, D. (2018): Project Maven: &#187;Google wird einfach ersetzt&#171;. Interview mit Paul Scharre. Zeit Online,
5.6.2018, https://www.zeit.de/digital/internet/2018-06/
maven-militaerprojekt-google-ausstieg-ruestungsexperte-paul-scharre (7.8.2020)
Pellerin, C. (2017): Project Maven to Deploy Computer Algorithms to War Zone by Year&#8217;s End. DOD,
21.7.2017, https://dod.defense.gov/News/Article/Article/1254719/project-maven-to-deploy-computer-
algorithms-to-war-zone-by-years-end/ (7.8.2020)
Perrow, C. (1989): Normale Katastrophen. Die unvermeidbaren Risiken der Gro&#223;technik. Reihe Campus
1028, Frankfurt a.M.
Petermann, T.; Socher, M.; Wennrich, C. (1997): Pr&#228;ventive R&#252;stungskontrolle bei neuen Technologien.
Utopie oder Notwendigkeit? Studien des B&#252;ros f&#252;r Technikfolgen-Absch&#228;tzung beim Deutschen Bundestag
3, Berlin
Pettersson, T.; Eck, K. (2018): Organized violence, 1989&#8211;2017. In: Journal of Peace Research 55(4), S. 535&#8211; 
547
Pocock, C. (2018): Airbus and Dassault Launch a New FCAS &#8211; without BAE. AINonline, 25.4.2018,
https://www.ainonline.com/aviation-news/defense/2018-04-25/airbus-and-dassault-launch-new-fcas-wit-
hout-bae (7.8.2020)
Pomerleau, M. (2016): DOD plans to invest $600M in unmanned underwater vehicles. Defense Systems,
4.2.2016, https://defensesystems.com/articles/2016/02/04/dod-navy-uuv-investments.aspx (7.8.2020)
Pomerleau, M. (2018): How DoD is getting serious about artificial intelligence. Dronewars.net, 19.12.2018,
https://www.c4isrnet.com/c2-comms/2018/12/19/how-dod-is-getting-serious-about-artificial-intelli-
gence/ (9.10.2020)
Pontin, J. (2018): Greedy, brittle, opaque, and shallow: The downsides to Deep Learning. We&#8217;ve been
promised a revolution in how and why nearly everything happens. But the limits of modern artificial
intelligence are closer than we think. WIRED, 2.2.2018, https://www.wired.com/story/greedy-brittle-opaque-
and-shallow-the-downsides-to-deep-learning/ (7.8.2020)
POST (Parliamentary Office of Science and Technology) (2015): Automation in Military Operations. Houses
of Parliament, POSTnote Nr. 511, London, http://researchbriefings.files.parliament.uk/documents/POST-
PN-0511/POST-PN-0511.pdf (7.8.2020)
Qinetiq North America (2018): MAARS. Modular Advanced Armed Robotic System. https://qinetiq-
na.com/products/unmanned-systems/maars/ (7.8.2020)
Rabinoff, J. (2010): Machine gun-toting robots deployed on DMZ. Stars and Stripes, 12.7.2010,
https://www.stripes.com/news/pacific/korea/machine-gun-toting-robots-deployed-on-dmz-1.110809 
(7.8.2020)
RAF (Royal Air Force) (2003): Royal Air Force Aircraft &amp; Weapons. https://www.raf.mod.uk/what-we-
do/centre-for-air-and-space-power-studies/documents1/royal-air-force-aircraft-weapons-fourth-
editionrevised/ (7.8.2020)
RAFAEL (Rafael Advanced Defense Systems Ltd.) (o. J.a): IRON DOME&#8482; Family. https://www.ra-
fael.co.il/worlds/air-missile-defense/short-range-air-missile-de
fense/ (9.10.2020)
RAFAEL (o. J.b): PROTECTOR&#8482; USV. https://www.rafael.co.il/worlds/naval/usvs/ (9.10.2020)
Ray, J.; Atha, K.; Francis, E.; Dependahl, C.; Mulvenon, J.; Alderman, D.; Ragland-Luce, L.A. (2016):
China&#8217;s Industrial and Military Robotics Development. Research Report Prepared on Behalf of the U.S.-
China Economic and Security Review. Vienna, https://www.uscc.gov/sites/default/files/Re-
search/DGI_China%27s
%20Industrial%20and%20Military%20Robotics%20Development.pdf (7.8.2020)
Raytheon Missiles &amp; Defense (o. J.a): Iron Dome System and SkyHunter Missile. https://www.ray-
theon.com/capabilities/products/irondome (9.10.2020)
Raytheon Missiles &amp; Defense (o. J.b): Phalanx Weapon System. https://www.raytheonmissilesandde-
fense.com/capabilities/products/phalanx-close-in-weapon-system (9.10.2020)
Raytheon Technologies (o. J.): MK 54 lightweight torpedo. https://www.raytheon.com/capabilities/prod-
ucts/mk54 (9.10.2020)
Rees, M. (2013): QinetiQ Opens UK&#8217;s First Maritime Autonomy Centre. Unmanned Systems Technology,
13.9.2013, https://www.unmannedsystemstechnology.com/
2013/09/qinetiq-opens-uks-first-maritime-autonomy-centre/ (7.8.2020)
Reilly, M.B. (2016): Beyond video games: New artificial intelligence beats tactical experts in combat
simulation. University of Cincinnati, 27.6.2016, https://maga
zine.uc.edu/editors_picks/recent_features/alpha.html (7.8.2020)
Reuters (2017): Connecticut Could Be First State to Allow Armed Police Drones. Newsweek, 31.3.2017, 
http://www.newsweek.com/connecticut-drones-police-police-drones-armed-drones-577648 (7.8.2020)
Rheinmetall Defence (o. J.): Flugabwehrsysteme. https://www.rheinmetall-defence.
com/de/rheinmetall_defence/systems_and_products/air_defence_systems/index.
php (9.10.2020)
Rheinmetall Group (2018): Rheinmetall launches Mission Master Cargo Unmanned Ground Vehicle in time 
for Eurosatory 2018. Pressemitteilung, 11.6.2018, https://www.rheinmetall-defence.com/media/edi-
tor_media/rm_defence/public
relations/pressemitteilungen/2018/2018_06_11_rheinmetall_eurosatory/eng
lisch_1/2018-06-11_Rheinmetall_Eurosatory_MM_UGV_Cargo_en.pdf (9.10.2020)
Richter, W. (2013): R&#252;stungskontrolle f&#252;r Kampfdrohnen. SWP-Aktuell Nr. 29, Berlin, https://www.swp-ber-
lin.org/fileadmin/contents/products/aktuell/2013A29_
rrw.pdf (7.8.2020)
Riecke, T. (2018): &#187;In einigen Bereichen hat der Mensch bereits die Kontrolle verloren.&#171; Handelsblatt,
7.6.2018, https://www.handelsblatt.com/technik/thespark/
physiker-und-philosoph-armin-grunwald-in-einigen-bereichen-hat-der-
menschbereits-die-kontrolle-verloren/22656954.html (9.10.2020)
Riley, T. (2017): Artificial intelligence goes deep to beat humans at poker. In: Science online, doi: 
10.1126/science.aal0863
Robbins, M. (2016): Has a rampaging AI algorithm really killed thousands in Pakistan? A killer
machinelearning algorithm guiding the U.S. drone program has killed thousands of innocent people according to 
some reports. What&#8217;s the truth? The Guardian, 18.2.2016, https://www.theguardian.com/science/the-lay-
scientist/
2016/feb/18/has-a-rampaging-ai-algorithm-really-killed-thousands-in-pakistan (7.8.2020)
Roblin, S. (2018): LRASM: The Navy&#8217;s Game Changer Missile Russia and China Should Fear? The National 
Interest, 21.4.2018, https://nationalinterest.org/blog/
the-buzz/lrasm-the-navys-game-changer-missile-russia-china-should-25490 (7.8.2020)
Roff, H. (2015): Autonomous or &#187;Semi&#171; Autonomous Weapons? A Distinction Without Difference.
Huffington Post, 16.1.2015, Updated 18.3.2015, https://www.huff
ingtonpost.com/heather-roff/autonomous-or-semi-autono_b_6487268.html?guc
counter=1 (7.8.2020)
Rogoway, T. (2016): The Alarming Case of the USAF&#8217;s Mysteriously Missing Unmanned Combat Air
Vehicles. The USAF has them but isn&#8217;t telling us they do, or they don&#8217;t. Either way we are in trouble. Here&#8217;s
why. The Drive, 9.6.2016, http://www.thedrive.com/the-war-zone/3889/the-alarming-case-of-the-usafs-
mysteriously-missing-unmanned-combat-air-vehicles (7.8.2020)
Rosen Jacobson, B. (2017): Lethal Autonomous Weapons Systems: Mapping the GGE debate. Diplo
Foundation Policy Papers and Briefs Nr. 8, Genf, https://www.diplo
macy.edu/sites/default/files/Policy_papers_briefs_08_BRJ.pdf (7.8.2020)
Rosen, S.P. (1991): Winning the Next War. Innovation and the Modern Military. Ithaca
Rosenberg, Z. (2013): RAC MiG to design Skat-based unmanned combat air vehicle. FlightGlobal, 3.6.2013,
https://www.flightglobal.com/news/articles/rac-mig-to-design-skat-based-unmanned-combat-air-vehicle-
386609/ (7.8.2020)
R&#246;tzer, F. (2016): Russischer Kampfroboterpanzer soll bald von Armee eingesetzt werden. Heise online,
30.3.2016, https://www.heise.de/tp/features/Russischer-Kampf
roboterpanzer-soll-bald-von-Armee-eingesetzt-werden-3379287.html (7.8.2020)
Royakkers, L.; van Est, R. (2015): Just Ordinary Robots: Automation from Love to War. Cleveland
Royal Navy (2016): Royal Navy tests unmanned fleet of the future. https://www.royalnavy.mod.uk/news-and-
latest-activity/news/2016/october/14/161014-royal-navy-tests-unmanned-fleet-of-the-future (7.8.2020)
Royal Society (2017): Machine learning. The power and promise of computers that learn by example.
London, https://royalsociety.org/~/media/policy/projects/ma
chine-learning/publications/machine-learning-report.pdf (7.8.2020)
RT (2015): Russia &#187;completely ending&#171; activities under Conventional Armed Forces in Europe treaty.
https://on.rt.com/7tju9e (9.10.2020)
Sadot, U. (2016): Proliferated Drones. A Perspective on Israel. http://drones.cnas.org/
wp-content/uploads/2016/05/A-Perspective-on-Israel-Proliferated-Drones.pdf (7.8.2020)
Sadowski, R.W. (2016): Enabling MUM-T within Army Formations. Robotics Community of Practice.
https://www.darpa.mil/attachments/DARPA_MUMT_Up
dates.pdf (7.8.2020)
&#350;ahin, E.; Spears, W.M. (2005): Swarm robotics. Revised selected papers. Lecture notes in computer science 
State-of-the-art survey 3342, Berlin
Scharre, P. (2014a): Robotics on the Battlefield Part I. Range, Persistence and Daring. 20YY Series,
https://s3.amazonaws.com/files.cnas.org/documents/CNAS_Robo
ticsOnTheBattlefield_Scharre.pdf (7.8.2020)
Scharre, P.(2014b): Robotics on the Battlefield Part II. The Coming Swarm. 20YY Series, https://s3.amazo-
naws.com/files.cnas.org/documents/CNAS_TheComing
Swarm_Scharre.pdf (7.8.2020)
Scharre, P. (2017): A security perspective: Security concerns and possible arms control approaches. In: 
UNODA (Hg.): Perspectives on lethal autonomous weapon systems. New York, S. 19&#8211;33
Scharre, P. (2018): Army of none: autonomous weapons and the future of war. New York/London
Schmitt, M.N. (2012): Essays on Law and War at the Fault Lines. Den Haag
Scholz, K.-A. (2020): Warum Donald Trump Landminen wieder erlaubt. Deutsche Welle, 4.2.2020,
https://www.dw.com/de/warum-donald-trump-landminen-wieder-
erlaubt/a-52253518 (9.10.2020)
Sch&#246;rnig, N. (2014): Automatisierte Kriegsf&#252;hrung &#8211; Wie viel Entscheidungsraum bleibt dem Menschen? In: 
Aus Politik und Zeitgeschichte 35&#8211;37, S. 27&#8211;34
Sch&#246;rnig, N. (2017): Preserve past achievements! Why drones should stay within the missile technology
control regime (for the time being). PRIF report no. 149, Frankfurt a.M.
Sculley, D.; Holt, G.; Golovin, D.; Davydov, E.; Phillips, T.; Ebner, D.; Chaudhary, V.; Young, M.; Crespo,
J.-F.; Dennison, D. (2015): Hidden Technical Debt in Machine Learning Systems. In: Cortes, C.;
Lawrence, N.D.; Lee, D.D.; Sugiyama, M.; Garnett, R. (Hg.): Advances in Neural Information Processing
Systems 28, S. 2503&#8211;2511
Sharkey, N. (2012a): Killing made easy: from joysticks to politics. In: Lin, P.; Abney, K.; Bekey, G.: Robot 
ethics: the ethical and social implications of robotics. Cambridge/London, S. 111&#8211;128
Sharkey, N. (2012b): The evitability of autonomous robot warfare. In: Int. rev. Red Cross 94(886), S. 787&#8211; 
799
Sharkey, N. (2016): Staying in the loop: human supervisory control of weapons. In: Bhuta, N.; Beck, S.; Gei&#223;,
R.; Liu, H.-Y.; Kre&#223;, C. (Hg.): Autonomous weapons systems. Law, ethics, policy. Cambridge, S. 23&#8211;38
Sharkey, N. (2018): UK and Definitions of Autonomous Weapons Systems. Written evidence (AIC0248).
House of Lords Select Committee on Artificial Intelligence, London, http://data.parliament.uk/written-
evidence/committeeevidence.svc/evi
dencedocument/artificial-intelligence-committee/artificial-intelligence/written/
78207.html (7.8.2020)
Sharot, T. (2014): Das optimistische Gehirn. Warum wir nicht anders k&#246;nnen, als positiv zu denken. Berlin
Siciliano, B.; Khatib, O.; Groen, F.; Buehler, M.; Iagnemma, K.; Singh, S. (2009): The DARPA Urban
Challenge. Autonomous Vehicles in City Traffic. Springer Tracts in Advanced Robotics 56, Berlin/
Heidelberg
Silver, D.; Huang, A.; Maddison, C.; Guez, A.; Sifre, L.; van den Driessche, G.; Schrittwieser, J.; Antonoglou,
I.; Panneershelvam, V.; Lanctot, M.; Dieleman, S., Grewe, D. et al. (2016): Mastering the game of Go
with deep neural networks and tree search. In: Nature 529(7587), S. 484&#8211;489
Silver, D.; Hubert, T.; Schrittwieser, J.; Antonoglou, I.; Lai, M.; Guez, A.; Lanctot, M.; Sifre, L.; Kumaran,
D.; Graepel, T.; Lillicrap, T.; Simonyan, K.; Hassabis, D. (2017): Mastering Chess and Shogi by Self-
Play with a General Reinforcement Learning Algorithm. http://arxiv.org/pdf/1712.01815v1 (7.8.2020)
SIPRI (Stockholm International Peace Research Institute) (2019): Military expenditure by country.
Stockholm, https://www.sipri.org/sites/default/files/Data%20for%20
all%20countries%20from%201988 %E2 %80 %932019 %20in%20constant%20 %
282018 %29 %20USD.pdf (31.8.2020)
Smalley, D. (2016): Autonomous Swarmboats: New Missions, Safe Harbors. Office of Naval Research,
14.12.2016, https://www.onr.navy.mil/en/Media-Center/Press-
Releases/2016/Autonomous-Swarmboats.aspx (31.8.2020)
Smith, D. (2019): Trump withdraws from UN arms treaty as NRA crowd cheers in delight. The Guardian,
26.4.2019, https://www.theguardian.com/us-news/2019/
apr/26/trump-nra-united-nations-arms-treaty-gun-control (9.10.2020)
Smith, M. (2017): Statement regarding defeat of bill to allow police to weaponize drones. ACLU of
Connecticut, 1.5.2017, https://www.acluct.org/en/press-release/
statement-regarding-defeat-of-bill-to-allow-police-to-weaponize-drones (9.10.2020)
Sparrow, R. (2007): Killer Robots. In: Journal of Applied Philosophy 24(1), S. 62&#8211;77
Sparrow, R. (2016): Robots and Respect: Assessing the Case Against Autonomous Weapon Systems. In:
Ethics int. aff. 30(01), S. 93&#8211;116
Spektrum (1999): Asilomar-Konferenz. https://www.spektrum.de/lexikon/biologie/
asilomar-konferenz/5423 (9.10.2020)
Spiegel Online (2013): X-47B: Kampfdrohne gl&#252;ckt Landung auf Flugzeugtr&#228;ger. http://www.spiegel.de/wis-
senschaft/technik/x-47b-drohne-glueckt-landung-auf-
flugzeugtraeger-a-910546.html (31.8.2020)
Steinke, R. (2018): Der Internationale Strafgerichtshof. Bundeszentrale f&#252;r politische Bildung, 30.7.2018,
www.bpb.de/internationales/weltweit/innerstaatliche-konflikte/
169554/der-internationale-strafgerichtshof (9.10.2020)
Stevenson, B. (2016): New $2.2 billion Anglo-French FCAS phase announced. FlightGlobal, 8.3.2016,
https://www.flightglobal.com/news/articles/new-22-bil
lion-anglo-french-fcas-phase-announced-422866/ (31.8.2020)
Stimson (2015): UAV Export Controls and Regulatory Challenges. Working Group Report. Washington D.C.,
https://www.stimson.org/sites/default/files/file-attach
ments/ECRC%20Working%20Group%20Report.pdf (31.8.2020)
Stoecker, R. (2010): Die philosophischen Schwierigkeiten mit der Menschenw&#252;rde &#8211; und wie sie sich
vielleicht aufl&#246;sen lassen. In: ZiF-Mitteilungen 1, S. 19&#8211;30
Strawser, B.J. (2010): Moral Predators: The Duty to Employ Uninhabited Aerial Vehicles. In: Journal of
Military Ethics 9(4), S. 342&#8211;368
Su, J.; Vargas, D.V.; Kouichi, S. (2017): One pixel attack for fooling deep neural networks.
http://arxiv.org/pdf/1710.08864v2 (31.8.2020)
S&#252;ddeutsche Zeitung (2014): Europ&#228;ische Drohnen entwickeln. Interview mit Ursula von der Leyen.
S&#252;ddeutsche Zeitung, 2.7.2014, https://www.bundesregierung.de/
breg-de/bundeskanzlerin/europaeische-drohnen-entwickeln-620670 (1.9.2020)
Swaine, J. (2013): Barack Obama &#187;has authority to use drone strikes to kill Americans on US soil&#171;. The
Telegraph, 6.3.2013, https://www.telegraph.co.uk/news/worldnews/barackobama/9913615/Barack-Obama-
has-authority-to-use-drone-strikes-to-kill-Americans-on-US-soil.html
Szegedy, C.; Zaremba, W.; Sutskever, I.; Bruna, J.; Erhan, D.; Goodfellow, I.; Fergus, R. (2014): Intriguing 
properties of neural networks. http://arxiv.org/pdf/1312.6199v4 (31.8.2020)
Szondy, D. (2018): DARPA hands autonomous sub-hunter prototype over to the US Navy. New Atlas,
5.2.2018, https://newatlas.com/darpa-actuv-us-navy/53247/ (1.9.2020)
TAB (B&#252;ro f&#252;r Technikfolgen-Absch&#228;tzung beim Deutschen) (2003): Milit&#228;rische Nutzung des Weltraums
und M&#246;glichkeiten der R&#252;stungskontrolle im Weltraum (Autoren: Petermann, T.; Coenen, C.; Gr&#252;nwald,
R.). Sachstandsbericht. Arbeitsbericht Nr. 85, Berlin
TAB (2011): Stand und Perspektiven der milit&#228;rischen Nutzung unbemannter Systeme (Autoren: Petermann,
T.; Gr&#252;nwald, R.). Endbericht zum TA-Projekt. Arbeitsbericht Nr. 144, Berlin
TAB (2016): Technologien und Visionen der Mensch-Maschine-Entgrenzung. Sachstandsbericht zum TA-
Projekt &#187;Mensch-Maschine-Entgrenzungen: zwischen k&#252;nstlicher Intelligenz und Human Enhancement&#171;
(Autoren: Kehl, C.; Coenen, C.). TAB-Arbeitsbericht Nr. 167, Berlin
Tagesschau (2019): Boeing findet weiteres Softwareproblem. 5.4.2019, https://www.
tagesschau.de/wirtschaft/boeing-software-101.html (9.10.2020)
TARDEC (2017): TARDEC 30-Year Strategy. https://api.army.mil/e2/c/down
loads/489179.pdf (31.8.2020)
The Economist (2019): Humans struggle to cope when automation fails. 14.3.2019, https://www.econo-
mist.com/business/2019/03/14/humans-struggle-to-
copewhen-automation-fails (13.7.2020)
The Maritime Executive (2019): Littoral Combat Ship&#8217;s MCM Package Moves Forward. 28.1.2019,
https://www.maritime-executive.com/article/littoral-combat-ship-s-mcm-package-moves-forwardBaker,
B. (2019): Orca XLUUV: Boeing&#8217;s whale of an unmanned sub. NavalTechnology, 1.7.2019,
https://www.naval-tech
nology.com/features/boeing-orca-xluuv-unmanned-submarine/ (9.10.2020)
Thielmann, G.; Zagorski, A. (2017): INF Treaty Compliance: A Challenge and an Opportunity. Deep Cuts
Working Paper No. 9, http://www.deepcuts.org/images/
PDF/DeepCuts_WP9_ThielmannZagorski.pdf (31.8.2020)
Think Defence (o. J.): Brimstone. https://www.thinkdefence.co.uk/uk-complex-weap
ons/brimstone/ (9.10.2020)
Thomas, W. (2017): Trump Budget Cuts Defense S&amp;T by 5.8 % While Funding Third Offset Priorities.
Science Policy News from AIP, 1.6.2017, https://www.aip.
org/fyi/2017/trump-budget-cuts-defense-st-58-while-funding-third-offset-prior
ities (31.8.2020)
Thompson, M. (2013): Costly Flight Hours. TIME USA, LLC., 2.4.2013, http://na
tion.time.com/2013/04/02/costly-flight-hours/ (31.8.2020)
Tolk, A. (2015): Merging Two Worlds: Agent-based Simulation Methods for Autonomous Systems. In:
Williams, A.P.; Scharre, P. (Hg.): Autonomous systems. Issues for defence policymakers. Norfolk, S. 291&#8211; 
317
Tran, P. (2016): &#187;Neuron&#171; Combat Drone Completes First Sea Trials. Defense News, 8.7.2016,
https://www.defensenews.com/home/2016/07/08/neuron-combat-drone-
completes-first-sea-trials/ (31.8.2020)
Tran, P. (2018): The French Army could have its first unmanned vehicle by 2025. Defense News, 12.6.2018,
https://www.defensenews.com/digital-show-dailies/euro
satory/2018/06/12/the-french-army-could-have-its-first-unmanned-vehicle-by-
2025/ (31.8.2020)
Tucker, P. (2015): Pentagon Sets Up a Silicon Valley Outpost. Defense One, 23.4.2015, https://www.defense-
one.com/technology/2015/04/pentagon-sets-silicon-
valleyoutpost/110845/ (31.8.2020)
Tucker, P. (2016): The US Navy&#8217;s Autonomous Swarm Boats Can Now Decide What to Attack. Defense One,
14.12.2016, https://www.defenseone.com/technology/
2016/12/navys-autonomous-swarm-boats-can-now-decide-what-attack/133896/ (31.8.2020)
Tucker, P. (2017): The Future the US Military is Constructing: a Giant, Armed Nervous System. Defense 
One, 26.9.2017, https://www.defenseone.com/technology/
2017/09/future-us-military-constructing-giant-armed-nervous-system/141303/ (31.8.2020)
U.S. Air Force (2014): RPA Vector. Vision and Enabling Concepts 2013-2038. Washington D.C.,
http://www.af.mil/Portals/1/documents/news/USAFRPAVectorVi
sionandEnablingConcepts2013-2038.pdf (31.8.2020)
U.S. Air Force (2016): Small Unmanned Aircraft Systems (SUAS) Flight Plan: 2016-2036. Bridging the Gap 
Between Tactical and Strategic. http://www.af.mil/Por
tals/1/documents/isr/Small_UAS_Flight_Plan_2016_to_2036.pdf (31.8.2020)
U.S. Air Force; Office of the Chief Scientist (2015): Autonomous Horizons. System Autonomy in the Air
Force &#8211; A Path to the Future. Volume I: Human-Autonomy Teaming. Nr. AF/ST TR 15-01,
https://www.hsdl.org/?view&amp;did=768107 (31.8.2020)
U.S. Army (2017a): Multi-Domain Battle: Evolution of Combined Arms for the 21st Century 2025-2040.
Version 1.0. https://www.tradoc.army.mil/Portals/14/Docu
ments/MDB_Evolutionfor21st%20(1).pdf (31.8.2020)
U.S. Army (2017b): The U.S. Army Robotic and Autonomous Systems Strategy. https://www.tra-
doc.army.mil/Portals/14/Documents/RAS_Strategy.pdf (31.8.2020)
U.S. Department of State (2018): U.S. Policy on the Export of Unmanned Aerial Systems. Fact Sheet.
19.4.18, Update 24.7.2020, https://www.state.gov/u-s-policy-on-
the-export-of-unmanned-aerial-systems-2/ (31.8.2020)
U.S. Navy (2013): U.S. Navy Information Dominance Roadmap 2013-2028. https://
defenseinnovationmarketplace.dtic.mil/wp-content/uploads/2018/02/Informa
tion_Dominance_Roadmap_March_2013.pdf (31.8.2020)
U.S. President (2019): Executive Order on Maintaining American Leadership in Artificial Intelligence, The 
White House, 11.2.2019, https://www.whitehouse.
gov/presidential-actions/executive-order-maintaining-american-leadership-arti
ficial-intelligence/ (31.8.2020)
UAS Vision (2017): Partners Agree Configuration of European MALE RPAS.
https://www.uasvision.com/2017/08/03/partners-agree-configuration-of-european-male-rpas/ (9.10.2020)
UK Mission (2019a): Agenda item 5(b): Characterisation of the systems under consideration in order to
promote a common understanding on concepts and characteristics relevant to the objectives and purposes of
the Convention. CCW GGE. Genf, https://www.unog.ch/80256EDD006B8954/(httpAssets)/A969
D779E1E5E28BC12583D3003FC0D9/$file/20190318-5(b)_Characterisation_
Statement.pdf (31.8.2020)
UK Mission (2019b): Agenda item 5(d): Further consideration of the human element in the use of lethal force; 
aspects of human-machine interaction in the development, deployment and use of emerging technologies
in the area of lethal autonomous weapons systems. CCW GGE. Genf, https://www.unog.ch/80256EDD
006B8954/(httpAssets)/85A4AA89AFCFD316C12583D3003EAB3E/$file/20190318-5(d)_HMI_State-
ment.pdf (31.8.2020)
UK Mission (2019c): Agenda item 5(e): Possible options for addressing the humanitarian and international 
security challenges posed by emerging technologies in the area of lethal autonomous weapons systems in
the context of the objectives and purposes of the Convention without prejudging policy outcomes and 
taking into account past, present and future proposal. CCW GGE. Genf, https://www.
unog.ch/80256EDD006B8954/(httpAssets)/40C2167E8F162030C12583D3003
F4B94/$file/20190318-5(e)_Policy_Statement.pdf (31.8.2020)
Ulgen, O. (Hg.) (2017a): Human Dignity in an Age of Autonomous Weapons: Are We in Danger of Losing an 
&#187;Elementary Consideration of Humanity&#171;? European Society of International Law (ESIL) Annual
Conference, Riga, 8-10.9.2016, https://papers.ssrn.com/sol3/papers.cfm?abstract_id=2912002 (31.8.2020)
Ulgen, O. (2017b): Kantian Ethics in the Age of Artificial Intelligence and Robotics. In: Questions of
International Law 43, S. 59&#8211;83
UN (United Nations) (1991): Resolutions adopted by the General Assembly at its 46th session. 46/36. General 
and complete disarmament. L Transparency in Armaments. https://undocs.org/pdf?sym-
bol=en/A/RES/46/36 (31.8.2020)
UN (2013): The Arms Trade Treaty. https://unoda-web.s3-accelerate.amazonaws.com/wp-content/up-
loads/2013/06/English7.pdf (9.10.2020)
UN (2016a): Continuing operation of the United Nations Register of Conventional Arms and its further
development. Nr. A/71/259, New York, https://documents-dds-ny.un.org/doc/UN-
DOC/GEN/N16/242/39/PDF/N1624239.pdf?OpenElement (31.8.2020)
UN (2016b): Resolution adopted by the General Assembly on 5 December 2016. Nr. A/RES/71/44.
Transparency in armaments. New York, https://documents-dds-ny.un.org/doc/UNDOC/GEN/N16/420/26/PDF/
N1642026.pdf?OpenElement (25.4.2018)
UN (2016c): Germany 2016. UNROCA original report. https://www.unroca.org/germany/report/2016 
(9.10.2020)
UN (2016d): Singapore 2016. UNROCA original report. https://www.unroca.org/singapore/report/2016 
(9.10.2020)
UN (2016e): Spain 2016. UNROCA original report. https://www.unroca.org/spain/report/2016 (9.10.2020)
UN (2016f): Switzerland 2016. UNROCA original report. https://www.unroca.org/switzerland/report/2016 
(9.10.2020)
UN (o. J.a): Arms Trade. https://www.un.org/disarmament/convarms/att/ (9.10.2020)
UN (o. J.b): The Convention on Certain Conventional Weapons. https://www.unog.ch/80256EE600585943/
(httpPages)/4F0DEF093B4860B4C1257180004B1B30?OpenDocument (9.10.2020)
UN (o. J.c): UN Register of Conventional Arms. https://www.un.org/disarmament/convarms/register/ 
(9.10.2020)
UNIDIR (United Nations Institute for Disarmament Research) (2014a): Framing Discussions on the
Weaponization of Increasingly Autonomous Technologies. UNIDIR Resources Nr. 1, Genf, http://uni-
dir.org/files/publications/pdfs/framing-discussions-on-the-weaponization-of-increasingly-autonomous-
technologies-en-606.pdf (31.8.2020)
UNIDIR (2014b): The Weaponization of Increasingly Autonomous Technologies: Considering how
Meanigful Control might move the discussion forward. UNIDIR Resources Nr. 2, Genf, http://uni-
dir.org/files/publications/pdfs/considering-how-meaningful-human-control-might-move-the-discussion-
forward-en-615.pdf (31.8.2020)
UNIDIR (2016): Safety, Unintentional Risk and Accidents in the Weaponization of Increasingly Autonomous
Technologies. UNIDIR Resources Nr. 5, Genf, http://unidir.org/files/publications/pdfs/safety-uninten-
tional-risk-and-accidents-en-668.pdf (31.8.2020)
UNIDIR (2017): The Weaponization of Increasingly Autonomous Technologies: Concerns, Characteristics
and Definitional Approaches. a primer. UNIDIR Resources Nr. 6, Genf, http://www.unidir.org/files/pub-
lications/pdfs/the-weaponization-of-increasingly-autonomous-technologies-concerns-characteristics-and-
definitional-approaches-en-689.pdf (26.1.2018)
UNIDIR (2018a): Algorithmic Bias and the Weaponization of Increasingly Autonomous Technologies. A
Primer. UNIDIR Resources Nr. 9, Genf, http://www.unidir.org/files/publications/pdfs/algorithmic-bias-and-
the-weaponization-of-increasingly-autonomous-technologies-en-720.pdf (31.8.2020)
UNIDIR (2018b): The Weaponization of Increasingly Autonomous Technologies: Artificial Intelligence. a 
primer for CCW delegates. UNIDIR Resources Nr. 8, Genf, http://www.unidir.ch/files/publica-
tions/pdfs/the-weaponization-of-increasingly-autonomous-technologies-artificial-intelligence-en-700.pdf
(31.8.2020)
Villani, C. (2018): For a Meaningful Artificial Intelligence. Towards a French and European Strategy.
Mission assigned by the Prime Minister &#201;douard Philippe. https://www.aiforhumanity.fr/pdfs/MissionVil-
lani_Report_ENG-VF.pdf (31.8.2020)
Vo&#223;enkuhl, W. (1983): Moralische und nicht-moralische Bedingungen verantwortlichen Handelns: Eine
ethische und handlungstheoretische Analyse. In: Baumgartner, H.; Eser, A. (Hg.): Schuld und
Verantwortung. Philosophische und juristische Beitr&#228;ge zur Zurechenbarkeit menschlichen Handelns. T&#252;bingen,
S. 109&#8211;140
Wallach, W.; Allen, C. (2008): Moral Machines. Teaching Robots Right from Wrong. New York
Walsh, T.: (2018): Belgisches Parlament unterst&#252;tzt Verbot von autonomen Waffensystemen! Killer Roboter
stoppen!, 5.7.2018, https://www.killer-roboter-stoppen.de/2018/07/belgien-geht-wieder-voran-und-will-
verbot-von-autonomen-waffensystemen-unterstuetzen/ (9.10.2020)
Wassenaar Arrangement (2017): List of Dual-Use Goods and Technologies and Munitions List. Nr. WA-LIST
(17)1, https://www.wassenaar.org/app/uploads/2019/
consolidated/2017-List-of-DU-Goods-and-Technologies-and-Munitions-List.pdf (1.9.2020)
Wassenaar Arrangement Secretariat (2019): Compendium of Best Practice Documents. Public Documents
Volume III. www.wassenaar.org/best-practices/ (9.10.2020)
WD (Wissenschaftliche Dienste) (2012): Der Einsatz von Kampfdrohnen aus v&#246;lkerrechtlicher Sicht.
Deutscher Bundestag, WD 2 &#8211; 3000 - 118/12, Berlin
White, Y. (2015): Kratos&#8217; Third UTAP-22 Flight Exceeds Objectives, Successfully Performing All Primary
and Alternate Test Points. Kratos Defense &amp; Security Solutions, Inc., 21.12.2015,
http://ir.kratosdefense.com/news-releases/news-release-
details/kratos-third-utap-22-flight-exceeds-objectives-successfully (1.9.2020)
Wiegold, T. (2018): Nach dem Brexit: Britischer Anlauf f&#252;r eigenes Future Combat Air System. Augen
geradeaus!, 16.7.2018, https://augengeradeaus.net/2018/07/nach-dem-brexit-britischer-anlauf-fuer-eigenes-
future-combat-air-system/ (1.9.2020)
Wikipedia (2002): Kernwaffe: Strategische Kernwaffen. https://de.wikipedia.org/wiki/Kernwaffe#Strategi-
sche_Kernwaffen (9.10.2020)
Wikipedia (2003a): H&#246;hlengleichnis. https://de.wikipedia.org/wiki/H&#246;hlengleichnis (9.10.2020)
Wikipedia (2003b): Arleigh Burke-class destroyer. https://en.wikipedia.org/w/index.php?title=Arleigh_Burke-
class_destroyer&amp;oldid=752191 (9.10.2020)
Wikipedia (2003c): g-force: Human tolerance. https://en.wikipedia.org/wiki/G-force#Human_tolerance
(9.10.2020)
Wikipedia (2003d): Genfer Konventionen. https://de.wikipedia.org/wiki/Genfer_Konventionen (9.10.2020)
Wikipedia (2004): &#220;bereinkommen &#252;ber das Verbot des Einsatzes, der Lagerung, der Herstellung und der
Weitergabe von Antipersonenminen und &#252;ber deren Vernichtung. https://de.wikipedia.org/wiki/&#220;berein-
kommen_&#252;ber_das_Verbot_des_Einsatzes,_der_Lagerung,_der_Herstellung_und_der_
Weitergabe_von_Antipersonenminen_und_&#252;ber_deren_Vernichtung (9.10.2020)
Wikipedia (2006a): Prompt Global Strike. https://de.wikipedia.org/wiki/Prompt_Global_Strike (9.10.2020)
Wikipedia (2006b): Iran-Air-Flug 655. https://de.wikipedia.org/wiki/Iran-Air-Flug_655 (9.10.2020)
Wikipedia (2008): Computer shogi. https://en.wikipedia.org/wiki/Computer_shogi#Game_complexity
(9.10.2020)
Wikipedia (2009a): MANTIS (Flugabwehrsystem). https://de.wikipedia.org/wiki/MANTIS_(Flugabwehr
system) (9.10.2020)
Wikipedia (2009b): OODA-Loop. https://de.wikipedia.org/wiki/OODA-Loop (9.10.2020)
Wikipedia (2010a): Watson (K&#252;nstliche Intelligenz). https://de.wikipedia.org/wiki/Watson_(K&#252;nstliche_Intel-
ligenz) (9.10.2020)
Wikipedia (2010b): Flash Crash. https://de.wikipedia.org/wiki/Flash_Crash (9.10.2020)
Wikipedia (2016a): AlphaGo gegen Lee Sedol. https://de.wikipedia.org/wiki/AlphaGo_gegen_Lee_Sedol
(9.10.2020)
Wikipedia (2016b): Sea Hunter. https://en.wikipedia.org/wiki/Sea_Hunter (9.10.2020)
Wikipedia (2017): Edge Computing. https://de.wikipedia.org/wiki/Edge_Computing (9.10.2020)
Williams, H. (2017): Unmanned and unguarded: operators and industry look to close a defence gap. In: Jane&#8217;s
International Defence Review 50, S. 52
Williams, K.W. (2006): Human Factors Implications of Unmanned Aircraft Accidents: Flight-Control
Problems. Federal Aviation Administration Nr. DOT/FAA/AM-06/8, Oklahoma City,
https://rosap.ntl.bts.gov/view/dot/18240/dot_18240_DS1.pdf? (1.9.2020)
Wilson, C. (2005): Network Centric Warfare: Background and Oversight Issues for Congress. CRS Report for
Congress Nr. RL32411a, Fort Belvoir
WMDC (Weapons of Mass Destruction Commission) (2006): Weapons of Terror. Freeing the World of
Nuclear, Biological and Chemical Arms. Stockholm
Wong, K. (2014): Airshow China 2014: Norinco debuts Battle Robot UGV. JANES,
https://www.janes.com/article/45626/airshow-china-2014-norinco-debuts-bat
tle-robot-ugv
Work, R. (2016): Remarks by Deputy Secretary Work on Third Offset Strategy. Rede in Br&#252;ssel am
28.4.2016, https://www.defense.gov/Newsroom/Speeches/Speech/Article/%20753482/remarks-by-de-
puty-secretary-work-on-third-offset-strategy/ (1.9.2020)
Work, R.; Brimley, S. (2014): 20YY Preparing for War in the Robotic Age. https://s3.amazo-
naws.com/files.cnas.org/documents/CNAS_20YY_WorkBrimley.pdf?mtime=20160906082222 
(1.9.2020)
Wright-Patterson Air Force Base (2019): XQ-58A Valkyrie demonstrator completes inaugural flight.
https://www.wpafb.af.mil/News/Article-Display/Article/17777
43/xq-58a-valkyrie-demonstrator-completes-inaugural-flight/ (9.19.2020)Ackerman, S. (2016): UK to 
double armed drone fleet in deal with US Predator manufacturer. The Guardian, 3.12.2016,
https://www.theguardian.com/world/2016/
dec/03/drones-us-uk-deal-predator-reaper-protector (5.8.2020)
Xiong, W.; Droppo, J.; Huang, X.; Seide, F.; Seltzer, M.; Stolcke, A.; Yu, D.; Zweig, G. (2016): Achieving
Human Parity in Conversational Speech Recognition. http://arxiv.org/pdf/1610.05256v2 (1.9.2020)
Yeni &#350;afak (2016): Turkey starts building automatic shooting gun towers at Syrian border. Global
Construction Review, 6.7.2016, https://www.globalconstructionreview.com/news/turkey-rush-build-auto7matic-
shoo7ting-tow7ers/ (7.8.2020)
Zenko, M.; Kreps, S.E. (2014): Limiting armed drone proliferation. Council special report 69, New York
11 Anhang
11.1 Abbildungen
Abb. 3.1 T&#228;uschung eines Algorithmus zur Bilderkennung 40
Abb. 3.2 Erkl&#228;rbare KI 42
Abb. 4.1 Drohne &#187;Harop&#171; 49
Abb. 4.2 Gesch&#252;tz des Flugabwehrsystems MANTIS 53
Abb. 4.3 Das israelische UGV &#187;Guardium&#171; 54
Abb. 5.1 Anzahl ben&#246;tigter Luftfahrzeuge f&#252;r 24/7-Abdeckung 70
Abb. 5.2 Kampfradien bemannter und unbemannter Flugzeuge 77
Abb. 9.1 Rahmen f&#252;r menschliche Kontrolle im Entwicklungs- und Nutzungszyklus
einer Waffe 131
11.2 Tabellen
Tab. 2.1 Menschliche Rolle bei Zielauswahl und -bek&#228;mpfung 28
Tab. 2.2 Rolle von Mensch und AWS bei Zielauswahl und Angriffsentscheidung 29
Tab. 4.1 Staatlicher Besitz, Beschaffung und Einsatz von fortschrittlichen
Kampfdrohnen 46
Tab. 5.1 Missionen, in denen operative Vorteile von AWS besonders zum Tragen
kommen 73
Tab. 9.1 &#220;bersicht der f&#252;r AWS relevanten R&#252;stungskontrollvertr&#228;ge 120
Tab. 9.2 Transparenz- und vertrauensbildende Ma&#223;nahmen, Nichtverbreitung und 
Exportkontrolle 123
Tab. 9.3 Formulierungsvarianten zur menschlichen Kontrolle &#252;ber AWS 126
Tab. 9.4 M&#246;gliche Ans&#228;tze zur Regulierung von AWS 141
11.3 K&#228;sten
Kasten 2.1 Der Autonomiebegriff aus philosophischer Sicht 25
Kasten 2.2 Anmerkung zum Sprachgebrauch 26
Kasten 2.3 AWS in Platos H&#246;hle 29
Kasten 3.1 Schwache und starke KI 34
Kasten 3.2 Milit&#228;rische KI-Forschung und zivilgesellschaftlicher Protest: zwei 
Fallbeispiele 44
Kasten 4.1 Schw&#228;rme als neue Form der Kriegsf&#252;hrung? 60
Kasten 5.1 Die milit&#228;rische KI-Strategie der US-Regierung 79
Kasten 7.1 &#187;Boxed autonomy&#171; 97
Kasten 8.1 Zwei Grundmodelle ethischen Argumentierens 102
Kasten 8.2 Moralische Maschinen? Die neue Disziplin der Maschinenethik 104
Kasten 9.1 Aktuelle deutsche Position 128
Kasten 9.2 Qualit&#228;t menschlicher Kontrolle aus britischer Sicht 131
Kasten 9.3 AWS und der Europ&#228;ische Verteidigungsfonds 132
Kasten 9.4 Aktuelle Position des IKRK 135
Kasten 9.5 Die weitere Arbeit der CCW 138
11.4 Abk&#252;rzungen
A2/AD anti access and area denial
A-KSE adaptierter KSE-Vertrag
ATT Arms Trade Treaty
AWS autonome Waffensysteme
AWV Au&#223;enwirtschaftsverordnung
BW&#220; Biowaffen&#252;bereinkommen
CCW Convention on Certain Conventional Weapons
CIWS close-in weapon system
CW&#220; Chemiewaffen&#252;bereinkommen
FuE Forschung und Entwicklung
FCAS future combat air system
GGE Group of Governmental Experts
GPS Global Positioning System
HCoC Hague Code of Conduct against Ballistic Missile Proliferation
HVR Humanit&#228;res V&#246;lkerrecht
IED improvised explosive device
IKRK Internationales Komitee vom Roten Kreuz
INF intermediate-range nuclear rorces
IS Islamischer Staat
ISR intelligence, surveillance and reconnaissance
JAIC Joint Artificial Intelligence Center
KI k&#252;nstliche Intelligenz
KSE Vertrag &#252;ber konventionelle Streitkr&#228;fte in Europa
LAWS lethal autonomous weapon system
LOCUST low-cost UAV swarming technology
LRASM long range anti-ship missile
MALE medium altitude long endurance
MHC meaningful human control
ML machine learning (maschinelles Lernen)
MMW Millimeterwellen
MTCR missile technology control regime
MUM-T manned-unmanned teaming
NBS N&#228;chstbereichschutzsystem
NATO North Atlantic Treaty Organization
NGO Non-Governmental Organisation/Nichtregierungsorganisation
ONR Office of Naval Research
OPCW Organisation for the Prohibition of Chemical Weapons
OSZE Organisation f&#252;r Sicherheit und Zusammenarbeit in Europa
RPAS remotely piloted aircraft system
SARMO sense and react to military objects
START Strategic Arms Reduction Treaty
TEVV test and evaluation, verification and validation
UAV unmanned aerial vehicle
UCAV unmanned combat aerial vehicle
UGV unmanned ground vehicle
UN United Nations
USV unmanned surface vehicle
UUV unmanned underwater vehicle
UWS unbemanntes Waffensystem
VSBM vertrauens- und sicherheitsbildende Ma&#223;nahmen
ZP Zusatzprotokoll
Gesamtherstellung: H. Heenemann GmbH &amp; Co. KG, Buch- und Offsetdruckerei, Bessemerstra&#223;e 83&#8211;91, 12103 Berlin, www.heenemann-druck.de
Vertrieb: Bundesanzeiger Verlag GmbH, Postfach 10 05 34, 50445 K&#246;ln, Telefon (02 21) 97 66 83 40, Fax (02 21) 97 66 83 44, www.betrifft-gesetze.de
ISSN 0722-8333]</text>
    <titel>Technikfolgenabsch&#228;tzung (TA)
Autonome Waffensysteme</titel>
    <datum>2020-10-21</datum>
  </document>
