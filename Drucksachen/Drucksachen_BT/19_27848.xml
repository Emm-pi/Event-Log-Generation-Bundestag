<document>
    <id>251855</id>
    <drucksachetyp>Antrag</drucksachetyp>
    <dokumentart>Drucksache</dokumentart>
    <autoren_anzahl>17</autoren_anzahl>
    <typ>Dokument</typ>
    <vorgangsbezug_anzahl>1</vorgangsbezug_anzahl>
    <dokumentnummer>19/27848</dokumentnummer>
    <wahlperiode>19</wahlperiode>
    <herausgeber>BT</herausgeber>
    <pdf_hash>3dad6888eaa6952974daa6b79404db1b</pdf_hash>
    <aktualisiert>2022-07-26T19:57:15+02:00</aktualisiert>
    <vorgangsbezug>
      <id>275622</id>
      <titel>F&#246;rderung der automatischen Erkennung KI-manipulierter Fotos und Videos</titel>
      <vorgangstyp>Antrag</vorgangstyp>
    </vorgangsbezug>
    <urheber>
      <einbringer>false</einbringer>
      <bezeichnung>AfD</bezeichnung>
      <titel>Fraktion der AfD</titel>
    </urheber>
    <fundstelle>
      <pdf_url>https://dserver.bundestag.de/btd/19/278/1927848.pdf</pdf_url>
      <id>251855</id>
      <dokumentnummer>19/27848</dokumentnummer>
      <datum>2021-03-24</datum>
      <verteildatum>2021-03-24</verteildatum>
      <dokumentart>Drucksache</dokumentart>
      <drucksachetyp>Antrag</drucksachetyp>
      <herausgeber>BT</herausgeber>
      <urheber>Fraktion der AfD</urheber>
    </fundstelle>
    <autoren_anzeige>
      <id>7625</id>
      <titel>Joana Cotar, MdB, AfD</titel>
      <autor_titel>Joana Cotar</autor_titel>
    </autoren_anzeige>
    <autoren_anzeige>
      <id>2142</id>
      <titel>Dr. Michael Espendiller, MdB, AfD</titel>
      <autor_titel>Michael Espendiller</autor_titel>
    </autoren_anzeige>
    <text>[Deutscher Bundestag Drucksache 19/27848
19. Wahlperiode 24.03.2021
Antrag
der Abgeordneten Joana Cotar, Dr. Michael Espendiller, Uwe Schulz,
Waldemar Herdt, J&#246;rn K&#246;nig, Tobias Matthias Peterka, Dr. Dirk Spaniel, Ren&#233;
Springer, Petr Bystron, Peter Felser, Armin-Paulus Hampel, Mariana Iris
Harder-K&#252;hnel, Jens Maier, Dr. Birgit Malsack-Winkemann, Christoph 
Neumann, Ulrich Oehme, J&#252;rgen Pohl und der Fraktion der AfD
F&#246;rderung der automatischen Erkennung KI-manipulierter Fotos und Videos
Der Bundestag wolle beschlie&#223;en:
I. Der Deutsche Bundestag stellt fest:
In der Fotografie werden Bilder h&#228;ufig nachtr&#228;glich bearbeitet, aus &#228;sthetischen,
kommerziellen oder politischen Gr&#252;nden. Seit den 1990er Jahren geschieht dies durch
Software, deren Bearbeitung digitaler Fotos keine sichtbaren Spuren mehr hinterl&#228;sst. Dies
ist mittlerweile auch beim bewegten Bild m&#246;glich. Sogenannte Deep Fakes,
manipulierte Videodateien, deren Bearbeitung mit blo&#223;em Auge nicht mehr zu erkennen ist,
setzen eine lange Tradition fort. Durch den Einsatz K&#252;nstlicher Intelligenz (KI) w&#228;chst
die Pr&#228;zision der Manipulation, w&#228;hrend der zeitliche wie technische Aufwand und
der Preis sinken. Die daf&#252;r ben&#246;tigten Programme sind auf dem freien Markt erh&#228;ltlich
(Enquete-Kommission K&#252;nstliche Intelligenz, Gesamtbericht, Drucksache 19/23700).
Das wenige Jahre alte Werkzeug des Deep Fake auf der Basis des Maschinellen
Lernens wird zum Beispiel im Filmgesch&#228;ft eingesetzt, um Trailer zu produzieren,
Filmmusik zu komponieren oder um verstorbenen Schauspielern einen Gastauftritt zu
verschaffen. Auch die Gamingbranche profitiert von der Kreation potenziell grenzenloser,
realistisch wirkender Bildwelten mithilfe eines Algorithmus (Marc Bovenschulte: 
Deepfakes &#8211; Manipulation von Filmsequenzen, Themenkurzprofil Nr. 25, B&#252;ro f&#252;r
Technikfolgen-Absch&#228;tzung beim Deutschen Bundestag, Mai 2019). Des Weiteren
werden Deep Fakes im k&#252;nstlerischen oder kom&#246;diantischen Kontext verwendet.
Die Erstellung und Nutzung von Deep Fakes kann aber auch erhebliche Sch&#228;digungen
nach sich ziehen. Ein Gro&#223;teil der erzeugten Deep Fakes entf&#228;llt bisher auf das Genre
der Pornographie, wo das Antlitz von Prominenten als auch von Privatpersonen
t&#228;uschend echt in einzelne Szenen montiert wird, meist in rufsch&#228;digender oder
erpresserischer Absicht. Eine Studie identifizierte rund 14 000 gef&#228;lschte Videos, die online
zu sehen waren (Henry Ajder et al.: The State of Deepfakes. Landscape, threats, and
impact, September 2019). 
Daneben besteht f&#252;r Personen des &#246;ffentlichen Lebens die Gefahr, Opfer eines Deep
Fake zu werden. Diese k&#246;nnen im schlimmsten Fall Teil einer Kampagne sein, um 
Wahlen zu beeinflussen und das Ansehen demokratischer Institutionen zu untergraben.
Um dieser Gefahr zu begegnen, hat die amerikanische Defence Advanced Research
Projects Agency (DARPA) im Vorfeld der j&#252;ngsten US-Pr&#228;sidentschaftswahlen 70
Mio. US-Dollar an Forschungsmitteln bereitgestellt, um Abwehrstrategien gegen Deep
Fakes zu entwickeln; dabei geht es prim&#228;r um L&#246;sungen, F&#228;lschungen automatisch zu 
erkennen (Norbert Lossau: Deep Fake: Gefahren, Herausforderungen und
L&#246;sungswege, in: Analysen &amp; Argumente, Konrad-Adenauer-Stiftung, Februar 2020).  
Deep Fakes haben das Potenzial, das Vertrauen in den Journalismus, in Social Media 
und in die gesellschaftliche Debatte zu verletzen, wenn bei potenziell jedem Foto und
jedem Film der Verdacht im Raum steht, die Aufnahme k&#246;nnte gef&#228;lscht, verfremdet
oder konstelliert worden sein (https://ninaschick.org/deepfakes). Darunter k&#246;nnen die 
Demokratie und die &#246;ffentliche Sicherheit leiden, f&#252;r die Verl&#228;sslichkeit eine
unabdingbare Ressource darstellt. Daher ist es von vorrangigem Interesse, Deep Fakes
sicher, rasch und kosteng&#252;nstig als solche identifizieren zu k&#246;nnen. Experten gehen von
einer Verdopplung der online gestellten Deep Fakes alle sechs Monate aus (https://sen-
sity.ai/deepfake-threat-intelligence-a-statistics-snapshot-from-june-2020/).
Der gegenw&#228;rtige Ansatz der Medienforensik untersucht das statistische Paket aller
Spuren &#252;ber Maschinelles Lernen auf Unregelm&#228;&#223;igkeiten (Enquete-Kommission
K&#252;nstliche Intelligenz, PG 6, Drucksache 006). Der Erfolg der Verifizierung eines
Videos als authentisch h&#228;ngt wesentlich davon, ob RAW-Bilder vorliegen, ob die
Aufnahmekamera zug&#228;nglich ist, ob die Metadaten vollst&#228;ndig sind, wie oft ein Video
editiert wurde. Stark komprimierte Bilder erschweren einen Pr&#252;ferfolg immens.
F&#252;r die Medienforensik wird es mit der technologischen Weiterentwicklung der
Software zur permanenten Herausforderung, echte von computergenerierten Videos zu
unterscheiden. Zurzeit ist von einem Wettlauf zwischen Verfahren zur Produktion von 
Deep Fakes und solchen zu ihrer Identifizierung die Rede (Norbert Lossau 2020, a. a.
O.). Kommt dann noch die Zirkulation &#252;ber soziale Netzwerke hinzu, sind
manipulierte Videos nach aktuellem Forschungsstand schwer zu entlarven. F&#252;r eine
manuelle Analyse ist das Volumen schlicht zu gro&#223;; vollautomatisierte Verfahren
sto&#223;en an semantische Grenzen, wenn sie etwa ironische oder satirische Darstellungen
nicht als solche erkennen. 
II. Der Deutsche Bundestag fordert die Bundesregierung auf,
&#8211; das Wissen &#252;ber die dynamische Erstellung und die Detektion von Deep Fakes in
Deutschland deutlich auszubauen, um der signifikanten Verbreitung von Deep 
Fakes auf der Basis von KI angemessen begegnen zu k&#246;nnen;
&#8211; medienforensische F&#228;higkeiten entlang der oben genannte Desiderate in
Beh&#246;rden, Unternehmen und Hochschulen wie auch au&#223;eruniversit&#228;rer
Forschungseinrichtungen (stellvertretend das Fraunhofer Institut f&#252;r Digitale
Medientechnologie IDMT) auszubauen, um auch in diesem Bereich eine digitale Souver&#228;nit&#228;t
Deutschlands zu gew&#228;hrleisten;
&#8211; das Wissen &#252;ber Fertigung, Wirkung, Verbreitung und Identifizierung von Deep
Fakes f&#252;r eine systematische Aufkl&#228;rung der Bev&#246;lkerung &#252;ber das technisch
Praktikable zu nutzen;
&#8211; Kooperationen unterschiedlichster Institutionen rechtlich, organisatorisch,
finanziell und technisch zu unterst&#252;tzen, um die bestehenden Kompetenzen zur
Verifizierung/Falsifizierung von Medien zu b&#252;ndeln und so die Pr&#252;fpraxis erheblich
schlagkr&#228;ftiger und schneller machen und die Glaubw&#252;rdigkeit der
Pr&#252;fergebnisse zu erh&#246;hen;
&#8211; den entsprechenden Wissenstransfer mit hinreichenden Mitteln auszustatten, um
mit der technologischen Entwicklung Schritt halten zu k&#246;nnen.
Berlin, den 17. Februar 2021
Dr. Alice Weidel, Dr. Alexander Gauland und Fraktion
Begr&#252;ndung
Das Anfertigen eines Deep Fake ist nicht per se illegal, entscheidend sind Motivation und Kontext seiner
Verbreitung. Die interdisziplin&#228;re Forschung muss in die Lage versetzt werden, zwischen den unterschiedlichen
Aspekten des technisch Machbaren und des jeweiligen Verwendungszusammenhangs differenzieren zu k&#246;nnen.
Daf&#252;r muss sie aber in der Lage sein, Deep Fakes als solche zu erkennen.
Gro&#223;e Internet-Konzerne rufen Programmierer regelm&#228;&#223;ig zu Wettbewerben zur Detektion und Identifikation
von Deep Fakes auf (etwa unter www.kaggle.com/c/deepfake-detection-challenge). Mit diesem dergestalt
erworbenen Wissen k&#246;nnen sie gegen mutma&#223;liche Deep Fakes auf ihren eigenen Plattformen vorgehen. Dessen
ungeachtet erscheint es als unabdingbar, dass auch die unabh&#228;ngige Forschung jenseits kommerzieller Interessen 
den jeweils aktuellen Stand der Deep-Fakes-Fabrikation abbilden kann.
Die Bundesregierung attestiert, dass Entwicklung und Forschung zum Thema Deep Fakes noch am Anfang
stehen (Antwort der Bundesregierung auf eine Kleine Anfrage zur &#8222;Besch&#228;ftigung der Bundesregierung mit
Deepfakes&#8220;, Drucksache 19/15657). Allerdings zieht sie aus diesem Befund nicht den Schluss, dass die
Forschung zum Thema gerade angesichts des weiter oben genannten Missbrauchspotenzials im politischen Kontext
zu f&#246;rdern w&#228;re. Diese L&#252;cke schlie&#223;t der vorliegende Antrag.
Gesamtherstellung: H. Heenemann GmbH &amp; Co. KG, Buch- und Offsetdruckerei, Bessemerstra&#223;e 83&#8211;91, 12103 Berlin, www.heenemann-druck.de 
Vertrieb: Bundesanzeiger Verlag GmbH, Postfach 10 05 34, 50445 K&#246;ln, Telefon (02 21) 97 66 83 40, Fax (02 21) 97 66 83 44, www.betrifft-gesetze.de 
ISSN 0722-8333]</text>
    <titel>F&#246;rderung der automatischen Erkennung KI-manipulierter Fotos und Videos</titel>
    <datum>2021-03-24</datum>
  </document>
  